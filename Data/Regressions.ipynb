{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2159f0c5",
   "metadata": {},
   "source": [
    "### H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7724af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 300934 entries, 1 to 300934\n",
      "Data columns (total 52 columns):\n",
      " #   Column                      Non-Null Count   Dtype         \n",
      "---  ------                      --------------   -----         \n",
      " 0   Unnamed: 0                  300934 non-null  int64         \n",
      " 1   symbol                      300934 non-null  object        \n",
      " 2   date                        300934 non-null  datetime64[ns]\n",
      " 3   AdrActCnt                   208655 non-null  float64       \n",
      " 4   CapMrktEstUSD               230887 non-null  float64       \n",
      " 5   PriceUSD                    300934 non-null  float64       \n",
      " 6   TxCnt                       215205 non-null  float64       \n",
      " 7   volume_trusted_spot_usd_1d  290425 non-null  float64       \n",
      " 8   fng_value                   277273 non-null  float64       \n",
      " 9   fng_classification          277273 non-null  object        \n",
      " 10  VIX                         300934 non-null  float64       \n",
      " 11  ConSIX                      300934 non-null  float64       \n",
      " 12  TwitSIX                     215281 non-null  float64       \n",
      " 13  TotRes                      297338 non-null  float64       \n",
      " 14  UnemRt                      297338 non-null  float64       \n",
      " 15  IndPro                      297338 non-null  float64       \n",
      " 16  CPIPrc                      300934 non-null  float64       \n",
      " 17  EPU_DUS                     300934 non-null  float64       \n",
      " 18  Bullish                     300934 non-null  float64       \n",
      " 19  Bearish                     300934 non-null  float64       \n",
      " 20  InvSIX                      300934 non-null  float64       \n",
      " 21  is_alt                      300934 non-null  int64         \n",
      " 22  is_meme                     300934 non-null  int64         \n",
      " 23  is_gp                       300934 non-null  int64         \n",
      " 24  is_stable                   300934 non-null  int64         \n",
      " 25  is_btc                      300934 non-null  int64         \n",
      " 26  is_gold                     300934 non-null  int64         \n",
      " 27  daily_return                300810 non-null  float64       \n",
      " 28  cap_group                   264080 non-null  object        \n",
      " 29  log_daily_return            300934 non-null  float64       \n",
      " 30  log_volume                  289932 non-null  float64       \n",
      " 31  log_market_cap              230488 non-null  float64       \n",
      " 32  log_TxCnt                   212227 non-null  float64       \n",
      " 33  log_AdrActCnt               205800 non-null  float64       \n",
      " 34  log_diff_volume             290052 non-null  float64       \n",
      " 35  log_diff_market_cap         230383 non-null  float64       \n",
      " 36  log_diff_TxCnt              213693 non-null  float64       \n",
      " 37  log_diff_AdrActCnt          207161 non-null  float64       \n",
      " 38  log_IndPro                  297338 non-null  float64       \n",
      " 39  log_TotRes                  297338 non-null  float64       \n",
      " 40  log_CPIPrc                  300934 non-null  float64       \n",
      " 41  log_UnemRt                  297338 non-null  float64       \n",
      " 42  log_diff_IndPro             297222 non-null  float64       \n",
      " 43  log_diff_TotRes             297222 non-null  float64       \n",
      " 44  log_diff_CPIPrc             300933 non-null  float64       \n",
      " 45  log_diff_UnemRt             297222 non-null  float64       \n",
      " 46  diff_VIX                    300933 non-null  float64       \n",
      " 47  diff_TwitSIX                215159 non-null  float64       \n",
      " 48  diff_EPU_DUS                300933 non-null  float64       \n",
      " 49  diff_InvSIX                 300933 non-null  float64       \n",
      " 50  diff_fng_value              277040 non-null  float64       \n",
      " 51  diff_ConSIX                 300933 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(41), int64(7), object(3)\n",
      "memory usage: 121.7+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/1358553121.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_last['cap_group'] = df_last['CapMrktEstUSD'].apply(assign_cap_group)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_crypto = pd.read_csv('new_cleaned_crypto_data.csv')\n",
    "df_crypto['date'] = pd.to_datetime(df_crypto['date'])\n",
    "\n",
    "df_alt = df_crypto[df_crypto['is_alt'] == 1].copy()\n",
    "df_alt['date'] = pd.to_datetime(df_alt['date'])\n",
    "df_alt = df_alt.sort_values(['symbol', 'date'])\n",
    "\n",
    "df_last = df_alt.groupby('symbol').tail(1)\n",
    "q33 = df_last['CapMrktEstUSD'].quantile(0.33)\n",
    "q66 = df_last['CapMrktEstUSD'].quantile(0.66)\n",
    "def assign_cap_group(mcap):\n",
    "    if mcap >= q66:\n",
    "        return 'High'\n",
    "    elif mcap >= q33:\n",
    "        return 'Mid'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "df_last['cap_group'] = df_last['CapMrktEstUSD'].apply(assign_cap_group)\n",
    "cap_map = df_last.set_index('symbol')['cap_group'].to_dict()\n",
    "df_crypto['cap_group'] = df_crypto['symbol'].map(cap_map)\n",
    "\n",
    "df_crypto['log_daily_return'] =  100 * np.log(df_crypto['PriceUSD'] / df_crypto['PriceUSD'].shift(1))\n",
    "\n",
    "df_crypto = df_crypto.dropna(subset=['log_daily_return'])\n",
    "\n",
    "# compute log levels\n",
    "df_crypto['log_volume'] = np.log(df_crypto['volume_trusted_spot_usd_1d'].replace(0, np.nan))\n",
    "df_crypto['log_market_cap'] = np.log(df_crypto['CapMrktEstUSD'].replace(0, np.nan))\n",
    "df_crypto['log_TxCnt'] = np.log(df_crypto['TxCnt'].replace(0, np.nan))\n",
    "df_crypto['log_AdrActCnt'] = np.log(df_crypto['AdrActCnt'].replace(0, np.nan))\n",
    "# compute log differences\n",
    "df_crypto['log_diff_volume'] = 100 * np.log(df_crypto['volume_trusted_spot_usd_1d'] / df_crypto['volume_trusted_spot_usd_1d'].shift(1))\n",
    "df_crypto['log_diff_market_cap'] = 100 * np.log(df_crypto['CapMrktEstUSD'] / df_crypto['CapMrktEstUSD'].shift(1))\n",
    "df_crypto['log_diff_TxCnt'] = 100 * np.log(df_crypto['TxCnt'] / df_crypto['TxCnt'].shift(1))\n",
    "df_crypto['log_diff_AdrActCnt'] = 100 * np.log(df_crypto['AdrActCnt'] / df_crypto['AdrActCnt'].shift(1))\n",
    "\n",
    "#compute log levels for economic indicators\n",
    "df_crypto['log_IndPro'] = np.log(df_crypto['IndPro'].replace(0, np.nan))\n",
    "df_crypto['log_TotRes'] = np.log(df_crypto['TotRes'].replace(0, np.nan))\n",
    "df_crypto['log_CPIPrc'] = np.log(df_crypto['CPIPrc'].replace(0, np.nan))\n",
    "df_crypto['log_UnemRt'] = np.log(df_crypto['UnemRt'].replace(0, np.nan))\n",
    "\n",
    "# compute log differences for economic indicators\n",
    "df_crypto['log_diff_IndPro'] = 100 * np.log(df_crypto['IndPro'] / df_crypto['IndPro'].shift(1))\n",
    "df_crypto['log_diff_TotRes'] = 100 * np.log(df_crypto['TotRes'] / df_crypto['TotRes'].shift(1))\n",
    "df_crypto['log_diff_CPIPrc'] = 100 * np.log(df_crypto['CPIPrc'] / df_crypto['CPIPrc'].shift(1))\n",
    "df_crypto['log_diff_UnemRt'] = 100 * np.log(df_crypto['UnemRt'] / df_crypto['UnemRt'].shift(1))\n",
    "\n",
    "# compute differences for sentiment proxies (VIX,TwitSIX, EPU_DUS, InvSIX, fng_value, ConSIX)\n",
    "df_crypto['diff_VIX'] = df_crypto['VIX'].diff()\n",
    "df_crypto['diff_TwitSIX'] = df_crypto['TwitSIX'].diff()\n",
    "df_crypto['diff_EPU_DUS'] = df_crypto['EPU_DUS'].diff()\n",
    "df_crypto['diff_InvSIX'] = df_crypto['InvSIX'].diff()\n",
    "df_crypto['diff_fng_value'] = df_crypto['fng_value'].diff()\n",
    "df_crypto['diff_ConSIX'] = df_crypto['ConSIX'].diff()\n",
    "\n",
    "df_crypto.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79c92c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/crypto_regression_summary_no_sentiment.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/crypto_regression_summary_EPU_DUS.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/crypto_regression_summary_VIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/crypto_regression_summary_InvSIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/crypto_regression_summary_TwitSIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/crypto_regression_summary_ConSIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/crypto_regression_summary_fng_value.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# ---------- 1) PREPROCESSING (two-part, per-symbol, NA-safe) ----------\n",
    "base_cols = ['AdrActCnt', 'volume_trusted_spot_usd_1d', 'TxCnt']\n",
    "\n",
    "for col in base_cols:\n",
    "    prev = df_crypto.groupby('symbol')[col].shift(1)   # per-symbol lag\n",
    "    curr = df_crypto[col]\n",
    "    df_crypto[f'jump_{col}'] = ((prev == 0) & (curr > 0)).astype(int)\n",
    "    # set to 0 outside >0→>0 regime so we don't drop rows\n",
    "    df_crypto[f'log_diff_{col}'] = np.where((prev > 0) & (curr > 0),\n",
    "                                            np.log(curr/prev), 0.0)\n",
    "\n",
    "# ---------- 2) ELIGIBILITY (robust) ----------\n",
    "def min_required_n(regressors, floor=365, margin=30):\n",
    "    p = 1 + len(regressors)  # + intercept\n",
    "    return max(floor, p + margin)\n",
    "\n",
    "def _symbol_eligible(sub, target, regressors):\n",
    "    if any(r not in sub.columns for r in regressors):\n",
    "        return False, \"missing_columns\"\n",
    "    cc = sub.dropna(subset=[target])  # X have no NA after step 1\n",
    "    if len(cc) < min_required_n(regressors):\n",
    "        return False, f\"too_few_obs:{len(cc)}\"\n",
    "    # require variation for continuous vars (allow jump_* to be constant)\n",
    "    for r in regressors:\n",
    "        if not r.startswith('jump_') and cc[r].nunique() < 2:\n",
    "            return False, f\"no_variation:{r}\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "# ---------- 3) REGRESSION ----------\n",
    "def run_symbol_regression(df, symbol, target, regressors):\n",
    "    sub = df[df['symbol'] == symbol].sort_values('date').copy()\n",
    "    if target not in sub.columns:\n",
    "        sub[target] = sub['log_daily_return'].shift(-1)\n",
    "\n",
    "    ok, reason = _symbol_eligible(sub, target, regressors)\n",
    "    if not ok:\n",
    "        return None\n",
    "\n",
    "    sub = sub.dropna(subset=[target])  # only target must be present\n",
    "\n",
    "    # standardize only continuous predictors (leave jump_* as is)\n",
    "    cont = [r for r in regressors if not r.startswith('jump_')]\n",
    "    if cont:\n",
    "        sub[cont] = StandardScaler().fit_transform(sub[cont])\n",
    "\n",
    "    rhs = ' + '.join(regressors)\n",
    "    fit = smf.ols(f\"{target} ~ {rhs}\", data=sub).fit()\n",
    "    if fit.df_resid <= 8:   # guard for reliable HAC/AIC\n",
    "        return None\n",
    "\n",
    "    n = len(sub)\n",
    "    max_hac_lag = int(n ** 0.25)\n",
    "\n",
    "    best_aic, best_lag, best_res = np.inf, 0, None\n",
    "    for lag in range(max_hac_lag + 1):\n",
    "        r = fit.get_robustcov_results(cov_type='HAC', maxlags=lag, use_correction=True)\n",
    "        if r.aic < best_aic:\n",
    "            best_aic, best_lag, best_res = r.aic, lag, r\n",
    "\n",
    "    dw_val = float(durbin_watson(fit.resid))\n",
    "\n",
    "    out = {\n",
    "        'symbol': symbol, 'n': n, 'nobs': n,\n",
    "        'n_regs': len(regressors), 'regs_used': ','.join(regressors),\n",
    "        'opt_hac_lag': best_lag, 'r2': best_res.rsquared,\n",
    "        'adj_r2': best_res.rsquared_adj, 'aic': best_res.aic, 'bic': best_res.bic,\n",
    "        'log_likelihood': best_res.llf, 'fstat': best_res.fvalue, 'f_pval': best_res.f_pvalue,\n",
    "        'durbin_watson': dw_val\n",
    "    }\n",
    "    for name, b, se, t, p in zip(best_res.model.exog_names,\n",
    "                                 best_res.params, best_res.bse, best_res.tvalues, best_res.pvalues):\n",
    "        out[f'coef_{name}']   = b\n",
    "        out[f'stderr_{name}'] = se\n",
    "        out[f'tval_{name}']   = t\n",
    "        out[f'pval_{name}']   = p\n",
    "    return out\n",
    "\n",
    "def run_all_symbols(df, target, regressors):\n",
    "    df = df.sort_values(['symbol','date']).copy()\n",
    "    # crea il target t+1 se non presente\n",
    "    if target not in df.columns:\n",
    "        df[target] = df.groupby('symbol')['log_daily_return'].shift(-1)\n",
    "\n",
    "    results = []\n",
    "    kept, dropped = [], []\n",
    "\n",
    "    for sym in df['symbol'].unique():\n",
    "        sub = df[df['symbol'] == sym]\n",
    "        if _symbol_eligible(sub, target, regressors):\n",
    "            r = run_symbol_regression(df, sym, target, regressors)\n",
    "            if r is not None:\n",
    "                results.append(r)\n",
    "                kept.append(sym)\n",
    "            else:\n",
    "                dropped.append(sym)\n",
    "        else:\n",
    "            dropped.append(sym)\n",
    "\n",
    "    out = pd.DataFrame(results)\n",
    "    # Log sintetico (opzionale)\n",
    "    print(f\"Kept {len(set(kept))} symbols; Dropped {len(set(dropped))}: {sorted(set(dropped))[:10]}...\")\n",
    "    return out\n",
    "\n",
    "# ---------------------- 3) SET UP YOUR LOOP ----------------------\n",
    "target = 'log_daily_next'\n",
    "\n",
    "base_regressors = [\n",
    "    'log_daily_return',\n",
    "    # 'jump_AdrActCnt',\n",
    "    'log_diff_AdrActCnt',\n",
    "    # 'jump_volume_trusted_spot_usd_1d',\n",
    "    'log_diff_volume_trusted_spot_usd_1d',\n",
    "    # 'jump_TxCnt',\n",
    "    'log_diff_TxCnt',\n",
    "    'log_UnemRt', 'log_IndPro', 'log_CPIPrc', 'log_TotRes',\n",
    "]\n",
    "\n",
    "sentiment_proxies = {\n",
    "    'no_sentiment': None,\n",
    "    'EPU_DUS':      'EPU_DUS',\n",
    "    'VIX':          'VIX',\n",
    "    'InvSIX':       'InvSIX',\n",
    "    'TwitSIX':      'TwitSIX',\n",
    "    'ConSIX':       'ConSIX',\n",
    "    'fng_value':    'fng_value'\n",
    "}\n",
    "\n",
    "for name, proxy in sentiment_proxies.items():\n",
    "    regs = base_regressors.copy()\n",
    "    if proxy is not None:\n",
    "        regs.insert(1, proxy)   # sentiment subito dopo il ritorno laggato\n",
    "    summary_df = run_all_symbols(df_crypto, target, regs)\n",
    "    fn = f\"Regressions/crypto_regression_summary_{name}.csv\"\n",
    "    summary_df.to_csv(fn, index=False)\n",
    "    print(f\"Saved {summary_df['symbol'].nunique()} symbols → {fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f200238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ crypto_regression_summary_ConSIX.csv -> Regressions/Tables/table_ConSIX.tex\n",
      "✅ crypto_regression_summary_EPU_DUS.csv -> Regressions/Tables/table_EPU_DUS.tex\n",
      "✅ crypto_regression_summary_InvSIX.csv -> Regressions/Tables/table_InvSIX.tex\n",
      "✅ crypto_regression_summary_TwitSIX.csv -> Regressions/Tables/table_TwitSIX.tex\n",
      "✅ crypto_regression_summary_VIX.csv -> Regressions/Tables/table_VIX.tex\n",
      "✅ crypto_regression_summary_fng_value.csv -> Regressions/Tables/table_fng_value.tex\n",
      "✅ crypto_regression_summary_no_sentiment.csv -> Regressions/Tables/table_log_diff_AdrActCnt.tex\n",
      "[skip] h2_functional_avg_then_transform_results.csv: 'symbol' column missing in Regressions/h2_functional_avg_then_transform_results.csv\n",
      "[skip] h2_wald_tests.csv: 'symbol' column missing in Regressions/h2_wald_tests.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Paths =====\n",
    "REGRESSIONS_DIR = \"Regressions\"\n",
    "OUT_DIR = \"Regressions/Tables\"\n",
    "Path(OUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# ===== Classification =====\n",
    "def classify_symbol(symbol):\n",
    "    if symbol == \"MTL_METAL\":\n",
    "        symbol = \"MTL\"\n",
    "    BTC = {\"BTC\"}\n",
    "    ALT_HIGH = {\"AAVE\",\"ADA\",\"ALGO\",\"ATOM\",\"AVAX\",\"BCH\",\"BNB\",\"BSV\",\"CRO\",\"CRV\",\"DOT\",\"ENS\",\"EOS\",\"ETC\",\n",
    "                \"ETH\",\"FIL\",\"FLOW\",\"GALA\",\"GRT\",\"ICP\",\"LDO\",\"LINK\",\"LTC\",\"MANA\",\"MKR\",\"QNT\",\"SAND\",\"SOL\",\n",
    "                \"TRX\",\"UNI\",\"VET\",\"XLM\",\"XMR\",\"XRP\",\"XTZ\",\"ZEC\"}\n",
    "    ALT_MID  = {\"1INCH\",\"ANT\",\"AUDIO\",\"BAT\",\"COMP\",\"CVC\",\"CVX\",\"DASH\",\"DCR\",\"DGB\",\"ELF\",\"ENJ\",\"FTT\",\"FXS\",\n",
    "                \"GAS\",\"GLM\",\"GNO\",\"ICX\",\"LPT\",\"LRC\",\"LUNA\",\"NEO\",\"QTUM\",\"RSR\",\"SKL\",\"SNT\",\"SNX\",\n",
    "                \"SUSHI\",\"UMA\",\"WAVES\",\"XVG\",\"YFI\",\"ZIL\",\"ZRX\"}\n",
    "    ALT_LOW  = {\"ALCX\",\"ALPHA\",\"API3\",\"BADGER\",\"BAL\",\"BIT\",\"BNT\",\"BTG\",\"CEL\",\"DRGN\",\"FUN\",\"GNT\",\"GRIN\",\"HEDG\",\"HT\",\n",
    "                \"KNC\",\"LEND\",\"LOOM\",\"LSK\",\"MAID\",\"MTL\",\"NMR\",\"OGN\",\"OMG\",\"PAY\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\n",
    "                \"QASH\",\"REN\",\"REP\",\"ROOK\",\"SRM\",\"STORJ\",\"SWRV\",\"VTC\",\"WNXM\",\"WTC\",\"XEM\"}\n",
    "    STABLE   = {\"USDT\",\"USDC\",\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\"}\n",
    "    GOLDPEG  = {\"PAXG\",\"XAUT\"}\n",
    "    MEME     = {\"DOGE\",\"SHIB\",\"APE\"}\n",
    "    if symbol in BTC:     return \"BTC\"\n",
    "    if symbol in ALT_HIGH:return \"ALT_HIGH\"\n",
    "    if symbol in ALT_MID: return \"ALT_MID\"\n",
    "    if symbol in ALT_LOW: return \"ALT_LOW\"\n",
    "    if symbol in STABLE:  return \"STABLE\"\n",
    "    if symbol in GOLDPEG: return \"GOLDPEG\"\n",
    "    if symbol in MEME:    return \"MEME\"\n",
    "    return \"UNCLASSIFIED\"\n",
    "\n",
    "# ===== Helpers =====\n",
    "def first_existing(df, names):\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "def starify(p):\n",
    "    if pd.isna(p): return \"\"\n",
    "    return \"**\" if p < 0.01 else (\"*\" if p < 0.05 else \"\")\n",
    "\n",
    "def fmt_num(x, d=3, int_ok=False):\n",
    "    if pd.isna(x): return \"\"\n",
    "    if int_ok:\n",
    "        try: return f\"{int(x)}\"\n",
    "        except: pass\n",
    "    try: return f\"{float(x):.{d}f}\"\n",
    "    except: return \"\"\n",
    "\n",
    "# ===== pick 10 symbols per your rule =====\n",
    "def pick_symbols(df, proxy):\n",
    "    pcol = first_existing(df, [f\"pval_{proxy}\", f\"pval_log_d_{proxy}\", f\"pval_log_diff_{proxy}\"])\n",
    "    if pcol is None:\n",
    "        # fallback: use any pval_* that exists (last resort)\n",
    "        pvals = [c for c in df.columns if c.startswith(\"pval_\") and c != \"pval_Intercept\"]\n",
    "        pcol = pvals[0] if pvals else None\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"group\"] = d[\"symbol\"].map(classify_symbol)\n",
    "\n",
    "    def pick_median(group):\n",
    "        sub = d[(d[\"group\"] == group) & d[pcol].notna()].sort_values(pcol)\n",
    "        if sub.empty: return None\n",
    "        return sub.iloc[len(sub)//2][\"symbol\"]\n",
    "\n",
    "    def pick_min(group):\n",
    "        sub = d[(d[\"group\"] == group) & d[pcol].notna()].sort_values(pcol)\n",
    "        if sub.empty: return None\n",
    "        return sub.iloc[0][\"symbol\"]\n",
    "\n",
    "    chosen = []\n",
    "    if \"BTC\" in set(d[\"symbol\"]): chosen.append(\"BTC\")\n",
    "\n",
    "    for grp in [\"ALT_HIGH\",\"ALT_MID\",\"ALT_LOW\"]:\n",
    "        s1 = pick_min(grp)\n",
    "        s2 = pick_median(grp)\n",
    "        picks = []\n",
    "        if s1: picks.append(s1)\n",
    "        if s2 and s2 not in picks: picks.append(s2)\n",
    "        if len(picks) < 2:\n",
    "            # deterministic pad\n",
    "            pool = [x for x in sorted(d.loc[d[\"group\"]==grp,\"symbol\"]) if x not in picks]\n",
    "            while len(picks) < 2 and pool:\n",
    "                picks.append(pool.pop(0))\n",
    "        chosen += picks[:2]\n",
    "\n",
    "    for grp in [\"STABLE\",\"GOLDPEG\",\"MEME\"]:\n",
    "        s = pick_median(grp)\n",
    "        if s: chosen.append(s)\n",
    "\n",
    "    return chosen[:10]\n",
    "\n",
    "# ===== Build one LaTeX table from a CSV =====\n",
    "def build_table_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"symbol\" not in df.columns:\n",
    "        raise ValueError(f\"'symbol' column missing in {csv_path}\")\n",
    "\n",
    "    # detect proxy (the sentiment var): first coef_* that is not intercept/logs\n",
    "    proxy_candidates = [c.replace(\"coef_\",\"\") for c in df.columns\n",
    "                        if c.startswith(\"coef_\")\n",
    "                        and c not in (\"coef_Intercept\",\"coef_log_daily_return\")]\n",
    "    if not proxy_candidates:\n",
    "        raise ValueError(f\"No proxy coef_ column found in {csv_path}\")\n",
    "    proxy = proxy_candidates[0]\n",
    "\n",
    "    # columns (LaTeX rows) you want, with flexible suffixes\n",
    "    row_specs = [\n",
    "        (\"Intercept\",           [\"Intercept\"]),\n",
    "        (proxy,                 [proxy, f\"log_d_{proxy}\", f\"log_diff_{proxy}\"]),\n",
    "        (r\"$R_{i,t}$\",          [\"log_daily_return\",\"log_returns_lag1\",\"lag_return\",\"returns_lag1\"]),\n",
    "        (r\"$\\Delta\\log\\ $TxCnt\",           [\"log_TxCnt\",\"log_diff_TxCnt\",\"log_d_TxCnt\"]),\n",
    "        (r\"$\\Delta\\log\\ $Volume\",          [\"log_volume_trusted_spot_usd_1d\",\"log_diff_volume_trusted_spot_usd_1d\",\"log_d_volume_trusted_spot_usd_1d\"]),\n",
    "        (r\"$\\Delta\\log\\ $AdrActCnt\",     [\"log_AdrActCnt\",\"log_diff_AdrActCnt\"]),\n",
    "        (r\"$\\log\\ $UnemRt\",          [\"log_UnemRt\"]),\n",
    "        (r\"$\\log\\ $IndPro\",          [\"log_IndPro\"]),\n",
    "        (r\"$\\log\\ $CPIPrc\",          [\"log_CPIPrc\"]),\n",
    "        (r\"$\\log\\ $TotRes\",          [\"log_TotRes\"]),\n",
    "    ]\n",
    "\n",
    "    # metric columns (flexible names)\n",
    "    col_N   = first_existing(df, [\"n\",\"nobs\",\"N\"])\n",
    "    col_DW  = first_existing(df, [\"dw\",\"durbin_watson\",\"DurbinWatson\"])\n",
    "    col_F   = first_existing(df, [\"fstat\",\"F\",\"f_stat\"])\n",
    "    col_R2  = first_existing(df, [\"r_squared\",\"r2\"])\n",
    "    col_AR2 = first_existing(df, [\"adj_r_squared\",\"adj_r2\",\"adjR\"])\n",
    "    \n",
    "    symbols = pick_symbols(df, proxy)\n",
    "    idx = {s: df[df[\"symbol\"]==s].iloc[0] for s in symbols if not df[df[\"symbol\"]==s].empty}\n",
    "\n",
    "    # resolve which exact CSV cols to use (coef/stderr/pval) for each row\n",
    "    resolved = []\n",
    "    for label, suffixes in row_specs:\n",
    "        coef_col  = first_existing(df, [f\"coef_{s}\"   for s in suffixes])\n",
    "        se_col    = first_existing(df, [f\"stderr_{s}\" for s in suffixes])  # <-- Newey–West SE expected here\n",
    "        pval_col  = first_existing(df, [f\"pval_{s}\"   for s in suffixes])\n",
    "        resolved.append((label, coef_col, se_col, pval_col))\n",
    "\n",
    "    # ---- Assemble LaTeX lines ----\n",
    "    lines = []\n",
    "    lines += [\n",
    "        r\"\\begin{table}[ht]\",\n",
    "        r\"\\centering\",\n",
    "        r\"\\scriptsize\",\n",
    "        r\"\\setlength{\\tabcolsep}{4pt}\",\n",
    "        rf\"\\begin{{tabular}}{{l *{{{len(symbols)}}}{{c}}}}\",\n",
    "        r\"\\toprule\",\n",
    "        r\"&\\multicolumn{1}{c}{\\textbf{Bitcoin}}&\\multicolumn{2}{c}{\\textbf{High-Cap}}&\\multicolumn{2}{c}{\\textbf{Mid-Cap}}&\\multicolumn{2}{c}{\\textbf{Low-Cap}}&\\multicolumn{1}{c}{\\textbf{Gold}}&\\multicolumn{1}{c}{\\textbf{Stable}}&\\multicolumn{1}{c}{\\textbf{Meme}}\\\\\",\n",
    "        r\"\\addlinespace\",\n",
    "        \" & \" + \" & \".join(symbols) + r\" \\\\\",\n",
    "        r\"\\midrule\"\n",
    "    ]\n",
    "\n",
    "    def add_var(label, coef_col, se_col, pval_col):\n",
    "        coefs = []\n",
    "        ses   = []\n",
    "        for s in symbols:\n",
    "            row = idx.get(s)\n",
    "            if row is None:\n",
    "                coefs.append(\"\")\n",
    "                ses.append(\"()\")\n",
    "                continue\n",
    "            c = row.get(coef_col, np.nan) if coef_col else np.nan\n",
    "            p = row.get(pval_col, np.nan) if pval_col else np.nan\n",
    "            se = row.get(se_col, np.nan)  if se_col   else np.nan\n",
    "            coefs.append(f\"{fmt_num(c,3)}{starify(p)}\")\n",
    "            ses.append(f\"({fmt_num(se,3)})\" if not pd.isna(se) else \"()\")\n",
    "        lines.append(f\"{label} & \" + \" & \".join(coefs) + r\" \\\\\")\n",
    "        lines.append(\" & \" + \" & \".join(ses) + r\" \\\\\")\n",
    "        lines.append(r\"\\addlinespace\")\n",
    "\n",
    "    for label, cc, sc, pc in resolved:\n",
    "        add_var(label, cc, sc, pc)\n",
    "\n",
    "    # ---- Bottom metrics ----\n",
    "    def metric_row(name, colname, int_flag=False, decimals=2):\n",
    "        vals = []\n",
    "        for s in symbols:\n",
    "            row = idx.get(s)\n",
    "            if row is None or colname is None:\n",
    "                vals.append(\"\")\n",
    "            else:\n",
    "                vals.append(fmt_num(row.get(colname, np.nan), d=decimals, int_ok=int_flag))\n",
    "        lines.append(f\"{name} & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "\n",
    "    lines.append(r\"\\midrule\")\n",
    "    metric_row(\"N\", col_N, int_flag=True, decimals=0)\n",
    "    metric_row(\"Durbin-Watson\", col_DW, int_flag=False, decimals=2)\n",
    "    metric_row(\"F-stat\", col_F, int_flag=False, decimals=2)\n",
    "    metric_row(r\"\\(R^2\\)\", col_R2, int_flag=False, decimals=3)\n",
    "    metric_row(r\"Adj.\\ \\(R^2\\)\", col_AR2, int_flag=False, decimals=3)\n",
    "    length = len(symbols) + 1\n",
    "\n",
    "    lines += [\n",
    "        r\"\\addlinespace\",\n",
    "        r\"\\midrule\",\n",
    "        rf\"\\multicolumn{{{length}}}{{c}}{{Specification tested: $R_{{i,t+1}} = \\alpha_i + \\beta_{{sent}} S_t + \\phi R_{{i,t}} + \\theta B_{{i,t}} + \\gamma M_t + \\varepsilon_{{i,t+1}}$}}\\\\\",\n",
    "        r\"\\bottomrule\",\n",
    "        r\"\\end{tabular}\",\n",
    "        rf\"\\caption{{\\textbf{{Estimation Results from Predictive Regressions - {proxy}}} \\\\\",\n",
    "        r\"This table presents coefficient estimates and Newey--West standard errors (in parentheses) from predictive regressions of next-day returns on the selected proxy for investor sentiment. The regression specification is reported at the top of the table. Each column corresponds to a selected cryptocurrency, chosen to represent the full range of categories in the sample: Bitcoin (BTC), two high-cap altcoins, two mid-cap altcoins, two low-cap altcoins, one stablecoin, one gold-pegged token, and one meme coin. Each regression is estimated separately using the available time series data for the respective cryptocurrency and the sentiment proxy. The dependent variable is the next-day log return. Standard errors are computed using the Newey--West estimator with automatic lag selection. Statistical significance is denoted as follows: *$p<0.05$, **$p<0.01$.}\",\n",
    "        rf\"\\label{{tab:{proxy.lower()}_result_h1}}\",\n",
    "        r\"\\end{table}\"\n",
    "    ]\n",
    "    return proxy, lines\n",
    "\n",
    "# ===== Run over all CSVs =====\n",
    "for csv in sorted(Path(REGRESSIONS_DIR).glob(\"*.csv\")):\n",
    "    try:\n",
    "        proxy, lines = build_table_from_csv(csv)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {csv.name}: {e}\")\n",
    "        continue\n",
    "    out = Path(OUT_DIR) / f\"table_{proxy}.tex\"\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    print(f\"✅ {csv.name} -> {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b8c1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:98: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:120: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved violin_ConSIX.png\n",
      "[ok] Saved violin_EPU_DUS.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:98: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:120: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:98: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:120: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:98: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:120: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved violin_InvSIX.png\n",
      "[ok] Saved violin_TwitSIX.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:98: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:120: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:98: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:103: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/4200540918.py:120: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved violin_VIX.png\n",
      "[ok] Saved violin_fng_value.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================== GLOBAL STYLE ==================\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "plt.rcParams.update({\n",
    "    \"font.family\":       \"serif\",\n",
    "    \"font.serif\":        [\"DejaVu Serif\"],\n",
    "    \"axes.titlesize\":    22,\n",
    "    \"axes.labelsize\":    20,\n",
    "    \"axes.titlecolor\":   (38/255, 38/255, 38/255),\n",
    "    \"xtick.labelsize\":   13,\n",
    "    \"ytick.labelsize\":   13,\n",
    "    \"legend.fontsize\":   10,\n",
    "    \"axes.titleweight\":  \"normal\",\n",
    "    \"axes.edgecolor\":    \"black\",\n",
    "    \"axes.linewidth\":    0.5,\n",
    "    \"grid.color\":        \"0.85\",\n",
    "    \"grid.linestyle\":    \"-\",\n",
    "    \"grid.linewidth\":    0.5,\n",
    "    \"figure.dpi\":        300,\n",
    "})\n",
    "\n",
    "# ====== Paths ======\n",
    "REG_DIR = \"Regressions\"\n",
    "OUT_DIR = \"Regressions/Figures\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====== Classification ======\n",
    "def classify_symbol(symbol: str) -> str:\n",
    "    if pd.isna(symbol):\n",
    "        return \"Unclassified\"\n",
    "    if symbol == \"MTL_METAL\":\n",
    "        symbol = \"MTL\"\n",
    "\n",
    "    BTC = {\"BTC\"}\n",
    "    ALT_HIGH = {\"AAVE\",\"ADA\",\"ALGO\",\"ATOM\",\"AVAX\",\"BCH\",\"BNB\",\"BSV\",\"CRO\",\"CRV\",\"DOT\",\"ENS\",\"EOS\",\"ETC\",\n",
    "                \"ETH\",\"FIL\",\"FLOW\",\"GALA\",\"GRT\",\"ICP\",\"LDO\",\"LINK\",\"LTC\",\"MANA\",\"MKR\",\"QNT\",\"SAND\",\"SOL\",\n",
    "                \"TRX\",\"UNI\",\"VET\",\"XLM\",\"XMR\",\"XRP\",\"XTZ\",\"ZEC\"}\n",
    "    ALT_MID  = {\"1INCH\",\"ANT\",\"AUDIO\",\"BAT\",\"COMP\",\"CVC\",\"CVX\",\"DASH\",\"DCR\",\"DGB\",\"ELF\",\"ENJ\",\"FTT\",\"FXS\",\n",
    "                \"GAS\",\"GLM\",\"GNO\",\"GNT\",\"ICX\",\"LPT\",\"LRC\",\"LUNA\",\"NEO\",\"QTUM\",\"RSR\",\"SKL\",\"SNT\",\"SNX\",\n",
    "                \"SUSHI\",\"UMA\",\"WAVES\",\"XVG\",\"YFI\",\"ZIL\",\"ZRX\"}\n",
    "    ALT_LOW  = {\"ALCX\",\"ALPHA\",\"API3\",\"BADGER\",\"BAL\",\"BIT\",\"BNT\",\"BTG\",\"CEL\",\"DRGN\",\"FUN\",\"GRIN\",\"HEDG\",\"HT\",\n",
    "                \"KNC\",\"LEND\",\"LOOM\",\"LSK\",\"MAID\",\"MTL\",\"NMR\",\"OGN\",\"OMG\",\"PAY\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\n",
    "                \"QASH\",\"REN\",\"REP\",\"ROOK\",\"SRM\",\"STORJ\",\"SWRV\",\"VTC\",\"WNXM\",\"WTC\",\"XEM\"}\n",
    "    STABLE   = {\"USDT\",\"USDC\",\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\"}\n",
    "    GOLDPEG  = {\"PAXG\",\"XAUT\"}\n",
    "    MEME     = {\"DOGE\",\"SHIB\",\"APE\"}\n",
    "\n",
    "    if symbol in BTC:     return \"Bitcoin\"\n",
    "    if symbol in ALT_HIGH:return \"Altcoins (High)\"\n",
    "    if symbol in ALT_MID: return \"Altcoins (Mid)\"\n",
    "    if symbol in ALT_LOW: return \"Altcoins (Low)\"\n",
    "    if symbol in STABLE:  return \"Stablecoins\"\n",
    "    if symbol in GOLDPEG: return \"Gold-Pegged\"\n",
    "    if symbol in MEME:    return \"Meme Coins\"\n",
    "    return \"Unclassified\"\n",
    "\n",
    "GROUP_ORDER = [\"Bitcoin\",\"Altcoins (High)\",\"Altcoins (Mid)\",\"Altcoins (Low)\",\n",
    "               \"Stablecoins\",\"Gold-Pegged\",\"Meme Coins\"]\n",
    "\n",
    "GROUP_PALETTE = {\n",
    "    \"Bitcoin\": \"#4C72B0\",\n",
    "    \"Altcoins (High)\": \"#599da2\",\n",
    "    \"Altcoins (Mid)\":  \"#83a075\",\n",
    "    \"Altcoins (Low)\":  \"#aca24a\",\n",
    "    \"Stablecoins\":     \"#eb9681\",\n",
    "    \"Gold-Pegged\":     \"#d2a022\",\n",
    "    \"Meme Coins\":      \"#8C8C8C\"\n",
    "}\n",
    "\n",
    "# ====== Proxy detection ======\n",
    "def detect_proxy(df: pd.DataFrame):\n",
    "    cands = [c for c in df.columns if c.startswith(\"coef_\")\n",
    "             and c != \"coef_Intercept\"\n",
    "             and not c.startswith(\"coef_log_\")]\n",
    "    if not cands:\n",
    "        return None, None, None\n",
    "    proxy = cands[0].replace(\"coef_\", \"\")\n",
    "    coef_col = f\"coef_{proxy}\"\n",
    "    pval_col = f\"pval_{proxy}\" if f\"pval_{proxy}\" in df.columns else None\n",
    "    return proxy, coef_col, pval_col\n",
    "\n",
    "# ====== Violin plot generator ======\n",
    "def plot_violin(df, proxy, coef_col, pval_col):\n",
    "    # classify\n",
    "    df[\"Group\"] = df[\"symbol\"].map(classify_symbol)\n",
    "    df = df[df[\"Group\"].isin(GROUP_ORDER)]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.violinplot(\n",
    "        data=df, x=\"Group\", y=coef_col,\n",
    "        order=GROUP_ORDER, palette=GROUP_PALETTE,\n",
    "        inner=None, cut=2\n",
    "    )\n",
    "    sns.pointplot(\n",
    "        data=df, x=\"Group\", y=coef_col,\n",
    "        order=GROUP_ORDER, join=False, estimator=np.median,\n",
    "        color=\"black\", markers=\"_\", scale=1.3, errwidth=0\n",
    "    )\n",
    "\n",
    "    if pval_col and pval_col in df.columns:\n",
    "        sig = df[df[pval_col] < 0.05]\n",
    "        sns.stripplot(\n",
    "            data=sig, x=\"Group\", y=coef_col,\n",
    "            order=GROUP_ORDER, color=\"red\", size=4.5, jitter=True, alpha=0.85\n",
    "        )\n",
    "\n",
    "    ax.axhline(0, color=\"gray\", lw=1)\n",
    "    ax.set_title(rf\"Distribution of $\\beta_{{sent}}$ by Group — {proxy}\")\n",
    "    ax.set_ylabel(r\"Estimated $\\beta_{\\mathrm{sent}}$\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(OUT_DIR) / f\"violin_{proxy}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[ok] Saved violin_{proxy}.png\")\n",
    "\n",
    "# ====== Run for all ======\n",
    "for csv in sorted(glob.glob(os.path.join(REG_DIR, \"*.csv\"))):\n",
    "    df = pd.read_csv(csv)\n",
    "    if \"symbol\" not in df.columns:\n",
    "        continue\n",
    "    proxy, coef_col, pval_col = detect_proxy(df)\n",
    "    if proxy:\n",
    "        plot_violin(df, proxy, coef_col, pval_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45b8b3",
   "metadata": {},
   "source": [
    "### H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Groups present: ['Utility', 'Asset', 'Payment', 'Hybrid (U-A)', 'Hybrid (U-P)', 'Hybrid (A-P)']\n",
      "[INFO] Wrote results to Regressions/h2_functional_avg_then_transform_results.csv (rows=42)\n",
      "[INFO] Wrote functional membership table to Regressions/h2_effective_membership_overall_table.tex\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "proxy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "nobs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_regs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "regs_used",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "opt_hac_lag",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "r2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "adj_r2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "aic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "log_likelihood",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fstat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f_pval",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "durbin_watson",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_TotRes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_TotRes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_TotRes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_TotRes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_EPU_DUS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_EPU_DUS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_EPU_DUS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_EPU_DUS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_VIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_VIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_VIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_VIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_InvSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_InvSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_InvSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_InvSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_TwitSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_TwitSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_TwitSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_TwitSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_ConSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_ConSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_ConSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_ConSIX",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_fng_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_fng_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_fng_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_fng_value",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "0456f851-4872-4400-8279-70ee14af29d2",
       "rows": [
        [
         "0",
         "Asset",
         "ConSIX",
         "1922",
         "1922",
         "9",
         "R_g_t_lag,ConSIX,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.015748616104513946",
         "0.011115633648938972",
         "13794.078303254697",
         "13849.689519149999",
         "-6887.0391516273485",
         "0.5367176416827744",
         "0.8486000631279099",
         "2.017900029861347",
         "0.1916286688264612",
         "0.19916451412356448",
         "0.9621627109113026",
         "0.3360895407999305",
         "-0.15326296933102962",
         "0.15847509446234043",
         "-0.9671107617951324",
         "0.3336109765861289",
         "0.6065720828629493",
         "0.537597800534797",
         "1.1283009012677085",
         "0.25933441824835457",
         "0.16421737178502915",
         "0.10395802033501869",
         "1.5796508172800579",
         "0.11435226283554076",
         "-1.0855738519670892",
         "0.9554462751705377",
         "-1.1361955979924931",
         "0.25601710075112594",
         "-1.8768787997682015",
         "2.019535101300066",
         "-0.929361811319828",
         "0.3528189256600719",
         "-0.9960685665873994",
         "1.0250267325276405",
         "-0.9717488675940846",
         "0.3312984133124566",
         "-0.8078436755098543",
         "0.8873650700664044",
         "-0.9103848041363637",
         "0.3627343604161173",
         "0.03826761938722011",
         "0.08593422610103249",
         "0.44531289945206515",
         "0.6561440586188951",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.7888474167421535",
         "0.792340069913308",
         "0.9955919771020333",
         "0.3195744127138898",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "Asset",
         "EPU_DUS",
         "1922",
         "1922",
         "9",
         "R_g_t_lag,EPU_DUS,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.010151701082066977",
         "0.005492373315193921",
         "13804.976739225278",
         "13860.58795512058",
         "-6892.488369612639",
         "0.5694094642538783",
         "0.8230826415311081",
         "2.0062845079759337",
         "0.19162866882646035",
         "0.19972998280484847",
         "0.9594386688236802",
         "0.3374591093571545",
         "-0.08782112852333496",
         "0.09427026678754624",
         "-0.9315888404267966",
         "0.35166666502457233",
         "0.602804516487286",
         "0.535525072752344",
         "1.125632668119828",
         "0.26046230552750804",
         "0.1493624375075794",
         "0.08167317933441204",
         "1.828781966427592",
         "0.06758797535267536",
         "-1.0567712605554833",
         "0.930994458018716",
         "-1.135099410585577",
         "0.25647594185097644",
         "-1.3402581824786697",
         "1.4939281721634128",
         "-0.8971369624402973",
         "0.36975878287152597",
         "-1.1550251701836467",
         "1.1865981381804103",
         "-0.9733920297185202",
         "0.33048162603494013",
         "-0.5702609072290561",
         "0.6637109831659006",
         "-0.8592006486150225",
         "0.3903375312869064",
         "0.039005254103060084",
         "0.08785424063014147",
         "0.44397690792489725",
         "0.6571095196238907",
         "-0.19187051523151294",
         "0.20487310730195096",
         "-0.9365334365174917",
         "0.3491168792729681",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "Asset",
         "InvSIX",
         "1922",
         "1922",
         "9",
         "R_g_t_lag,InvSIX,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.012965574291939674",
         "0.008319491744150698",
         "13799.505228048156",
         "13855.116443943458",
         "-6889.752614024078",
         "0.7647603967791933",
         "0.6492940404181484",
         "2.0119550812178955",
         "0.19162866882645385",
         "0.19944589137814273",
         "0.9608052966262028",
         "0.33677156085754023",
         "-0.12108959898175764",
         "0.12000584423568914",
         "-1.0090308497304517",
         "0.31308751390931744",
         "0.610678757247558",
         "0.5412528451672126",
         "1.128268909254227",
         "0.2593479214900731",
         "0.1520626519638385",
         "0.08561664081062077",
         "1.7760875750800897",
         "0.07587748443789603",
         "-1.074271628583649",
         "0.9438212081916694",
         "-1.138215182345731",
         "0.2551732400762742",
         "-1.776403983707192",
         "1.862870204673985",
         "-0.9535844092895753",
         "0.34041458721533735",
         "-1.2112990906104757",
         "1.206066727840055",
         "-1.0043383692209065",
         "0.3153426302977528",
         "-0.9343314542555143",
         "0.9809900729455652",
         "-0.9524372162605564",
         "0.34099567791149443",
         "-0.021892241434820037",
         "0.08053252062656979",
         "-0.2718434896175001",
         "0.7857717133487976",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.5204547430034923",
         "0.4267588317246158",
         "1.2195523661460805",
         "0.22278502988447546",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "Asset",
         "TwitSIX",
         "1239",
         "1239",
         "9",
         "R_g_t_lag,TwitSIX,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.01921254784049342",
         "0.012030214993108967",
         "9421.211058657129",
         "9472.43165747342",
         "-4700.605529328564",
         "0.5086526118188233",
         "0.8690906190709513",
         "2.0132377250259497",
         "0.29807899909840563",
         "0.30665065704187916",
         "0.9720474822191465",
         "0.33121819587456336",
         "-0.21421984568901437",
         "0.214136650426443",
         "-1.0003885148217537",
         "0.31731944659237593",
         "0.9794650070303095",
         "0.8972506513588683",
         "1.0916291958628608",
         "0.2752100707931119",
         "0.22313129828196684",
         "0.1509313573332975",
         "1.4783627618827562",
         "0.13956693862688996",
         "-1.7003843640804177",
         "1.5201811358540964",
         "-1.118540629123829",
         "0.26355474259006306",
         "-2.68165107983565",
         "2.7844028018035973",
         "-0.9630973931281099",
         "0.33568810358907697",
         "-0.4540570193432581",
         "0.5305172148442269",
         "-0.8558761273686106",
         "0.39223313097912693",
         "-2.599574302881559",
         "2.6161514090010045",
         "-0.9936635524754374",
         "0.320582285794543",
         "-0.5319818389261569",
         "0.5407078045344188",
         "-0.983861957354628",
         "0.32537703370381577",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.2305190486496263",
         "0.3221318506928658",
         "0.7156046449731944",
         "0.47437143359716794",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "Asset",
         "VIX",
         "1922",
         "1922",
         "9",
         "R_g_t_lag,VIX,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.013934955688585826",
         "0.00929343612854261",
         "13797.616675319245",
         "13853.227891214547",
         "-6888.808337659622",
         "0.5437673054418634",
         "0.8432276001441494",
         "2.017047763132956",
         "0.19162866882644777",
         "0.1993479279103234",
         "0.9612774551268567",
         "0.33653422828940305",
         "-0.13580021828884997",
         "0.14186270764812267",
         "-0.957265094824567",
         "0.33855448972366253",
         "0.6148130618834085",
         "0.545412797342413",
         "1.1272435573187067",
         "0.2597809618593399",
         "0.1535693709215248",
         "0.09114912641405656",
         "1.68481451181349",
         "0.0921875606161656",
         "-1.0585425303501932",
         "0.9289222980842018",
         "-1.139538293497011",
         "0.2546214433228284",
         "-1.7166371836179144",
         "1.8650336700840175",
         "-0.9204322748449801",
         "0.3574630211932669",
         "-1.4555648235082228",
         "1.488891357657879",
         "-0.9776165440290546",
         "0.3283876861036269",
         "-0.9328933033064833",
         "1.0156768878103464",
         "-0.9184941731987891",
         "0.3584760583595822",
         "-0.03262200238193465",
         "0.0881155330568043",
         "-0.3702185216414074",
         "0.7112606825491938",
         null,
         null,
         null,
         null,
         "-0.6943018040153082",
         "0.7119413774610802",
         "-0.9752232782020928",
         "0.3295728823900467",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 75,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>proxy</th>\n",
       "      <th>n</th>\n",
       "      <th>nobs</th>\n",
       "      <th>n_regs</th>\n",
       "      <th>regs_used</th>\n",
       "      <th>opt_hac_lag</th>\n",
       "      <th>r2</th>\n",
       "      <th>adj_r2</th>\n",
       "      <th>aic</th>\n",
       "      <th>...</th>\n",
       "      <th>tval_TwitSIX</th>\n",
       "      <th>pval_TwitSIX</th>\n",
       "      <th>coef_ConSIX</th>\n",
       "      <th>stderr_ConSIX</th>\n",
       "      <th>tval_ConSIX</th>\n",
       "      <th>pval_ConSIX</th>\n",
       "      <th>coef_fng_value</th>\n",
       "      <th>stderr_fng_value</th>\n",
       "      <th>tval_fng_value</th>\n",
       "      <th>pval_fng_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asset</td>\n",
       "      <td>ConSIX</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>9</td>\n",
       "      <td>R_g_t_lag,ConSIX,log_diff_AdrActCnt,log_diff_v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015749</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>13794.078303</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.788847</td>\n",
       "      <td>0.79234</td>\n",
       "      <td>0.995592</td>\n",
       "      <td>0.319574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asset</td>\n",
       "      <td>EPU_DUS</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>9</td>\n",
       "      <td>R_g_t_lag,EPU_DUS,log_diff_AdrActCnt,log_diff_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.005492</td>\n",
       "      <td>13804.976739</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asset</td>\n",
       "      <td>InvSIX</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>9</td>\n",
       "      <td>R_g_t_lag,InvSIX,log_diff_AdrActCnt,log_diff_v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012966</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>13799.505228</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asset</td>\n",
       "      <td>TwitSIX</td>\n",
       "      <td>1239</td>\n",
       "      <td>1239</td>\n",
       "      <td>9</td>\n",
       "      <td>R_g_t_lag,TwitSIX,log_diff_AdrActCnt,log_diff_...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>9421.211059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715605</td>\n",
       "      <td>0.474371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asset</td>\n",
       "      <td>VIX</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>9</td>\n",
       "      <td>R_g_t_lag,VIX,log_diff_AdrActCnt,log_diff_volu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.009293</td>\n",
       "      <td>13797.616675</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   group    proxy     n  nobs  n_regs  \\\n",
       "0  Asset   ConSIX  1922  1922       9   \n",
       "1  Asset  EPU_DUS  1922  1922       9   \n",
       "2  Asset   InvSIX  1922  1922       9   \n",
       "3  Asset  TwitSIX  1239  1239       9   \n",
       "4  Asset      VIX  1922  1922       9   \n",
       "\n",
       "                                           regs_used  opt_hac_lag        r2  \\\n",
       "0  R_g_t_lag,ConSIX,log_diff_AdrActCnt,log_diff_v...            0  0.015749   \n",
       "1  R_g_t_lag,EPU_DUS,log_diff_AdrActCnt,log_diff_...            0  0.010152   \n",
       "2  R_g_t_lag,InvSIX,log_diff_AdrActCnt,log_diff_v...            0  0.012966   \n",
       "3  R_g_t_lag,TwitSIX,log_diff_AdrActCnt,log_diff_...            0  0.019213   \n",
       "4  R_g_t_lag,VIX,log_diff_AdrActCnt,log_diff_volu...            0  0.013935   \n",
       "\n",
       "     adj_r2           aic  ...  tval_TwitSIX  pval_TwitSIX  coef_ConSIX  \\\n",
       "0  0.011116  13794.078303  ...           NaN           NaN     0.788847   \n",
       "1  0.005492  13804.976739  ...           NaN           NaN          NaN   \n",
       "2  0.008319  13799.505228  ...           NaN           NaN          NaN   \n",
       "3  0.012030   9421.211059  ...      0.715605      0.474371          NaN   \n",
       "4  0.009293  13797.616675  ...           NaN           NaN          NaN   \n",
       "\n",
       "   stderr_ConSIX  tval_ConSIX  pval_ConSIX  coef_fng_value  stderr_fng_value  \\\n",
       "0        0.79234     0.995592     0.319574             NaN               NaN   \n",
       "1            NaN          NaN          NaN             NaN               NaN   \n",
       "2            NaN          NaN          NaN             NaN               NaN   \n",
       "3            NaN          NaN          NaN             NaN               NaN   \n",
       "4            NaN          NaN          NaN             NaN               NaN   \n",
       "\n",
       "   tval_fng_value  pval_fng_value  \n",
       "0             NaN             NaN  \n",
       "1             NaN             NaN  \n",
       "2             NaN             NaN  \n",
       "3             NaN             NaN  \n",
       "4             NaN             NaN  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# -------------------- TOGGLES --------------------\n",
    "USE_JUMPS        = False  # include jump_* dummies as controls\n",
    "STANDARDIZE_CONT = True   # z-score continuous regressors only (not jumps)\n",
    "VERBOSE          = True\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATE_COL   = \"date\"\n",
    "SYMBOL_COL = \"symbol\"\n",
    "RET_COL    = \"log_daily_return\"   # already in df_crypto\n",
    "TARGET_COL = \"R_g_t_plus1\"        # target built below\n",
    "\n",
    "# Raw blockchain columns to average at group level\n",
    "BASE_BLOCKCHAIN = [\"TxCnt\", \"AdrActCnt\", \"volume_trusted_spot_usd_1d\"]\n",
    "\n",
    "# Macro (log levels in df) and sentiments\n",
    "MACRO_COLS     = [\"log_UnemRt\", \"log_IndPro\", \"log_CPIPrc\", \"log_TotRes\"]\n",
    "SENTIMENT_COLS = [\"VIX\", \"TwitSIX\", \"EPU_DUS\", \"InvSIX\", \"fng_value\", \"ConSIX\"]\n",
    "\n",
    "# Regression controls\n",
    "MIN_OBS_FLOOR = 365    # minimum sample length\n",
    "MARGIN_OBS    = 30     # buffer above number of parameters\n",
    "\n",
    "# Outputs (ONLY these two)\n",
    "EXPORT_DIR     = Path(\"Regressions\")\n",
    "RESULTS_CSV    = EXPORT_DIR / \"h2_functional_avg_then_transform_results.csv\"\n",
    "FUNC_TABLE_TEX = EXPORT_DIR / \"h2_effective_membership_overall_table.tex\"\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ---------- Functional groups (as per your taxonomy) ----------\n",
    "UTILITY = {\"1INCH\",\"AAVE\",\"ANT\",\"APE\",\"BADGER\",\"BAL\",\"BAT\",\"BIT\",\"CEL\",\"COMP\",\"CRV\",\"DCR\",\"ENS\",\"FXS\",\n",
    "           \"HEDG\",\"LDO\",\"MKR\",\"QNT\",\"ROOK\",\"SUSHI\",\"SWRV\",\"UNI\",\"YFI\"}\n",
    "ASSET   = {\"ALCX\",\"ALPHA\",\"CVX\",\"PAXG\",\"XAUT\"}\n",
    "PAYMENT = {\"BCH\",\"BNB\",\"BSV\",\"BTC\",\"BTG\",\"CRO\",\"DASH\",\"DGB\",\"DOGE\",\"ETC\",\"ETH\",\"FTT\",\"GRIN\",\"HT\",\"LTC\",\n",
    "           \"MTL_METAL\",\"PAY\",\"SHIB\",\"VTC\",\"XLM\",\"XMR\",\"XRP\",\"XVG\",\"ZEC\"}\n",
    "HYBRID_UA = {\"CVC\"}\n",
    "HYBRID_UP = {\"ADA\",\"ALGO\",\"API3\",\"ATOM\",\"AUDIO\",\"AVAX\",\"BNT\",\"DOT\",\"DRGN\",\"ELF\",\"ENJ\",\"EOS\",\"FIL\",\"FLOW\",\n",
    "             \"FUN\",\"GALA\",\"GAS\",\"GLM\",\"GNO\",\"GNT\",\"GRT\",\"ICP\",\"ICX\",\"KNC\",\"LEND\",\"LINK\",\"LOOM\",\"LPT\",\"LRC\",\n",
    "             \"LSK\",\"LUNA\",\"MAID\",\"MANA\",\"NEO\",\"NMR\",\"OGN\",\"OMG\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\"QASH\",\"QTUM\",\n",
    "             \"REN\",\"REP\",\"RSR\",\"SAND\",\"SKL\",\"SNT\",\"SNX\",\"SOL\",\"SRM\",\"STORJ\",\"TRX\",\"UMA\",\"VET\",\"WAVES\",\n",
    "             \"WNXM\",\"WTC\",\"XEM\",\"XTZ\",\"ZIL\",\"ZRX\"}\n",
    "HYBRID_AP = {\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\",\"USDC\",\"USDT\"}\n",
    "\n",
    "GROUP_ORDER = [\"Utility\", \"Asset\", \"Payment\", \"Hybrid (U-A)\", \"Hybrid (U-P)\", \"Hybrid (A-P)\"]\n",
    "\n",
    "def map_functional_group(sym: str) -> str:\n",
    "    s = str(sym).upper()\n",
    "    if s in UTILITY:   return \"Utility\"\n",
    "    if s in ASSET:     return \"Asset\"\n",
    "    if s in PAYMENT:   return \"Payment\"\n",
    "    if s in HYBRID_UA: return \"Hybrid (U-A)\"\n",
    "    if s in HYBRID_UP: return \"Hybrid (U-P)\"\n",
    "    if s in HYBRID_AP: return \"Hybrid (A-P)\"\n",
    "    return \"Unclassified\"\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def min_required_n(regressors: List[str], floor: int = MIN_OBS_FLOOR, margin: int = MARGIN_OBS) -> int:\n",
    "    p = 1 + len(regressors)  # + intercept\n",
    "    return max(floor, p + margin)\n",
    "\n",
    "def group_eligible(sub: pd.DataFrame, target: str, regressors: List[str]) -> Tuple[bool, str]:\n",
    "    if any(r not in sub.columns for r in regressors):\n",
    "        return False, \"missing_columns\"\n",
    "    cc = sub.dropna(subset=[target])\n",
    "    if len(cc) < min_required_n(regressors):\n",
    "        return False, f\"too_few_obs:{len(cc)}\"\n",
    "    # Require variation for continuous vars (allow jump_* to be constant)\n",
    "    for r in regressors:\n",
    "        if not r.startswith(\"jump_\") and r in cc.columns and cc[r].nunique() < 2:\n",
    "            return False, f\"no_variation:{r}\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "def select_hac_lag_via_resid_aic(resid: pd.Series, kmax: int) -> int:\n",
    "    n = resid.shape[0]\n",
    "    if n < 20 or kmax <= 0:\n",
    "        return 0\n",
    "    best_k, best_aic = 0, np.inf\n",
    "    for k in range(kmax + 1):\n",
    "        try:\n",
    "            if k == 0:\n",
    "                e = resid - resid.mean()\n",
    "                sigma2 = np.var(e, ddof=1)\n",
    "                aic = n * np.log(sigma2 + 1e-12) + 2\n",
    "            else:\n",
    "                ar = AutoReg(resid, lags=k, old_names=False, trend=\"c\").fit()\n",
    "                aic = ar.aic\n",
    "        except Exception:\n",
    "            aic = np.inf\n",
    "        if aic < best_aic:\n",
    "            best_aic, best_k = aic, k\n",
    "    return int(best_k)\n",
    "\n",
    "def _tex_escape_ticker(s: str) -> str:\n",
    "    return str(s).replace(\"_\", r\"\\_\")\n",
    "\n",
    "# ---------- Build group panel (avg raw → transform) + capture daily contributors ----------\n",
    "def make_group_panel_avg_then_transform_with_members(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      grp : group panel with R_g_t, R_g_t_lag, R_g_t_plus1 and group-level log_diffs/jumps\n",
    "      mem_day : long DataFrame with [date, h2_group, symbol] listing daily contributors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[DATE_COL]   = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "    df[SYMBOL_COL] = df[SYMBOL_COL].astype(str).str.upper()\n",
    "    df[\"h2_group\"] = df[SYMBOL_COL].map(map_functional_group)\n",
    "    df = df[df[\"h2_group\"].isin(GROUP_ORDER)].copy()\n",
    "\n",
    "    # Eligible rows for raw group means: need raw blockchain cols present that day\n",
    "    elig = df.dropna(subset=BASE_BLOCKCHAIN).copy()\n",
    "\n",
    "    # (A) daily contributor list per (date, group)\n",
    "    mem_day = (elig.loc[:, [DATE_COL, \"h2_group\", SYMBOL_COL]]\n",
    "                    .drop_duplicates()\n",
    "                    .rename(columns={SYMBOL_COL: \"symbol\"})\n",
    "                    .sort_values([\"h2_group\", DATE_COL, \"symbol\"])\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "    # (B) group means of RAW blockchain + returns\n",
    "    agg_cols = [RET_COL] + BASE_BLOCKCHAIN\n",
    "    grp_raw = (elig.groupby([DATE_COL, \"h2_group\"], dropna=False)[agg_cols]\n",
    "                    .mean(numeric_only=True)\n",
    "                    .reset_index()\n",
    "                    .sort_values([\"h2_group\", DATE_COL]))\n",
    "\n",
    "    # (C) zero-safe log-diffs (+ optional jump dummies) on group averages\n",
    "    for col in BASE_BLOCKCHAIN:\n",
    "        prev = grp_raw.groupby(\"h2_group\")[col].shift(1)\n",
    "        curr = grp_raw[col]\n",
    "        if USE_JUMPS:\n",
    "            grp_raw[f\"jump_{col}\"] = ((prev == 0) & (curr > 0)).astype(int)\n",
    "        grp_raw[f\"log_diff_{col}\"] = np.where((prev > 0) & (curr > 0), np.log(curr / prev), 0.0)\n",
    "\n",
    "    # (D) attach date-level macro + sentiment (unique per date)\n",
    "    date_cols = [DATE_COL] + [c for c in (MACRO_COLS + SENTIMENT_COLS) if c in df.columns]\n",
    "    date_level = df[date_cols].drop_duplicates(subset=[DATE_COL]).sort_values(DATE_COL)\n",
    "    grp = grp_raw.merge(date_level, on=DATE_COL, how=\"left\")\n",
    "\n",
    "    # (E) target and lags\n",
    "    grp[\"R_g_t\"]       = grp.groupby(\"h2_group\")[RET_COL].shift(0)\n",
    "    grp[\"R_g_t_lag\"]   = grp.groupby(\"h2_group\")[RET_COL].shift(1)\n",
    "    grp[\"R_g_t_plus1\"] = grp.groupby(\"h2_group\")[RET_COL].shift(-1)\n",
    "\n",
    "    # (F) drop missing edges\n",
    "    grp = grp.dropna(subset=[\"R_g_t_lag\", \"R_g_t_plus1\"]).reset_index(drop=True)\n",
    "    return grp, mem_day\n",
    "\n",
    "# ---------- Effective membership utilities ----------\n",
    "def build_effective_membership(mem_day: pd.DataFrame,\n",
    "                               used_dates: pd.Index,\n",
    "                               group_name: str) -> pd.DataFrame:\n",
    "    sub = mem_day[(mem_day[\"h2_group\"] == group_name) & (mem_day[DATE_COL].isin(used_dates))].copy()\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=[\"group\",\"symbol\",\"contrib_days\"])\n",
    "    out = (sub.groupby([\"h2_group\",\"symbol\"], as_index=False)\n",
    "               .agg(contrib_days=(\"symbol\",\"size\"))\n",
    "               .rename(columns={\"h2_group\":\"group\"}))\n",
    "    return out\n",
    "\n",
    "def summarize_membership(df_long: pd.DataFrame,\n",
    "                         group_order=GROUP_ORDER) -> pd.DataFrame:\n",
    "    if df_long.empty:\n",
    "        return pd.DataFrame(columns=[\"Token Type\",\"Ticker\",\"# Coins\"])\n",
    "    tmp = (df_long.groupby(\"group\", as_index=False)\n",
    "                 .agg(symbols=(\"symbol\", lambda s: sorted(set(s))),\n",
    "                      n_coins=(\"symbol\", \"nunique\")))\n",
    "    tmp[\"Ticker\"] = tmp[\"symbols\"].apply(lambda lst: \", \".join(_tex_escape_ticker(s) for s in lst))\n",
    "    tmp = tmp.drop(columns=[\"symbols\"]).rename(columns={\"group\":\"Token Type\", \"n_coins\":\"# Coins\"})\n",
    "    tmp[\"Token Type\"] = pd.Categorical(tmp[\"Token Type\"], categories=group_order, ordered=True)\n",
    "    return tmp.sort_values(\"Token Type\").reset_index(drop=True)\n",
    "\n",
    "def render_latex_functional_table(df_tbl: pd.DataFrame,\n",
    "                                  caption: str,\n",
    "                                  label: str,\n",
    "                                  colwidths=(\"3cm\",\"10cm\",\"1.2cm\")) -> str:\n",
    "    lines = []\n",
    "    lines += [r\"\\begin{table}[ht]\",\n",
    "              r\"\\centering\",\n",
    "              r\"\\scriptsize\",\n",
    "              r\"\\setlength{\\tabcolsep}{4pt}\",\n",
    "              rf\"\\begin{{tabular}}{{p{{{colwidths[0]}}}p{{{colwidths[1]}}}c}}\",\n",
    "              r\"\\toprule\",\n",
    "              r\"\\textbf{Token Type} & \\textbf{Ticker} & \\textbf{\\# Coins} \\\\\",\n",
    "              r\"\\midrule\"]\n",
    "    total = 0\n",
    "    for _, row in df_tbl.iterrows():\n",
    "        lines.append(f\"{row['Token Type']} & {row['Ticker']} & {int(row['# Coins'])} \\\\\\\\\")\n",
    "        lines.append(r\"\\addlinespace\")\n",
    "        total += int(row['# Coins'])\n",
    "    lines += [rf\"\\textbf{{Total:}} & & \\textbf{{{total}}} \\\\\",\n",
    "              r\"\\bottomrule\",\n",
    "              r\"\\end{tabular}\",\n",
    "              rf\"\\caption{{\\textbf{{{caption}}}}}\",\n",
    "              rf\"\\label{{{label}}}\",\n",
    "              r\"\\end{table}\"]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- One regression (group × proxy) that returns H1-style row ----------\n",
    "def run_group_proxy_regression(dfg: pd.DataFrame,\n",
    "                               group_name: str,\n",
    "                               proxy_label: Optional[str],\n",
    "                               regressors: List[str]) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    dfg must contain: TARGET_COL and regressors (already cleaned & standardized if needed)\n",
    "    Returns one row with H1-like columns: n, nobs, n_regs, regs_used, opt_hac_lag, r2, adj_r2, aic, bic,\n",
    "    log_likelihood, fstat, f_pval, durbin_watson, and per-parameter coef_*, stderr_*, tval_*, pval_*.\n",
    "    \"\"\"\n",
    "    rhs = \" + \".join(regressors)\n",
    "    fit = smf.ols(f\"{TARGET_COL} ~ {rhs}\", data=dfg).fit()\n",
    "    n = int(fit.nobs)\n",
    "    if n <= 8:\n",
    "        return None\n",
    "\n",
    "    # HAC lag selection via AR(k)-AIC\n",
    "    kmax = int(n ** 0.25)\n",
    "    best_aic, best_lag, best_res = np.inf, 0, None\n",
    "    for lag in range(kmax + 1):\n",
    "        r = fit.get_robustcov_results(cov_type=\"HAC\", maxlags=lag, use_correction=True)\n",
    "        if r.aic < best_aic:\n",
    "            best_aic, best_lag, best_res = r.aic, lag, r\n",
    "\n",
    "    # Build output row mirroring H1 style\n",
    "    row = {\n",
    "        \"group\": group_name,\n",
    "        \"proxy\": proxy_label if proxy_label is not None else \"no_sentiment\",\n",
    "        \"n\": n,                        # H1 included both n and nobs\n",
    "        \"nobs\": int(best_res.nobs),\n",
    "        \"n_regs\": len(regressors),\n",
    "        \"regs_used\": \",\".join(regressors),\n",
    "        \"opt_hac_lag\": int(best_lag),\n",
    "        \"r2\": float(best_res.rsquared),\n",
    "        \"adj_r2\": float(best_res.rsquared_adj),\n",
    "        \"aic\": float(best_res.aic),\n",
    "        \"bic\": float(best_res.bic),\n",
    "        \"log_likelihood\": float(best_res.llf),\n",
    "        \"fstat\": float(best_res.fvalue) if best_res.fvalue is not None else np.nan,\n",
    "        \"f_pval\": float(best_res.f_pvalue) if best_res.f_pvalue is not None else np.nan,\n",
    "        \"durbin_watson\": float(durbin_watson(fit.resid)),\n",
    "    }\n",
    "\n",
    "    # Add per-parameter columns, as in H1\n",
    "    for name, b, se, t, p in zip(best_res.model.exog_names,\n",
    "                                 best_res.params, best_res.bse, best_res.tvalues, best_res.pvalues):\n",
    "        row[f\"coef_{name}\"]   = float(b)\n",
    "        row[f\"stderr_{name}\"] = float(se)\n",
    "        row[f\"tval_{name}\"]   = float(t)\n",
    "        row[f\"pval_{name}\"]   = float(p)\n",
    "\n",
    "    return row\n",
    "\n",
    "# ---------- Driver: run all proxies, write one CSV + one LaTeX table ----------\n",
    "def run_h2_write_only_results_and_func_table(df_crypto: pd.DataFrame,\n",
    "                                             export_dir: Path = EXPORT_DIR,\n",
    "                                             verbose: bool = VERBOSE) -> pd.DataFrame:\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Build group panel and daily membership (to know who actually contributes)\n",
    "    grp, mem_day = make_group_panel_avg_then_transform_with_members(df_crypto)\n",
    "\n",
    "    # 2) Regressors lists\n",
    "    base_cont = [\"R_g_t_lag\",\n",
    "                 \"log_diff_AdrActCnt\", \"log_diff_volume_trusted_spot_usd_1d\", \"log_diff_TxCnt\"]\n",
    "    base_jumps = ([\"jump_AdrActCnt\", \"jump_volume_trusted_spot_usd_1d\", \"jump_TxCnt\"] if USE_JUMPS else [])\n",
    "    macro      = [c for c in MACRO_COLS if c in grp.columns]\n",
    "\n",
    "    proxy_dict = {\n",
    "        \"no_sentiment\": None,\n",
    "        \"EPU_DUS\":      \"EPU_DUS\",\n",
    "        \"VIX\":          \"VIX\",\n",
    "        \"InvSIX\":       \"InvSIX\",\n",
    "        \"TwitSIX\":      \"TwitSIX\",\n",
    "        \"ConSIX\":       \"ConSIX\",\n",
    "        \"fng_value\":    \"fng_value\"\n",
    "    }\n",
    "\n",
    "    results_rows = []\n",
    "    overall_members = []  # collect by (group, symbol) across all successful regressions\n",
    "\n",
    "    present_groups = [g for g in GROUP_ORDER if g in grp[\"h2_group\"].unique()]\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Groups present: {present_groups}\")\n",
    "\n",
    "    for label, proxy in proxy_dict.items():\n",
    "        for g in present_groups:\n",
    "            sub = grp[grp[\"h2_group\"] == g].copy()\n",
    "\n",
    "            # Assemble RHS\n",
    "            regressors = base_cont.copy()\n",
    "            if proxy is not None and proxy in sub.columns:\n",
    "                regressors.insert(1, proxy)  # sentiment after lagged return\n",
    "            regressors += base_jumps + macro\n",
    "\n",
    "            # Keep only needed cols + date for membership\n",
    "            cols_needed = [TARGET_COL] + regressors\n",
    "            dfg = sub[cols_needed + [DATE_COL]].copy()\n",
    "\n",
    "            # Coerce numeric & drop non-finite\n",
    "            for c in cols_needed:\n",
    "                dfg[c] = pd.to_numeric(dfg[c], errors=\"coerce\")\n",
    "            dfg = dfg.replace([np.inf, -np.inf], np.nan).dropna(subset=cols_needed)\n",
    "\n",
    "            # Eligibility\n",
    "            ok, reason = group_eligible(dfg, TARGET_COL, regressors)\n",
    "            if not ok:\n",
    "                if verbose: print(f\"[SKIP] {g} × {proxy or 'no_sentiment'} — {reason}\")\n",
    "                continue\n",
    "\n",
    "            # Standardize continuous (not jumps)\n",
    "            cont = [r for r in regressors if not r.startswith(\"jump_\")]\n",
    "            if STANDARDIZE_CONT and cont:\n",
    "                dfg[cont] = StandardScaler().fit_transform(dfg[cont])\n",
    "\n",
    "            # Run regression and append H1-style row\n",
    "            row = run_group_proxy_regression(dfg, g, proxy, regressors)\n",
    "            if row is None:\n",
    "                if verbose: print(f\"[SKIP] {g} × {proxy or 'no_sentiment'} — too few nobs after clean\")\n",
    "                continue\n",
    "            results_rows.append(row)\n",
    "\n",
    "            # Track effective membership for the dates actually used in this regression\n",
    "            used_dates = dfg[DATE_COL]\n",
    "            eff = build_effective_membership(mem_day, used_dates, g)\n",
    "            if not eff.empty:\n",
    "                overall_members.append(eff)\n",
    "\n",
    "    # 3) Write the single results CSV\n",
    "    results_df = pd.DataFrame(results_rows).sort_values([\"group\",\"proxy\"]).reset_index(drop=True)\n",
    "    results_df.to_csv(RESULTS_CSV, index=False)\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Wrote results to {RESULTS_CSV} (rows={len(results_df)})\")\n",
    "\n",
    "    # 4) Build and write ONLY the functional membership LaTeX table (overall union)\n",
    "    if overall_members:\n",
    "        overall_long = pd.concat(overall_members, ignore_index=True)\n",
    "        # Union across proxies by (group, symbol)\n",
    "        overall_union = overall_long.drop_duplicates(subset=[\"group\",\"symbol\"])\n",
    "        overall_wide  = summarize_membership(overall_union)\n",
    "        latex_code = render_latex_functional_table(\n",
    "            overall_wide,\n",
    "            caption=\"Functional Groups Used in the Final Regression Sample\",\n",
    "            label=\"tab:functional_groups_effective_sample\"\n",
    "        )\n",
    "        with open(FUNC_TABLE_TEX, \"w\") as f:\n",
    "            f.write(latex_code)\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Wrote functional membership table to {FUNC_TABLE_TEX}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"[WARN] No functional membership captured (no regressions passed eligibility).\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "# df_crypto must already be loaded & pre-engineered as in your earlier prep\n",
    "results_h2 = run_h2_write_only_results_and_func_table(df_crypto)\n",
    "results_h2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618af9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wrote 7 tables:\n",
      " - Regressions/Tables/h2_functional_results_ConSIX.tex\n",
      " - Regressions/Tables/h2_functional_results_EPU_DUS.tex\n",
      " - Regressions/Tables/h2_functional_results_InvSIX.tex\n",
      " - Regressions/Tables/h2_functional_results_TwitSIX.tex\n",
      " - Regressions/Tables/h2_functional_results_VIX.tex\n",
      " - Regressions/Tables/h2_functional_results_fng_value.tex\n",
      " - Regressions/Tables/h2_functional_results_no_sentiment.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:78: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:79: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:82: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:97: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:98: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:78: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:79: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:82: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:97: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:98: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/2154738755.py:78: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  (\"$\\Delta\\log\\ $ TxCnt\",              \"coef_log_diff_TxCnt\",           \"stderr_log_diff_TxCnt\",           \"pval_log_diff_TxCnt\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/2154738755.py:79: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  (\"$\\Delta\\log\\ $ Volume\",             \"coef_log_diff_volume_trusted_spot_usd_1d\",\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/2154738755.py:82: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  (\"$\\Delta\\log\\ $ AdrActCnt\",   \"coef_log_diff_AdrActCnt\",       \"stderr_log_diff_AdrActCnt\",       \"pval_log_diff_AdrActCnt\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/2154738755.py:97: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $ UnemRt\",             \"coef_log_UnemRt\",               \"stderr_log_UnemRt\",               \"pval_log_UnemRt\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/2154738755.py:98: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $ IndPro\",             \"coef_log_IndPro\",               \"stderr_log_IndPro\",               \"pval_log_IndPro\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/2154738755.py:99: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $ CPIPrc\",             \"coef_log_CPIPrc\",               \"stderr_log_CPIPrc\",               \"pval_log_CPIPrc\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_7306/2154738755.py:100: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $ TotRes\",             \"coef_log_TotRes\",               \"stderr_log_TotRes\",               \"pval_log_TotRes\"),\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Build LaTeX tables (functional groups as columns) from H2 CSV\n",
    "# - Uses the new combined results file with coef/SE/t/p\n",
    "# - Runs for ALL proxies found in the results file\n",
    "# - Auto-detects optional jump_* rows if present\n",
    "# - Writes one .tex per proxy to Reg./Tables/\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Paths ----\n",
    "RESULTS_CSV = Path(\"Regressions/h2_functional_avg_then_transform_results.csv\")\n",
    "OUT_DIR     = Path(\"Regressions/Tables\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Functional group column order (match H2 runs) ----\n",
    "GROUP_ORDER = [\"Utility\", \"Asset\", \"Payment\", \"Hybrid (U-A)\", \"Hybrid (U-P)\", \"Hybrid (A-P)\"]\n",
    "\n",
    "# ---- Significance thresholds ----\n",
    "# ** for p<0.01; * for p<0.05 (matches your earlier tables)\n",
    "STAR_THRESHOLDS = [(0.01, \"**\"), (0.05, \"*\")]\n",
    "\n",
    "# ---- Formatting helpers ----\n",
    "def star_for_p(p):\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    for thr, mark in STAR_THRESHOLDS:\n",
    "        if p < thr:\n",
    "            return mark\n",
    "    return \"\"\n",
    "\n",
    "def fmt_coef(c, p):\n",
    "    if c is None or (isinstance(c, float) and (np.isnan(c) or np.isinf(c))):\n",
    "        return \"\"\n",
    "    return f\"{float(c):.3f}{star_for_p(p)}\"\n",
    "\n",
    "def fmt_se(se):\n",
    "    if se is None or (isinstance(se, float) and (np.isnan(se) or np.isinf(se))):\n",
    "        return \"\"\n",
    "    return f\"({float(se):.3f})\"\n",
    "\n",
    "def fmt_stat(x, ints=False):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    if ints:\n",
    "        try:\n",
    "            return f\"{int(x)}\"\n",
    "        except Exception:\n",
    "            return f\"{x}\"\n",
    "    return f\"{float(x):.2f}\"\n",
    "\n",
    "def header_line(num_cols:int) -> str:\n",
    "    # Safer multi-part build to avoid truncation issues\n",
    "    num_cols = num_cols + 1\n",
    "    return (\n",
    "        \"\\\\multicolumn{\" + str(num_cols) + \"}{c}{Specification tested: $\"\n",
    "        \"R_{g,t+1} = \"\n",
    "        \"\\\\alpha_g + \"\n",
    "        \"\\\\beta_{sent,g} S_t + \"\n",
    "        \"\\\\phi R_{g,t} + \"\n",
    "        \"\\\\theta B_{g,t} + \"\n",
    "        \"\\\\gamma M_t + \"\n",
    "        \"\\\\varepsilon_{g,t+1}$}\\\\\\\\\"\n",
    "    )\n",
    "\n",
    "def build_row_specs(proxy_label: str, available_cols: set):\n",
    "    \"\"\"\n",
    "    Build the list of (row_label, coef_key, se_key, p_key) for this proxy,\n",
    "    including jump_* rows only if columns exist.\n",
    "    \"\"\"\n",
    "    specs = [\n",
    "        (\"Intercept\",              \"coef_Intercept\",                \"stderr_Intercept\",                \"pval_Intercept\"),\n",
    "        (proxy_label,              f\"coef_{proxy_label}\",           f\"stderr_{proxy_label}\",           f\"pval_{proxy_label}\"),\n",
    "        (\"$R_{g,t}$\",              \"coef_R_g_t_lag\",                \"stderr_R_g_t_lag\",                \"pval_R_g_t_lag\"),\n",
    "        (\"$\\Delta\\log\\ $TxCnt\",              \"coef_log_diff_TxCnt\",           \"stderr_log_diff_TxCnt\",           \"pval_log_diff_TxCnt\"),\n",
    "        (\"$\\Delta\\log\\ $Volume\",             \"coef_log_diff_volume_trusted_spot_usd_1d\",\n",
    "                                   \"stderr_log_diff_volume_trusted_spot_usd_1d\",\n",
    "                                   \"pval_log_diff_volume_trusted_spot_usd_1d\"),\n",
    "        (\"$\\Delta\\log\\ $AdrActCnt\",   \"coef_log_diff_AdrActCnt\",       \"stderr_log_diff_AdrActCnt\",       \"pval_log_diff_AdrActCnt\"),\n",
    "    ]\n",
    "    # Optional jump rows — include only if present in the CSV\n",
    "    jump_rows = [\n",
    "        (\"Jump TxCnt\",             \"coef_jump_TxCnt\",               \"stderr_jump_TxCnt\",               \"pval_jump_TxCnt\"),\n",
    "        (\"Jump Volume\",            \"coef_jump_volume_trusted_spot_usd_1d\",\n",
    "                                   \"stderr_jump_volume_trusted_spot_usd_1d\",\n",
    "                                   \"pval_jump_volume_trusted_spot_usd_1d\"),\n",
    "        (\"Jump Active Addresses\",  \"coef_jump_AdrActCnt\",           \"stderr_jump_AdrActCnt\",           \"pval_jump_AdrActCnt\"),\n",
    "    ]\n",
    "    for r in jump_rows:\n",
    "        if (r[1] in available_cols) or (r[2] in available_cols) or (r[3] in available_cols):\n",
    "            specs.append(r)\n",
    "\n",
    "    specs += [\n",
    "        (\"$\\log\\ $UnemRt\",             \"coef_log_UnemRt\",               \"stderr_log_UnemRt\",               \"pval_log_UnemRt\"),\n",
    "        (\"$\\log\\ $IndPro\",             \"coef_log_IndPro\",               \"stderr_log_IndPro\",               \"pval_log_IndPro\"),\n",
    "        (\"$\\log\\ $CPIPrc\",             \"coef_log_CPIPrc\",               \"stderr_log_CPIPrc\",               \"pval_log_CPIPrc\"),\n",
    "        (\"$\\log\\ $TotRes\",             \"coef_log_TotRes\",               \"stderr_log_TotRes\",               \"pval_log_TotRes\"),\n",
    "    ]\n",
    "    return specs\n",
    "\n",
    "def render_table_for_proxy(df_proxy: pd.DataFrame, proxy_label: str):\n",
    "    \"\"\"\n",
    "    Render LaTeX table string for one proxy, using df_proxy rows (one per group).\n",
    "    \"\"\"\n",
    "    present_groups = [g for g in GROUP_ORDER if g in set(df_proxy[\"group\"].astype(str))]\n",
    "    if not present_groups:\n",
    "        return None\n",
    "\n",
    "    sub_idx = df_proxy.set_index(\"group\")\n",
    "    available_cols = set(df_proxy.columns)\n",
    "\n",
    "    row_specs = build_row_specs(proxy_label, available_cols)\n",
    "\n",
    "    lines = []\n",
    "    num_cols = len(present_groups)\n",
    "    cols_spec = f\"l *{{{num_cols}}}{{c}}\"\n",
    "    lines += [\n",
    "        r\"\\begin{table}[ht]\",\n",
    "        r\"\\centering\",\n",
    "        r\"\\scriptsize\",\n",
    "        r\"\\setlength{\\tabcolsep}{4pt}\",\n",
    "        rf\"\\begin{{tabular}}{{{cols_spec}}}\",\n",
    "        r\"\\toprule\",\n",
    "        \" & \" + \" & \".join(present_groups) + r\" \\\\\",\n",
    "        r\"\\midrule\"\n",
    "    ]\n",
    "\n",
    "    # Coefficient row + SE row per item\n",
    "    for label, coef_key, se_key, p_key in row_specs:\n",
    "        # coef line\n",
    "        coef_cells = []\n",
    "        for g in present_groups:\n",
    "            row = sub_idx.loc[g] if g in sub_idx.index else None\n",
    "            coef_cells.append(fmt_coef(row.get(coef_key, np.nan), row.get(p_key, np.nan)) if row is not None else \"\")\n",
    "        lines.append(label + \" & \" + \" & \".join(coef_cells) + r\" \\\\\")\n",
    "        # se line\n",
    "        se_cells = []\n",
    "        for g in present_groups:\n",
    "            row = sub_idx.loc[g] if g in sub_idx.index else None\n",
    "            se_cells.append(fmt_se(row.get(se_key, np.nan)) if row is not None else \"\")\n",
    "        lines.append(\" & \" + \" & \".join(se_cells) + r\" \\\\\")\n",
    "        lines.append(r\"\\addlinespace\")\n",
    "\n",
    "    # Bottom stats\n",
    "    lines.append(r\"\\midrule\")\n",
    "    bottom_specs = [\n",
    "        (\"N\",              \"nobs\",           True),\n",
    "        (\"Durbin-Watson\",  \"durbin_watson\",  False),\n",
    "        (\"F-stat\",         \"fstat\",          False),\n",
    "        (r\"\\(R^2\\)\",       \"r2\",             False),\n",
    "        (r\"Adj.\\ \\(R^2\\)\", \"adj_r2\",         False),\n",
    "    ]\n",
    "    for label, stat_key, as_int in bottom_specs:\n",
    "        vals = []\n",
    "        for g in present_groups:\n",
    "            v = sub_idx.loc[g].get(stat_key, np.nan) if g in sub_idx.index else np.nan\n",
    "            vals.append(fmt_stat(v, ints=as_int))\n",
    "        lines.append(label + \" & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "\n",
    "    # Caption + label (multi-line caption to avoid truncation issues)\n",
    "    nice_proxy = proxy_label\n",
    "    lines += [\n",
    "        r\"\\addlinespace\",\n",
    "        r\"\\midrule\",\n",
    "        header_line(num_cols),\n",
    "        r\"\\bottomrule\",\n",
    "        r\"\\end{tabular}\",\n",
    "        rf\"\\caption{{\\textbf{{Estimation Results from Predictive Regressions — {nice_proxy}}} \\\\\",\n",
    "        r\"This table presents coefficient estimates and Newey--West standard errors (in parentheses) from predictive regressions of next-day group returns on the selected proxy for investor sentiment.\",\n",
    "        r\"The regression specification is reported at the top of the table. Each column corresponds to a functional group portfolio (equal-weight) as defined in Section~\\ref{sec:groupings}.\",\n",
    "        r\"The dependent variable is the next-day log return of the group portfolio. Standard errors are computed using the Newey--West estimator with automatic lag selection. Statistical significance is denoted as follows: *$p<0.05$, **$p<0.01$.}\",\n",
    "        rf\"\\label{{tab:h2_{proxy_label.lower()}_results}}\",\n",
    "        r\"\\end{table}\"\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---- Load results ----\n",
    "df = pd.read_csv(RESULTS_CSV)\n",
    "\n",
    "# Normalize types\n",
    "df[\"group\"] = df[\"group\"].astype(str)\n",
    "df[\"proxy\"] = df[\"proxy\"].astype(str)\n",
    "\n",
    "# Get proxies present\n",
    "proxies = list(df[\"proxy\"].dropna().unique())\n",
    "\n",
    "# Generate a table for each proxy found\n",
    "written = []\n",
    "for proxy in proxies:\n",
    "    df_proxy = df[df[\"proxy\"] == proxy].copy()\n",
    "    tex = render_table_for_proxy(df_proxy, proxy)\n",
    "    if tex is None:\n",
    "        continue\n",
    "    out_tex = OUT_DIR / f\"h2_functional_results_{proxy}.tex\"\n",
    "    with open(out_tex, \"w\") as f:\n",
    "        f.write(tex)\n",
    "    written.append(str(out_tex))\n",
    "\n",
    "print(\"[INFO] Wrote\", len(written), \"tables:\")\n",
    "for p in written:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e1fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:78: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:79: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:82: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:97: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:98: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:78: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:79: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:82: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:97: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:98: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_3660/2847391677.py:78: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  (\"$\\Delta\\log\\ $TxCnt\",              \"coef_log_diff_TxCnt\",           \"stderr_log_diff_TxCnt\",           \"pval_log_diff_TxCnt\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_3660/2847391677.py:79: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  (\"$\\Delta\\log\\ $Volume\",             \"coef_log_diff_volume_trusted_spot_usd_1d\",\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_3660/2847391677.py:82: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  (\"$\\Delta\\log\\ $AdrActCnt\",   \"coef_log_diff_AdrActCnt\",       \"stderr_log_diff_AdrActCnt\",       \"pval_log_diff_AdrActCnt\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_3660/2847391677.py:97: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $UnemRt\",             \"coef_log_UnemRt\",               \"stderr_log_UnemRt\",               \"pval_log_UnemRt\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_3660/2847391677.py:98: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $IndPro\",             \"coef_log_IndPro\",               \"stderr_log_IndPro\",               \"pval_log_IndPro\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_3660/2847391677.py:99: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $CPIPrc\",             \"coef_log_CPIPrc\",               \"stderr_log_CPIPrc\",               \"pval_log_CPIPrc\"),\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_3660/2847391677.py:100: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  (\"$\\log\\ $TotRes\",             \"coef_log_TotRes\",               \"stderr_log_TotRes\",               \"pval_log_TotRes\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wrote 7 tables:\n",
      " - Regressions/Tables/h2_functional_results_ConSIX.tex\n",
      " - Regressions/Tables/h2_functional_results_EPU_DUS.tex\n",
      " - Regressions/Tables/h2_functional_results_InvSIX.tex\n",
      " - Regressions/Tables/h2_functional_results_TwitSIX.tex\n",
      " - Regressions/Tables/h2_functional_results_VIX.tex\n",
      " - Regressions/Tables/h2_functional_results_fng_value.tex\n",
      " - Regressions/Tables/h2_functional_results_no_sentiment.tex\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Paths ----\n",
    "RESULTS_CSV = Path(\"Regressions/h2_functional_avg_then_transform_results.csv\")\n",
    "OUT_DIR     = Path(\"Regressions/Tables\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Functional group column order (match H2 runs) ----\n",
    "GROUP_ORDER = [\"Utility\", \"Asset\", \"Payment\", \"Hybrid (U-A)\", \"Hybrid (U-P)\", \"Hybrid (A-P)\"]\n",
    "\n",
    "# ---- Significance thresholds ----\n",
    "# ** for p<0.01; * for p<0.05 (matches your earlier tables)\n",
    "STAR_THRESHOLDS = [(0.01, \"**\"), (0.05, \"*\")]\n",
    "\n",
    "# ---- Formatting helpers ----\n",
    "def star_for_p(p):\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    for thr, mark in STAR_THRESHOLDS:\n",
    "        if p < thr:\n",
    "            return mark\n",
    "    return \"\"\n",
    "\n",
    "def fmt_coef(c, p):\n",
    "    if c is None or (isinstance(c, float) and (np.isnan(c) or np.isinf(c))):\n",
    "        return \"\"\n",
    "    return f\"{float(c):.3f}{star_for_p(p)}\"\n",
    "\n",
    "def fmt_se(se):\n",
    "    if se is None or (isinstance(se, float) and (np.isnan(se) or np.isinf(se))):\n",
    "        return \"\"\n",
    "    return f\"({float(se):.3f})\"\n",
    "\n",
    "def fmt_stat(x, ints=False, ndigits=2):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    if ints:\n",
    "        try:\n",
    "            return f\"{int(x)}\"\n",
    "        except Exception:\n",
    "            return f\"{x}\"\n",
    "    return f\"{float(x):.{ndigits}f}\"\n",
    "\n",
    "def header_line(num_cols:int) -> str:\n",
    "    # Safer multi-part build to avoid truncation issues\n",
    "    num_cols = num_cols + 1\n",
    "    return (\n",
    "        \"\\\\multicolumn{\" + str(num_cols) + \"}{c}{Specification tested: $\"\n",
    "        \"R_{g,t+1} = \"\n",
    "        \"\\\\alpha_g + \"\n",
    "        \"\\\\beta_{sent,g} S_t + \"\n",
    "        \"\\\\phi R_{g,t} + \"\n",
    "        \"\\\\theta B_{g,t} + \"\n",
    "        \"\\\\gamma M_t + \"\n",
    "        \"\\\\varepsilon_{g,t+1}$}\\\\\\\\\"\n",
    "    )\n",
    "\n",
    "def build_row_specs(proxy_label: str, available_cols: set):\n",
    "    \"\"\"\n",
    "    Build the list of (row_label, coef_key, se_key, p_key) for this proxy,\n",
    "    including jump_* rows only if columns exist.\n",
    "    \"\"\"\n",
    "    specs = [\n",
    "        (\"Intercept\",              \"coef_Intercept\",                \"stderr_Intercept\",                \"pval_Intercept\"),\n",
    "        (proxy_label,              f\"coef_{proxy_label}\",           f\"stderr_{proxy_label}\",           f\"pval_{proxy_label}\"),\n",
    "        (\"$R_{g,t}$\",              \"coef_R_g_t_lag\",                \"stderr_R_g_t_lag\",                \"pval_R_g_t_lag\"),\n",
    "        (\"$\\Delta\\log\\ $TxCnt\",              \"coef_log_diff_TxCnt\",           \"stderr_log_diff_TxCnt\",           \"pval_log_diff_TxCnt\"),\n",
    "        (\"$\\Delta\\log\\ $Volume\",             \"coef_log_diff_volume_trusted_spot_usd_1d\",\n",
    "                                   \"stderr_log_diff_volume_trusted_spot_usd_1d\",\n",
    "                                   \"pval_log_diff_volume_trusted_spot_usd_1d\"),\n",
    "        (\"$\\Delta\\log\\ $AdrActCnt\",   \"coef_log_diff_AdrActCnt\",       \"stderr_log_diff_AdrActCnt\",       \"pval_log_diff_AdrActCnt\"),\n",
    "    ]\n",
    "    \n",
    "    jump_rows = [\n",
    "        (\"Jump TxCnt\",             \"coef_jump_TxCnt\",               \"stderr_jump_TxCnt\",               \"pval_jump_TxCnt\"),\n",
    "        (\"Jump Volume\",            \"coef_jump_volume_trusted_spot_usd_1d\",\n",
    "                                   \"stderr_jump_volume_trusted_spot_usd_1d\",\n",
    "                                   \"pval_jump_volume_trusted_spot_usd_1d\"),\n",
    "        (\"Jump Active Addresses\",  \"coef_jump_AdrActCnt\",           \"stderr_jump_AdrActCnt\",           \"pval_jump_AdrActCnt\"),\n",
    "    ]\n",
    "    for r in jump_rows:\n",
    "        if (r[1] in available_cols) or (r[2] in available_cols) or (r[3] in available_cols):\n",
    "            specs.append(r)\n",
    "\n",
    "    specs += [\n",
    "        (\"$\\log\\ $UnemRt\",             \"coef_log_UnemRt\",               \"stderr_log_UnemRt\",               \"pval_log_UnemRt\"),\n",
    "        (\"$\\log\\ $IndPro\",             \"coef_log_IndPro\",               \"stderr_log_IndPro\",               \"pval_log_IndPro\"),\n",
    "        (\"$\\log\\ $CPIPrc\",             \"coef_log_CPIPrc\",               \"stderr_log_CPIPrc\",               \"pval_log_CPIPrc\"),\n",
    "        (\"$\\log\\ $TotRes\",             \"coef_log_TotRes\",               \"stderr_log_TotRes\",               \"pval_log_TotRes\"),\n",
    "    ]\n",
    "    return specs\n",
    "\n",
    "def render_table_for_proxy(df_proxy: pd.DataFrame, proxy_label: str):\n",
    "    \"\"\"\n",
    "    Render LaTeX table string for one proxy, using df_proxy rows (one per group).\n",
    "    \"\"\"\n",
    "    present_groups = [g for g in GROUP_ORDER if g in set(df_proxy[\"group\"].astype(str))]\n",
    "    if not present_groups:\n",
    "        return None\n",
    "\n",
    "    sub_idx = df_proxy.set_index(\"group\")\n",
    "    available_cols = set(df_proxy.columns)\n",
    "\n",
    "    row_specs = build_row_specs(proxy_label, available_cols)\n",
    "\n",
    "    lines = []\n",
    "    num_cols = len(present_groups)\n",
    "    cols_spec = f\"l *{{{num_cols}}}{{c}}\"\n",
    "    lines += [\n",
    "        r\"\\begin{table}[ht]\",\n",
    "        r\"\\centering\",\n",
    "        r\"\\scriptsize\",\n",
    "        r\"\\setlength{\\tabcolsep}{4pt}\",\n",
    "        rf\"\\begin{{tabular}}{{{cols_spec}}}\",\n",
    "        r\"\\toprule\",\n",
    "        \" & \" + \" & \".join(present_groups) + r\" \\\\\",\n",
    "        r\"\\midrule\"\n",
    "    ]\n",
    "\n",
    "    # Coefficient row + SE row per item\n",
    "    for label, coef_key, se_key, p_key in row_specs:\n",
    "        # coef line\n",
    "        coef_cells = []\n",
    "        for g in present_groups:\n",
    "            row = sub_idx.loc[g] if g in sub_idx.index else None\n",
    "            coef_cells.append(fmt_coef(row.get(coef_key, np.nan), row.get(p_key, np.nan)) if row is not None else \"\")\n",
    "        lines.append(label + \" & \" + \" & \".join(coef_cells) + r\" \\\\\")\n",
    "        # se line\n",
    "        se_cells = []\n",
    "        for g in present_groups:\n",
    "            row = sub_idx.loc[g] if g in sub_idx.index else None\n",
    "            se_cells.append(fmt_se(row.get(se_key, np.nan)) if row is not None else \"\")\n",
    "        lines.append(\" & \" + \" & \".join(se_cells) + r\" \\\\\")\n",
    "        lines.append(r\"\\addlinespace\")\n",
    "\n",
    "    # Bottom stats\n",
    "    lines.append(r\"\\midrule\")\n",
    "# label, key, as_int, ndigits\n",
    "    bottom_specs = [\n",
    "        (\"N\",              \"nobs\",           True,  None),\n",
    "        (\"Durbin-Watson\",  \"durbin_watson\",  False, 2),\n",
    "        (\"F-stat\",         \"fstat\",          False, 2),\n",
    "        (r\"\\(R^2\\)\",       \"r2\",             False, 3),\n",
    "        (r\"Adj.\\ \\(R^2\\)\", \"adj_r2\",         False, 3),\n",
    "    ]\n",
    "\n",
    "    for label, stat_key, as_int, nd in bottom_specs:\n",
    "        vals = []\n",
    "        for g in present_groups:\n",
    "            v = sub_idx.loc[g].get(stat_key, np.nan) if g in sub_idx.index else np.nan\n",
    "            vals.append(fmt_stat(v, ints=as_int, ndigits=(nd or 2)))\n",
    "        lines.append(label + \" & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "    # Caption + label (multi-line caption to avoid truncation issues)\n",
    "    nice_proxy = proxy_label\n",
    "    lines += [\n",
    "        r\"\\addlinespace\",\n",
    "        r\"\\midrule\",\n",
    "        header_line(num_cols),\n",
    "        r\"\\bottomrule\",\n",
    "        r\"\\end{tabular}\",\n",
    "        rf\"\\caption{{\\textbf{{Estimation Results from Predictive Regressions — {nice_proxy}}} \\\\\",\n",
    "        r\"This table presents coefficient estimates and Newey--West standard errors (in parentheses) from predictive regressions of next-day group returns on the selected proxy for investor sentiment.\",\n",
    "        r\"The regression specification is reported at the top of the table. Each column corresponds to a functional group portfolio (equal-weight) as defined in Section~\\ref{sec:groupings}.\",\n",
    "        r\"The dependent variable is the next-day log return of the group portfolio. Standard errors are computed using the Newey--West estimator with automatic lag selection. Statistical significance is denoted as follows: *$p<0.05$, **$p<0.01$.}\",\n",
    "        rf\"\\label{{tab:h2_{proxy_label.lower()}_results}}\",\n",
    "        r\"\\end{table}\"\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---- Load results ----\n",
    "df = pd.read_csv(RESULTS_CSV)\n",
    "\n",
    "# Normalize types\n",
    "df[\"group\"] = df[\"group\"].astype(str)\n",
    "df[\"proxy\"] = df[\"proxy\"].astype(str)\n",
    "\n",
    "# Get proxies present\n",
    "proxies = list(df[\"proxy\"].dropna().unique())\n",
    "\n",
    "# Generate a table for each proxy found\n",
    "written = []\n",
    "for proxy in proxies:\n",
    "    df_proxy = df[df[\"proxy\"] == proxy].copy()\n",
    "    tex = render_table_for_proxy(df_proxy, proxy)\n",
    "    if tex is None:\n",
    "        continue\n",
    "    out_tex = OUT_DIR / f\"h2_functional_results_{proxy}.tex\"\n",
    "    with open(out_tex, \"w\") as f:\n",
    "        f.write(tex)\n",
    "    written.append(str(out_tex))\n",
    "\n",
    "print(\"[INFO] Wrote\", len(written), \"tables:\")\n",
    "for p in written:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b6350",
   "metadata": {},
   "source": [
    "WALD TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c586b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "proxy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "groups_tested",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nobs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hac_maxlags",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wald_test",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "chi2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "df_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "p_value",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "446d264d-2799-4a24-8e9c-922e2d94bd70",
       "rows": [
        [
         "0",
         "ConSIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18014",
         "0",
         "equal_betas_across_groups",
         "0.6492662399057572",
         "5",
         "0.6620712414735872"
        ],
        [
         "1",
         "EPU_DUS",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18014",
         "0",
         "equal_betas_across_groups",
         "1.7412732078277906",
         "5",
         "0.12142583317806833"
        ],
        [
         "2",
         "InvSIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18014",
         "0",
         "equal_betas_across_groups",
         "0.8007701366275886",
         "5",
         "0.5488777496763233"
        ],
        [
         "3",
         "TwitSIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "13916",
         "0",
         "equal_betas_across_groups",
         "0.6151496697959395",
         "5",
         "0.6883100613825825"
        ],
        [
         "4",
         "VIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18014",
         "0",
         "equal_betas_across_groups",
         "0.4651636848990021",
         "5",
         "0.8024610782456834"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proxy</th>\n",
       "      <th>groups_tested</th>\n",
       "      <th>nobs</th>\n",
       "      <th>hac_maxlags</th>\n",
       "      <th>wald_test</th>\n",
       "      <th>chi2</th>\n",
       "      <th>df_num</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConSIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18014</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>0.649266</td>\n",
       "      <td>5</td>\n",
       "      <td>0.662071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPU_DUS</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18014</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>1.741273</td>\n",
       "      <td>5</td>\n",
       "      <td>0.121426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>InvSIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18014</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>0.800770</td>\n",
       "      <td>5</td>\n",
       "      <td>0.548878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TwitSIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>13916</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>0.615150</td>\n",
       "      <td>5</td>\n",
       "      <td>0.688310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18014</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>0.465164</td>\n",
       "      <td>5</td>\n",
       "      <td>0.802461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     proxy                                      groups_tested   nobs  \\\n",
       "0   ConSIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18014   \n",
       "1  EPU_DUS  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18014   \n",
       "2   InvSIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18014   \n",
       "3  TwitSIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  13916   \n",
       "4      VIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18014   \n",
       "\n",
       "   hac_maxlags                  wald_test      chi2  df_num   p_value  \n",
       "0            0  equal_betas_across_groups  0.649266       5  0.662071  \n",
       "1            0  equal_betas_across_groups  1.741273       5  0.121426  \n",
       "2            0  equal_betas_across_groups  0.800770       5  0.548878  \n",
       "3            0  equal_betas_across_groups  0.615150       5  0.688310  \n",
       "4            0  equal_betas_across_groups  0.465164       5  0.802461  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# H2 Wald test: equality of sentiment slopes across groups\n",
    "# - Pooled OLS with group FE and group-specific sentiment slopes\n",
    "# - Newey–West HAC (AIC-selected lag)\n",
    "# - One χ² Wald test per sentiment proxy\n",
    "# - Returns DataFrame; optionally writes Reg/h2_wald_tests.csv\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "DATE_COL   = \"date\"\n",
    "SYMBOL_COL = \"symbol\"\n",
    "RET_COL    = \"log_daily_return\"\n",
    "TARGET_COL = \"R_g_t_plus1\"  # constructed below\n",
    "\n",
    "# Raw blockchain columns to be averaged (RAW → then transform)\n",
    "BASE_BLOCKCHAIN = [\"TxCnt\", \"AdrActCnt\", \"volume_trusted_spot_usd_1d\"]\n",
    "\n",
    "# Macros (log levels already in df_crypto) and Sentiments\n",
    "MACRO_COLS     = [\"log_UnemRt\", \"log_IndPro\", \"log_CPIPrc\", \"log_TotRes\"]\n",
    "SENTIMENT_COLS = [\"VIX\", \"TwitSIX\", \"EPU_DUS\", \"InvSIX\", \"fng_value\", \"ConSIX\"]\n",
    "\n",
    "# Functional groups (as in your taxonomy)\n",
    "UTILITY = {\"1INCH\",\"AAVE\",\"ANT\",\"APE\",\"BADGER\",\"BAL\",\"BAT\",\"BIT\",\"CEL\",\"COMP\",\"CRV\",\"DCR\",\"ENS\",\"FXS\",\n",
    "           \"HEDG\",\"LDO\",\"MKR\",\"QNT\",\"ROOK\",\"SUSHI\",\"SWRV\",\"UNI\",\"YFI\"}\n",
    "ASSET   = {\"ALCX\",\"ALPHA\",\"CVX\",\"PAXG\",\"XAUT\"}\n",
    "PAYMENT = {\"BCH\",\"BNB\",\"BSV\",\"BTC\",\"BTG\",\"CRO\",\"DASH\",\"DGB\",\"DOGE\",\"ETC\",\"ETH\",\"FTT\",\"GRIN\",\"HT\",\"LTC\",\n",
    "           \"MTL_METAL\",\"PAY\",\"SHIB\",\"VTC\",\"XLM\",\"XMR\",\"XRP\",\"XVG\",\"ZEC\"}\n",
    "HYBRID_UA = {\"CVC\"}\n",
    "HYBRID_UP = {\"ADA\",\"ALGO\",\"API3\",\"ATOM\",\"AUDIO\",\"AVAX\",\"BNT\",\"DOT\",\"DRGN\",\"ELF\",\"ENJ\",\"EOS\",\"FIL\",\"FLOW\",\n",
    "             \"FUN\",\"GALA\",\"GAS\",\"GLM\",\"GNO\",\"GNT\",\"GRT\",\"ICP\",\"ICX\",\"KNC\",\"LEND\",\"LINK\",\"LOOM\",\"LPT\",\"LRC\",\n",
    "             \"LSK\",\"LUNA\",\"MAID\",\"MANA\",\"NEO\",\"NMR\",\"OGN\",\"OMG\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\"QASH\",\"QTUM\",\n",
    "             \"REN\",\"REP\",\"RSR\",\"SAND\",\"SKL\",\"SNT\",\"SNX\",\"SOL\",\"SRM\",\"STORJ\",\"TRX\",\"UMA\",\"VET\",\"WAVES\",\n",
    "             \"WNXM\",\"WTC\",\"XEM\",\"XTZ\",\"ZIL\",\"ZRX\"}\n",
    "HYBRID_AP = {\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\",\"USDC\",\"USDT\"}\n",
    "\n",
    "GROUP_ORDER = [\"Utility\", \"Asset\", \"Payment\", \"Hybrid (U-A)\", \"Hybrid (U-P)\", \"Hybrid (A-P)\"]\n",
    "\n",
    "# Output\n",
    "EXPORT_CSV       = Path(\"Regressions/h2_wald_tests.csv\")\n",
    "WRITE_WALD_CSV   = True  # set False if you don't want a CSV\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def map_functional_group(sym: str) -> str:\n",
    "    s = str(sym).upper()\n",
    "    if s in UTILITY:   return \"Utility\"\n",
    "    if s in ASSET:     return \"Asset\"\n",
    "    if s in PAYMENT:   return \"Payment\"\n",
    "    if s in HYBRID_UA: return \"Hybrid (U-A)\"\n",
    "    if s in HYBRID_UP: return \"Hybrid (U-P)\"\n",
    "    if s in HYBRID_AP: return \"Hybrid (A-P)\"\n",
    "    return \"Unclassified\"\n",
    "\n",
    "def select_hac_lag_via_resid_aic(resid: pd.Series, kmax: int) -> int:\n",
    "    \"\"\"\n",
    "    Choose HAC lag via AIC on AR(k) residual model (k=0..kmax).\n",
    "    For k=0, use log-variance proxy objective.\n",
    "    \"\"\"\n",
    "    n = resid.shape[0]\n",
    "    if n < 20 or kmax <= 0:\n",
    "        return 0\n",
    "    best_k, best_aic = 0, np.inf\n",
    "    for k in range(kmax + 1):\n",
    "        try:\n",
    "            if k == 0:\n",
    "                e = resid - resid.mean()\n",
    "                sigma2 = np.var(e, ddof=1)\n",
    "                aic = n * np.log(sigma2 + 1e-12) + 2\n",
    "            else:\n",
    "                ar = AutoReg(resid, lags=k, old_names=False, trend=\"c\").fit()\n",
    "                aic = ar.aic\n",
    "        except Exception:\n",
    "            aic = np.inf\n",
    "        if aic < best_aic:\n",
    "            best_aic, best_k = aic, k\n",
    "    return int(best_k)\n",
    "\n",
    "def make_group_panel_avg_then_transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    H2 panel: average RAW → zero-safe log-diffs → build lags.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    d[DATE_COL]   = pd.to_datetime(d[DATE_COL], errors=\"coerce\")\n",
    "    d[SYMBOL_COL] = d[SYMBOL_COL].astype(str).str.upper()\n",
    "    d[\"h2_group\"] = d[SYMBOL_COL].map(map_functional_group)\n",
    "    d = d[d[\"h2_group\"].isin(GROUP_ORDER)].copy()\n",
    "\n",
    "    # days where all raw blockchain inputs are present\n",
    "    elig = d.dropna(subset=BASE_BLOCKCHAIN).copy()\n",
    "\n",
    "    # equal-weight means (RAW)\n",
    "    agg_cols = [RET_COL] + BASE_BLOCKCHAIN\n",
    "    grp_raw = (elig.groupby([DATE_COL, \"h2_group\"], dropna=False)[agg_cols]\n",
    "                   .mean(numeric_only=True)\n",
    "                   .reset_index()\n",
    "                   .sort_values([\"h2_group\", DATE_COL]))\n",
    "\n",
    "    # zero-safe log-diffs on group averages\n",
    "    for col in BASE_BLOCKCHAIN:\n",
    "        prev = grp_raw.groupby(\"h2_group\")[col].shift(1)\n",
    "        curr = grp_raw[col]\n",
    "        grp_raw[f\"log_diff_{col}\"] = np.where((prev > 0) & (curr > 0), np.log(curr/prev), 0.0)\n",
    "\n",
    "    # attach date-level macros + sentiments (unique per date)\n",
    "    date_cols = [DATE_COL] + [c for c in (MACRO_COLS + SENTIMENT_COLS) if c in d.columns]\n",
    "    date_level = d[date_cols].drop_duplicates(subset=[DATE_COL]).sort_values(DATE_COL)\n",
    "    grp = grp_raw.merge(date_level, on=DATE_COL, how=\"left\")\n",
    "\n",
    "    # target and lags\n",
    "    grp[\"R_g_t\"]       = grp.groupby(\"h2_group\")[RET_COL].shift(0)\n",
    "    grp[\"R_g_t_lag\"]   = grp.groupby(\"h2_group\")[RET_COL].shift(1)\n",
    "    grp[\"R_g_t_plus1\"] = grp.groupby(\"h2_group\")[RET_COL].shift(-1)\n",
    "\n",
    "    grp = grp.dropna(subset=[\"R_g_t_lag\", \"R_g_t_plus1\"]).reset_index(drop=True)\n",
    "    return grp\n",
    "\n",
    "def find_interaction_param_names(param_names, group_list, proxy):\n",
    "    \"\"\"\n",
    "    Map each group to the parameter name of its interaction slope with the proxy.\n",
    "    Handles both 'C(h2_group)[T.Group]:Proxy' and 'Proxy:C(h2_group)[T.Group]'.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for g in group_list:\n",
    "        pat1 = re.compile(rf\"C\\(h2_group\\)\\[(?:T\\.)?{re.escape(g)}\\]\\:{re.escape(proxy)}$\")\n",
    "        pat2 = re.compile(rf\"{re.escape(proxy)}\\:C\\(h2_group\\)\\[(?:T\\.)?{re.escape(g)}\\]$\")\n",
    "        hit = next((n for n in param_names if pat1.search(n) or pat2.search(n)), None)\n",
    "        mapping[g] = hit\n",
    "    return mapping\n",
    "\n",
    "def build_R_equal_betas(param_names, beta_names):\n",
    "    \"\"\"\n",
    "    Build R for H0: beta_g1 = beta_g2 = ... = beta_gk, implemented as\n",
    "    (beta_gi - beta_g1) = 0 for i=2..k.\n",
    "    \"\"\"\n",
    "    valid = [b for b in beta_names if b is not None]\n",
    "    if len(valid) < 2:\n",
    "        return None, None\n",
    "    p = len(param_names)\n",
    "    base = valid[0]\n",
    "    rows = []\n",
    "    for b in valid[1:]:\n",
    "        r = np.zeros(p)\n",
    "        r[param_names.index(b)] = 1.0\n",
    "        r[param_names.index(base)] = -1.0\n",
    "        rows.append(r)\n",
    "    R = np.vstack(rows)\n",
    "    r = np.zeros(R.shape[0])\n",
    "    return R, r\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "def run_h2_wald_tests(df_crypto: pd.DataFrame,\n",
    "                      write_csv: bool = WRITE_WALD_CSV,\n",
    "                      export_csv: Path = EXPORT_CSV) -> pd.DataFrame:\n",
    "    grp = make_group_panel_avg_then_transform(df_crypto)\n",
    "\n",
    "    present_groups = [g for g in GROUP_ORDER if g in grp[\"h2_group\"].unique()]\n",
    "    if len(present_groups) < 2:\n",
    "        raise ValueError(\"Not enough groups present for a Wald equality test.\")\n",
    "\n",
    "    # common controls (same slope across groups)\n",
    "    controls = [\"R_g_t_lag\", \"log_diff_AdrActCnt\",\n",
    "                \"log_diff_volume_trusted_spot_usd_1d\", \"log_diff_TxCnt\"] \\\n",
    "               + [c for c in MACRO_COLS if c in grp.columns]\n",
    "\n",
    "    out_rows = []\n",
    "    for proxy in [c for c in SENTIMENT_COLS if c in grp.columns]:\n",
    "        # keep rows with all needed vars\n",
    "        sub = grp.dropna(subset=[TARGET_COL, proxy] + controls).copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        # standardize continuous controls + proxy (not FE)\n",
    "        z_cols = controls + [proxy]\n",
    "        for c in z_cols:\n",
    "            m, s = sub[c].mean(), sub[c].std(ddof=0)\n",
    "            if s and np.isfinite(s) and s > 0:\n",
    "                sub[c] = (sub[c] - m) / s\n",
    "\n",
    "        # pooled OLS: group FE + group-specific sentiment slopes + common controls\n",
    "        # FE: 0 + C(h2_group)\n",
    "        # Group-specific proxy slopes: 0 + C(h2_group):proxy\n",
    "        rhs = \" + \".join([\"0 + C(h2_group)\", f\"0 + C(h2_group):{proxy}\"] + controls)\n",
    "        formula = f\"{TARGET_COL} ~ {rhs}\"\n",
    "\n",
    "        fit = smf.ols(formula, data=sub).fit()\n",
    "        n = int(fit.nobs)\n",
    "        if n <= 8:\n",
    "            continue\n",
    "\n",
    "        # HAC lag selection by AIC\n",
    "        kmax = int(n ** 0.25)\n",
    "        best_aic, best_lag, best_res = np.inf, 0, None\n",
    "        for lag in range(kmax + 1):\n",
    "            r = fit.get_robustcov_results(cov_type=\"HAC\", maxlags=lag, use_correction=True)\n",
    "            if r.aic < best_aic:\n",
    "                best_aic, best_lag, best_res = r.aic, lag, r\n",
    "\n",
    "        param_names = list(best_res.model.exog_names)\n",
    "        beta_map = find_interaction_param_names(param_names, present_groups, proxy)\n",
    "        beta_names = [beta_map[g] for g in present_groups]\n",
    "\n",
    "        # build R for equality of those slopes\n",
    "        R, rvec = build_R_equal_betas(param_names, beta_names)\n",
    "        if R is None:\n",
    "            \n",
    "            continue\n",
    "\n",
    "        wt = best_res.wald_test((R, rvec), scalar=False)  # χ²\n",
    "        chi2 = float(np.asarray(wt.statistic).ravel()[0])\n",
    "        pval = float(np.asarray(wt.pvalue).ravel()[0])\n",
    "        df_num = int(R.shape[0])\n",
    "\n",
    "        out_rows.append({\n",
    "            \"proxy\": proxy,\n",
    "            \"groups_tested\": \",\".join(present_groups),\n",
    "            \"nobs\": n,\n",
    "            \"hac_maxlags\": best_lag,\n",
    "            \"wald_test\": \"equal_betas_across_groups\",\n",
    "            \"chi2\": chi2,\n",
    "            \"df_num\": df_num,\n",
    "            \"p_value\": pval\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(out_rows).sort_values([\"proxy\"]).reset_index(drop=True)\n",
    "    if write_csv:\n",
    "        export_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "        out.to_csv(export_csv, index=False)\n",
    "    return out\n",
    "\n",
    "\n",
    "wald_results = run_h2_wald_tests(df_crypto)\n",
    "wald_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f77d39e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wrote LaTeX Wald table → Regressions/Tables/h2_wald_tests.tex\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('Regressions/Tables/h2_wald_tests.tex')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Build LaTeX table from H2 Wald tests\n",
    "# - Reads from DataFrame or CSV: Regressions/h2_wald_tests.csv\n",
    "# - Orders proxies consistently\n",
    "# - Adds significance stars on p-values\n",
    "# - Writes a single LaTeX table to Regressions/Tables/h2_wald_tests.tex\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------ Paths ------------\n",
    "IN_CSV   = Path(\"Regressions/h2_wald_tests.csv\")\n",
    "OUT_DIR  = Path(\"Regressions/Tables\")\n",
    "OUT_TEX  = OUT_DIR / \"h2_wald_tests.tex\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------ Proxy order (consistent with H2 figures/tables) ------------\n",
    "PROXY_ORDER = [\"VIX\", \"TwitSIX\", \"EPU_DUS\", \"InvSIX\", \"fng_value\", \"ConSIX\"]\n",
    "\n",
    "# ------------ Stars for significance (on p-values) ------------\n",
    "STAR_THRESHOLDS = [(0.01, \"**\"), (0.05, \"*\")]\n",
    "\n",
    "def star_for_p(p):\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    for thr, mark in STAR_THRESHOLDS:\n",
    "        if p < thr:\n",
    "            return mark\n",
    "    return \"\"\n",
    "\n",
    "def fmt_float(x, digits=3):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    try:\n",
    "        return f\"{float(x):.{digits}f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def fmt_int(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    try:\n",
    "        return f\"{int(x)}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def label_for_proxy(p):\n",
    "    \"\"\"Nicer LaTeX-safe proxy labels if needed.\"\"\"\n",
    "    # Escape underscores\n",
    "    return str(p).replace(\"_\", r\"\\_\")\n",
    "\n",
    "def render_wald_table(df: pd.DataFrame,\n",
    "                      caption: str = r\"\\textbf{Wald Tests for Equality of Sentiment Slopes Across Functional Groups}\\\\ The table presents the results of Wald $\\chi^2$ tests evaluating the null hypothesis that sentiment coefficients are identical across functional groups. Test statistics are computed using HAC (Newey--West) covariance estimates, with the lag length determined by the Akaike Information Criterion (AIC). The analysis includes six functional groups: Utility, Asset, Payment, and Hybrid variants (U-A, U-P, A-P). Reported are the $\\chi^2$ statistic, the corresponding degrees of freedom (df), the p-value, the number of observations, and the number of HAC lags used.\",\n",
    "                      label: str   = \"tab:h2_wald_tests\") -> str:\n",
    "    \"\"\"\n",
    "    Renders a LaTeX table:\n",
    "      columns: Proxy | χ² | df | p-value | N | HAC maxlags\n",
    "    Notes: groups tested are stored in df['groups_tested'] and listed in a footnote.\n",
    "    \"\"\"\n",
    "    # Keep only columns we need and sort by PROXY_ORDER\n",
    "    need_cols = [\"proxy\", \"chi2\", \"df_num\", \"p_value\", \"nobs\", \"hac_maxlags\", \"groups_tested\"]\n",
    "    for c in need_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Required column '{c}' not found in the Wald results DataFrame.\")\n",
    "    df2 = df[need_cols].copy()\n",
    "\n",
    "    # ordering\n",
    "    cat = pd.Categorical(df2[\"proxy\"], categories=[p for p in PROXY_ORDER if p in df2[\"proxy\"].unique()], ordered=True)\n",
    "    # Put any unexpected proxies (if present) after the known ones\n",
    "    other = [p for p in df2[\"proxy\"].unique() if p not in PROXY_ORDER]\n",
    "    df2[\"_order\"] = pd.Categorical(df2[\"proxy\"], categories=list(cat.categories)+other, ordered=True)\n",
    "    df2 = df2.sort_values(\"_order\").drop(columns=[\"_order\"]).reset_index(drop=True)\n",
    "\n",
    "    # Build LaTeX\n",
    "    lines = []\n",
    "    lines += [\n",
    "        r\"\\begin{table}[ht]\",\n",
    "        r\"\\centering\",\n",
    "        r\"\\scriptsize\",\n",
    "        r\"\\setlength{\\tabcolsep}{4pt}\",\n",
    "        r\"\\begin{tabular}{l c c c c c}\",\n",
    "        r\"\\toprule\",\n",
    "        r\"\\textbf{Proxy} & \\(\\boldsymbol{\\chi^2}\\) & \\textbf{df} & \\textbf{p-value} & \\textbf{\\# Obs} & \\textbf{HAC lags} \\\\\",\n",
    "        r\"\\midrule\"\n",
    "    ]\n",
    "\n",
    "    # Table rows\n",
    "    for _, row in df2.iterrows():\n",
    "        proxy = label_for_proxy(row[\"proxy\"])\n",
    "        chi2  = fmt_float(row[\"chi2\"], 2)\n",
    "        dfnum = fmt_int(row[\"df_num\"])\n",
    "        pval  = fmt_float(row[\"p_value\"], 3)\n",
    "        pstar = star_for_p(row[\"p_value\"])\n",
    "        nobs  = fmt_int(row[\"nobs\"])\n",
    "        lag   = fmt_int(row[\"hac_maxlags\"])\n",
    "\n",
    "        lines.append(f\"{proxy} & {chi2} & {dfnum} & {pval}{pstar} & {nobs} & {lag} \\\\\\\\\")\n",
    "    lines += [r\"\\midrule\"]\n",
    "\n",
    "    # Footnote with groups tested (unique across rows)\n",
    "    # If you prefer to show per-proxy groups, comment these two lines and print row-wise after each line.\n",
    "    groups_sets = sorted(set(df2[\"groups_tested\"]))\n",
    "    foot_groups = \"; \".join(groups_sets).replace(\"_\", r\"\\_\")\n",
    "\n",
    "    lines += [\n",
    "        r\"\\multicolumn{6}{c}{$H_0 : \\beta_{sent,g_1} = \\beta_{sent,g_2}=\\ ...\\ =\\beta_{sent,g}$}\\\\\",\n",
    "        r\"\\bottomrule\",\n",
    "        r\"\\end{tabular}\",\n",
    "        rf\"\\caption{{{caption}}}\",\n",
    "        rf\"\\label{{{label}}}\",\n",
    "        r\"\\end{table}\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_wald_table_from_df_or_csv(df_wald: pd.DataFrame | None = None,\n",
    "                                    in_csv: Path = IN_CSV,\n",
    "                                    out_tex: Path = OUT_TEX) -> Path:\n",
    "    \"\"\"\n",
    "    If df_wald is provided, use it; otherwise load from CSV.\n",
    "    Writes LaTeX table to out_tex and returns the path.\n",
    "    \"\"\"\n",
    "    if df_wald is None:\n",
    "        if not in_csv.exists():\n",
    "            raise FileNotFoundError(f\"Input CSV not found: {in_csv}\")\n",
    "        df_wald = pd.read_csv(in_csv)\n",
    "\n",
    "    tex = render_wald_table(df_wald)\n",
    "    with open(out_tex, \"w\") as f:\n",
    "        f.write(tex)\n",
    "    print(f\"[INFO] Wrote LaTeX Wald table → {out_tex}\")\n",
    "    return out_tex\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "# Case A) You just ran the tests in this session:\n",
    "# from your_previous_cell import run_h2_wald_tests\n",
    "# wald_results = run_h2_wald_tests(df_crypto)\n",
    "build_wald_table_from_df_or_csv(df_wald=wald_results)\n",
    "\n",
    "# Case B) Load from CSV already on disk:\n",
    "# build_wald_table_from_df_or_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b26059",
   "metadata": {},
   "source": [
    "### ROBUSTNESS CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45eae93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:125: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:126: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:127: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:128: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:129: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:130: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:125: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:126: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:127: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:128: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:129: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:130: SyntaxWarning: invalid escape sequence '\\D'\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2197930866.py:125: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '$\\Delta$ EPU_DUS':      'diff_EPU_DUS',\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2197930866.py:126: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '$\\Delta$ VIX':          'diff_VIX',\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2197930866.py:127: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '$\\Delta$ InvSIX':       'diff_InvSIX',\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2197930866.py:128: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '$\\Delta$ TwitSIX':      'diff_TwitSIX',\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2197930866.py:129: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '$\\Delta$ ConSIX':       'diff_ConSIX',\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2197930866.py:130: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '$\\Delta$ fng_value':    'diff_fng_value'\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/checkrobustness_$\\Delta$ EPU_DUS.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/checkrobustness_$\\Delta$ VIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/checkrobustness_$\\Delta$ InvSIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/checkrobustness_$\\Delta$ TwitSIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/checkrobustness_$\\Delta$ ConSIX.csv\n",
      "Kept 85 symbols; Dropped 40: ['ALCX', 'APE', 'API3', 'ATOM', 'AUDIO', 'AVAX', 'BADGER', 'BIT', 'BNT', 'CEL']...\n",
      "Saved 85 symbols → Regressions/checkrobustness_$\\Delta$ fng_value.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# ---------- 1) PREPROCESSING (two-part, per-symbol, NA-safe) ----------\n",
    "base_cols = ['AdrActCnt', 'volume_trusted_spot_usd_1d', 'TxCnt']\n",
    "\n",
    "for col in base_cols:\n",
    "    prev = df_crypto.groupby('symbol')[col].shift(1)   # per-symbol lag\n",
    "    curr = df_crypto[col]\n",
    "    df_crypto[f'jump_{col}'] = ((prev == 0) & (curr > 0)).astype(int)\n",
    "    # set to 0 outside >0→>0 regime so we don't drop rows\n",
    "    df_crypto[f'log_diff_{col}'] = np.where((prev > 0) & (curr > 0),\n",
    "                                            np.log(curr/prev), 0.0)\n",
    "\n",
    "# ---------- 2) ELIGIBILITY (robust) ----------\n",
    "def min_required_n(regressors, floor=365, margin=30):\n",
    "    p = 1 + len(regressors)  # + intercept\n",
    "    return max(floor, p + margin)\n",
    "\n",
    "def _symbol_eligible(sub, target, regressors):\n",
    "    if any(r not in sub.columns for r in regressors):\n",
    "        return False, \"missing_columns\"\n",
    "    cc = sub.dropna(subset=[target])  # X have no NA after step 1\n",
    "    if len(cc) < min_required_n(regressors):\n",
    "        return False, f\"too_few_obs:{len(cc)}\"\n",
    "    # require variation for continuous vars (allow jump_* to be constant)\n",
    "    for r in regressors:\n",
    "        if not r.startswith('jump_') and cc[r].nunique() < 2:\n",
    "            return False, f\"no_variation:{r}\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "# ---------- 3) REGRESSION ----------\n",
    "def run_symbol_regression(df, symbol, target, regressors):\n",
    "    sub = df[df['symbol'] == symbol].sort_values('date').copy()\n",
    "    if target not in sub.columns:\n",
    "        sub[target] = sub['log_daily_return'].shift(-1)\n",
    "\n",
    "    ok, reason = _symbol_eligible(sub, target, regressors)\n",
    "    if not ok:\n",
    "        return None\n",
    "\n",
    "    sub = sub.dropna(subset=[target])  # only target must be present\n",
    "\n",
    "    # standardize only continuous predictors (leave jump_* as is)\n",
    "    cont = [r for r in regressors if not r.startswith('jump_')]\n",
    "    if cont:\n",
    "        sub[cont] = StandardScaler().fit_transform(sub[cont])\n",
    "\n",
    "    rhs = ' + '.join(regressors)\n",
    "    fit = smf.ols(f\"{target} ~ {rhs}\", data=sub).fit()\n",
    "    if fit.df_resid <= 8:   # guard for reliable HAC/AIC\n",
    "        return None\n",
    "\n",
    "    n = len(sub)\n",
    "    max_hac_lag = int(n ** 0.25)\n",
    "\n",
    "    best_aic, best_lag, best_res = np.inf, 0, None\n",
    "    for lag in range(max_hac_lag + 1):\n",
    "        r = fit.get_robustcov_results(cov_type='HAC', maxlags=lag, use_correction=True)\n",
    "        if r.aic < best_aic:\n",
    "            best_aic, best_lag, best_res = r.aic, lag, r\n",
    "\n",
    "    dw_val = float(durbin_watson(fit.resid))\n",
    "\n",
    "    out = {\n",
    "        'symbol': symbol, 'n': n, 'nobs': n,\n",
    "        'n_regs': len(regressors), 'regs_used': ','.join(regressors),\n",
    "        'opt_hac_lag': best_lag, 'r2': best_res.rsquared,\n",
    "        'adj_r2': best_res.rsquared_adj, 'aic': best_res.aic, 'bic': best_res.bic,\n",
    "        'log_likelihood': best_res.llf, 'fstat': best_res.fvalue, 'f_pval': best_res.f_pvalue,\n",
    "        'durbin_watson': dw_val\n",
    "    }\n",
    "    for name, b, se, t, p in zip(best_res.model.exog_names,\n",
    "                                 best_res.params, best_res.bse, best_res.tvalues, best_res.pvalues):\n",
    "        out[f'coef_{name}']   = b\n",
    "        out[f'stderr_{name}'] = se\n",
    "        out[f'tval_{name}']   = t\n",
    "        out[f'pval_{name}']   = p\n",
    "    return out\n",
    "\n",
    "def run_all_symbols(df, target, regressors):\n",
    "    df = df.sort_values(['symbol','date']).copy()\n",
    "    # crea il target t+1 se non presente\n",
    "    if target not in df.columns:\n",
    "        df[target] = df.groupby('symbol')['log_daily_return'].shift(-1)\n",
    "\n",
    "    results = []\n",
    "    kept, dropped = [], []\n",
    "\n",
    "    for sym in df['symbol'].unique():\n",
    "        sub = df[df['symbol'] == sym]\n",
    "        if _symbol_eligible(sub, target, regressors):\n",
    "            r = run_symbol_regression(df, sym, target, regressors)\n",
    "            if r is not None:\n",
    "                results.append(r)\n",
    "                kept.append(sym)\n",
    "            else:\n",
    "                dropped.append(sym)\n",
    "        else:\n",
    "            dropped.append(sym)\n",
    "\n",
    "    out = pd.DataFrame(results)\n",
    "    # Log sintetico (opzionale)\n",
    "    print(f\"Kept {len(set(kept))} symbols; Dropped {len(set(dropped))}: {sorted(set(dropped))[:10]}...\")\n",
    "    return out\n",
    "\n",
    "# ---------------------- 3) SET UP YOUR LOOP ----------------------\n",
    "target = 'log_daily_next'\n",
    "\n",
    "base_regressors = [\n",
    "    'log_daily_return',\n",
    "    # 'jump_AdrActCnt',\n",
    "    'log_diff_AdrActCnt',\n",
    "    # 'jump_volume_trusted_spot_usd_1d',\n",
    "    'log_diff_volume_trusted_spot_usd_1d',\n",
    "    # 'jump_TxCnt',\n",
    "    'log_diff_TxCnt',\n",
    "    'log_UnemRt', 'log_IndPro', 'log_CPIPrc', 'log_TotRes',\n",
    "]\n",
    "\n",
    "sentiment_proxies = {\n",
    "    '$\\Delta$ EPU_DUS':      'diff_EPU_DUS',\n",
    "    '$\\Delta$ VIX':          'diff_VIX',\n",
    "    '$\\Delta$ InvSIX':       'diff_InvSIX',\n",
    "    '$\\Delta$ TwitSIX':      'diff_TwitSIX',\n",
    "    '$\\Delta$ ConSIX':       'diff_ConSIX',\n",
    "    '$\\Delta$ fng_value':    'diff_fng_value'\n",
    "}\n",
    "\n",
    "for name, proxy in sentiment_proxies.items():\n",
    "    regs = base_regressors.copy()\n",
    "    if proxy is not None:\n",
    "        regs.insert(1, proxy)   # sentiment subito dopo il ritorno laggato\n",
    "    summary_df = run_all_symbols(df_crypto, target, regs)\n",
    "    fn = f\"Regressions/checkrobustness_{name}.csv\"\n",
    "    summary_df.to_csv(fn, index=False)\n",
    "    print(f\"Saved {summary_df['symbol'].nunique()} symbols → {fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9342dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ checkrobustness_$\\Delta$ ConSIX.csv -> Regressions/Tables/rob_tab_diff_ConSIX.tex\n",
      "✅ checkrobustness_$\\Delta$ EPU_DUS.csv -> Regressions/Tables/rob_tab_diff_EPU_DUS.tex\n",
      "✅ checkrobustness_$\\Delta$ InvSIX.csv -> Regressions/Tables/rob_tab_diff_InvSIX.tex\n",
      "✅ checkrobustness_$\\Delta$ TwitSIX.csv -> Regressions/Tables/rob_tab_diff_TwitSIX.tex\n",
      "✅ checkrobustness_$\\Delta$ VIX.csv -> Regressions/Tables/rob_tab_diff_VIX.tex\n",
      "✅ checkrobustness_$\\Delta$ fng_value.csv -> Regressions/Tables/rob_tab_diff_fng_value.tex\n",
      "✅ crypto_regression_summary_ConSIX.csv -> Regressions/Tables/rob_tab_ConSIX.tex\n",
      "✅ crypto_regression_summary_EPU_DUS.csv -> Regressions/Tables/rob_tab_EPU_DUS.tex\n",
      "✅ crypto_regression_summary_InvSIX.csv -> Regressions/Tables/rob_tab_InvSIX.tex\n",
      "✅ crypto_regression_summary_TwitSIX.csv -> Regressions/Tables/rob_tab_TwitSIX.tex\n",
      "✅ crypto_regression_summary_VIX.csv -> Regressions/Tables/rob_tab_VIX.tex\n",
      "✅ crypto_regression_summary_fng_value.csv -> Regressions/Tables/rob_tab_fng_value.tex\n",
      "✅ crypto_regression_summary_no_sentiment.csv -> Regressions/Tables/rob_tab_log_diff_AdrActCnt.tex\n",
      "[skip] h2_functional_avg_then_transform_results.csv: 'symbol' column missing in Regressions/h2_functional_avg_then_transform_results.csv\n",
      "[skip] h2_wald_tests.csv: 'symbol' column missing in Regressions/h2_wald_tests.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Paths =====\n",
    "REGRESSIONS_DIR = \"Regressions\"\n",
    "OUT_DIR = \"Regressions/Tables\"\n",
    "Path(OUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# ===== Classification =====\n",
    "def classify_symbol(symbol):\n",
    "    if symbol == \"MTL_METAL\":\n",
    "        symbol = \"MTL\"\n",
    "    BTC = {\"BTC\"}\n",
    "    ALT_HIGH = {\"AAVE\",\"ADA\",\"ALGO\",\"ATOM\",\"AVAX\",\"BCH\",\"BNB\",\"BSV\",\"CRO\",\"CRV\",\"DOT\",\"ENS\",\"EOS\",\"ETC\",\n",
    "                \"ETH\",\"FIL\",\"FLOW\",\"GALA\",\"GRT\",\"ICP\",\"LDO\",\"LINK\",\"LTC\",\"MANA\",\"MKR\",\"QNT\",\"SAND\",\"SOL\",\n",
    "                \"TRX\",\"UNI\",\"VET\",\"XLM\",\"XMR\",\"XRP\",\"XTZ\",\"ZEC\"}\n",
    "    ALT_MID  = {\"1INCH\",\"ANT\",\"AUDIO\",\"BAT\",\"COMP\",\"CVC\",\"CVX\",\"DASH\",\"DCR\",\"DGB\",\"ELF\",\"ENJ\",\"FTT\",\"FXS\",\n",
    "                \"GAS\",\"GLM\",\"GNO\",\"ICX\",\"LPT\",\"LRC\",\"LUNA\",\"NEO\",\"QTUM\",\"RSR\",\"SKL\",\"SNT\",\"SNX\",\n",
    "                \"SUSHI\",\"UMA\",\"WAVES\",\"XVG\",\"YFI\",\"ZIL\",\"ZRX\"}\n",
    "    ALT_LOW  = {\"ALCX\",\"ALPHA\",\"API3\",\"BADGER\",\"BAL\",\"BIT\",\"BNT\",\"BTG\",\"CEL\",\"DRGN\",\"FUN\",\"GNT\",\"GRIN\",\"HEDG\",\"HT\",\n",
    "                \"KNC\",\"LEND\",\"LOOM\",\"LSK\",\"MAID\",\"MTL\",\"NMR\",\"OGN\",\"OMG\",\"PAY\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\n",
    "                \"QASH\",\"REN\",\"REP\",\"ROOK\",\"SRM\",\"STORJ\",\"SWRV\",\"VTC\",\"WNXM\",\"WTC\",\"XEM\"}\n",
    "    STABLE   = {\"USDT\",\"USDC\",\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\"}\n",
    "    GOLDPEG  = {\"PAXG\",\"XAUT\"}\n",
    "    MEME     = {\"DOGE\",\"SHIB\",\"APE\"}\n",
    "    if symbol in BTC:     return \"BTC\"\n",
    "    if symbol in ALT_HIGH:return \"ALT_HIGH\"\n",
    "    if symbol in ALT_MID: return \"ALT_MID\"\n",
    "    if symbol in ALT_LOW: return \"ALT_LOW\"\n",
    "    if symbol in STABLE:  return \"STABLE\"\n",
    "    if symbol in GOLDPEG: return \"GOLDPEG\"\n",
    "    if symbol in MEME:    return \"MEME\"\n",
    "    return \"UNCLASSIFIED\"\n",
    "\n",
    "# ===== Helpers =====\n",
    "def first_existing(df, names):\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "def starify(p):\n",
    "    if pd.isna(p): return \"\"\n",
    "    return \"**\" if p < 0.01 else (\"*\" if p < 0.05 else \"\")\n",
    "\n",
    "def fmt_num(x, d=3, int_ok=False):\n",
    "    if pd.isna(x): return \"\"\n",
    "    if int_ok:\n",
    "        try: return f\"{int(x)}\"\n",
    "        except: pass\n",
    "    try: return f\"{float(x):.{d}f}\"\n",
    "    except: return \"\"\n",
    "\n",
    "# ===== pick 10 symbols per your rule =====\n",
    "def pick_symbols(df, proxy):\n",
    "    pcol = first_existing(df, [f\"pval_{proxy}\", f\"pval_log_d_{proxy}\", f\"pval_log_diff_{proxy}\"])\n",
    "    if pcol is None:\n",
    "        # fallback: use any pval_* that exists (last resort)\n",
    "        pvals = [c for c in df.columns if c.startswith(\"pval_\") and c != \"pval_Intercept\"]\n",
    "        pcol = pvals[0] if pvals else None\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"group\"] = d[\"symbol\"].map(classify_symbol)\n",
    "\n",
    "    def pick_median(group):\n",
    "        sub = d[(d[\"group\"] == group) & d[pcol].notna()].sort_values(pcol)\n",
    "        if sub.empty: return None\n",
    "        return sub.iloc[len(sub)//2][\"symbol\"]\n",
    "\n",
    "    def pick_min(group):\n",
    "        sub = d[(d[\"group\"] == group) & d[pcol].notna()].sort_values(pcol)\n",
    "        if sub.empty: return None\n",
    "        return sub.iloc[0][\"symbol\"]\n",
    "\n",
    "    chosen = []\n",
    "    if \"BTC\" in set(d[\"symbol\"]): chosen.append(\"BTC\")\n",
    "\n",
    "    for grp in [\"ALT_HIGH\",\"ALT_MID\",\"ALT_LOW\"]:\n",
    "        s1 = pick_min(grp)\n",
    "        s2 = pick_median(grp)\n",
    "        picks = []\n",
    "        if s1: picks.append(s1)\n",
    "        if s2 and s2 not in picks: picks.append(s2)\n",
    "        if len(picks) < 2:\n",
    "            # deterministic pad\n",
    "            pool = [x for x in sorted(d.loc[d[\"group\"]==grp,\"symbol\"]) if x not in picks]\n",
    "            while len(picks) < 2 and pool:\n",
    "                picks.append(pool.pop(0))\n",
    "        chosen += picks[:2]\n",
    "\n",
    "    for grp in [\"STABLE\",\"GOLDPEG\",\"MEME\"]:\n",
    "        s = pick_median(grp)\n",
    "        if s: chosen.append(s)\n",
    "\n",
    "    return chosen[:10]\n",
    "\n",
    "# ===== Build one LaTeX table from a CSV =====\n",
    "def build_table_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"symbol\" not in df.columns:\n",
    "        raise ValueError(f\"'symbol' column missing in {csv_path}\")\n",
    "\n",
    "    # detect proxy (the sentiment var): first coef_* that is not intercept/logs\n",
    "    proxy_candidates = [c.replace(\"coef_\",\"\") for c in df.columns\n",
    "                        if c.startswith(\"coef_\")\n",
    "                        and c not in (\"coef_Intercept\",\"coef_log_daily_return\")]\n",
    "    if not proxy_candidates:\n",
    "        raise ValueError(f\"No proxy coef_ column found in {csv_path}\")\n",
    "    proxy = proxy_candidates[0]\n",
    "\n",
    "    # columns (LaTeX rows) you want, with flexible suffixes\n",
    "    row_specs = [\n",
    "        (\"Intercept\",           [\"Intercept\"]),\n",
    "        (proxy,                 [proxy, f\"log_d_{proxy}\", f\"log_diff_{proxy}\"]),\n",
    "        (r\"$R_{i,t}$\",          [\"log_daily_return\",\"log_returns_lag1\",\"lag_return\",\"returns_lag1\"]),\n",
    "        (r\"$\\Delta\\log\\ $TxCnt\",           [\"log_TxCnt\",\"log_diff_TxCnt\",\"log_d_TxCnt\"]),\n",
    "        (r\"$\\Delta\\log\\ $Volume\",          [\"log_volume_trusted_spot_usd_1d\",\"log_diff_volume_trusted_spot_usd_1d\",\"log_d_volume_trusted_spot_usd_1d\"]),\n",
    "        (r\"$\\Delta\\log\\ $AdrActCnt\",     [\"log_AdrActCnt\",\"log_diff_AdrActCnt\"]),\n",
    "        (r\"$\\log\\ $UnemRt\",          [\"log_UnemRt\"]),\n",
    "        (r\"$\\log\\ $IndPro\",          [\"log_IndPro\"]),\n",
    "        (r\"$\\log\\ $CPIPrc\",          [\"log_CPIPrc\"]),\n",
    "        (r\"$\\log\\ $TotRes\",          [\"log_TotRes\"]),\n",
    "    ]\n",
    "\n",
    "    # metric columns (flexible names)\n",
    "    col_N   = first_existing(df, [\"n\",\"nobs\",\"N\"])\n",
    "    col_DW  = first_existing(df, [\"dw\",\"durbin_watson\",\"DurbinWatson\"])\n",
    "    col_F   = first_existing(df, [\"fstat\",\"F\",\"f_stat\"])\n",
    "    col_R2  = first_existing(df, [\"r_squared\",\"r2\"])\n",
    "    col_AR2 = first_existing(df, [\"adj_r_squared\",\"adj_r2\",\"adjR\"])\n",
    "    \n",
    "    symbols = pick_symbols(df, proxy)\n",
    "    idx = {s: df[df[\"symbol\"]==s].iloc[0] for s in symbols if not df[df[\"symbol\"]==s].empty}\n",
    "\n",
    "    # resolve which exact CSV cols to use (coef/stderr/pval) for each row\n",
    "    resolved = []\n",
    "    for label, suffixes in row_specs:\n",
    "        coef_col  = first_existing(df, [f\"coef_{s}\"   for s in suffixes])\n",
    "        se_col    = first_existing(df, [f\"stderr_{s}\" for s in suffixes])  # <-- Newey–West SE expected here\n",
    "        pval_col  = first_existing(df, [f\"pval_{s}\"   for s in suffixes])\n",
    "        resolved.append((label, coef_col, se_col, pval_col))\n",
    "\n",
    "    # ---- Assemble LaTeX lines ----\n",
    "    lines = []\n",
    "    lines += [\n",
    "        r\"\\begin{table}[ht]\",\n",
    "        r\"\\centering\",\n",
    "        r\"\\scriptsize\",\n",
    "        r\"\\setlength{\\tabcolsep}{4pt}\",\n",
    "        rf\"\\begin{{tabular}}{{l *{{{len(symbols)}}}{{c}}}}\",\n",
    "        r\"\\toprule\",\n",
    "        r\"&\\multicolumn{1}{c}{\\textbf{Bitcoin}}&\\multicolumn{2}{c}{\\textbf{High-Cap}}&\\multicolumn{2}{c}{\\textbf{Mid-Cap}}&\\multicolumn{2}{c}{\\textbf{Low-Cap}}&\\multicolumn{1}{c}{\\textbf{Gold}}&\\multicolumn{1}{c}{\\textbf{Stable}}&\\multicolumn{1}{c}{\\textbf{Meme}}\\\\\",\n",
    "        r\"\\addlinespace\",\n",
    "        \" & \" + \" & \".join(symbols) + r\" \\\\\",\n",
    "        r\"\\midrule\"\n",
    "    ]\n",
    "\n",
    "    def add_var(label, coef_col, se_col, pval_col):\n",
    "        coefs = []\n",
    "        ses   = []\n",
    "        for s in symbols:\n",
    "            row = idx.get(s)\n",
    "            if row is None:\n",
    "                coefs.append(\"\")\n",
    "                ses.append(\"()\")\n",
    "                continue\n",
    "            c = row.get(coef_col, np.nan) if coef_col else np.nan\n",
    "            p = row.get(pval_col, np.nan) if pval_col else np.nan\n",
    "            se = row.get(se_col, np.nan)  if se_col   else np.nan\n",
    "            coefs.append(f\"{fmt_num(c,3)}{starify(p)}\")\n",
    "            ses.append(f\"({fmt_num(se,3)})\" if not pd.isna(se) else \"()\")\n",
    "        lines.append(f\"{label} & \" + \" & \".join(coefs) + r\" \\\\\")\n",
    "        lines.append(\" & \" + \" & \".join(ses) + r\" \\\\\")\n",
    "        lines.append(r\"\\addlinespace\")\n",
    "\n",
    "    for label, cc, sc, pc in resolved:\n",
    "        add_var(label, cc, sc, pc)\n",
    "\n",
    "    # ---- Bottom metrics ----\n",
    "    def metric_row(name, colname, int_flag=False, decimals=2):\n",
    "        vals = []\n",
    "        for s in symbols:\n",
    "            row = idx.get(s)\n",
    "            if row is None or colname is None:\n",
    "                vals.append(\"\")\n",
    "            else:\n",
    "                vals.append(fmt_num(row.get(colname, np.nan), d=decimals, int_ok=int_flag))\n",
    "        lines.append(f\"{name} & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "\n",
    "    lines.append(r\"\\midrule\")\n",
    "    metric_row(\"N\", col_N, int_flag=True, decimals=0)\n",
    "    metric_row(\"Durbin-Watson\", col_DW, int_flag=False, decimals=2)\n",
    "    metric_row(\"F-stat\", col_F, int_flag=False, decimals=2)\n",
    "    metric_row(r\"\\(R^2\\)\", col_R2, int_flag=False, decimals=3)\n",
    "    metric_row(r\"Adj.\\ \\(R^2\\)\", col_AR2, int_flag=False, decimals=3)\n",
    "    length = len(symbols) + 1\n",
    "\n",
    "    lines += [\n",
    "        r\"\\addlinespace\",\n",
    "        r\"\\midrule\",\n",
    "        rf\"\\multicolumn{{{length}}}{{c}}{{Specification tested: $R_{{i,t+1}} = \\alpha_i + \\beta_{{sent}} S_t + \\phi R_{{i,t}} + \\theta B_{{i,t}} + \\gamma M_t + \\varepsilon_{{i,t+1}}$}}\\\\\",\n",
    "        r\"\\bottomrule\",\n",
    "        r\"\\end{tabular}\",\n",
    "        rf\"\\caption{{\\textbf{{Estimation Results from Predictive Regressions - {proxy}}} \\\\\",\n",
    "        r\"This table presents coefficient estimates and Newey--West standard errors (in parentheses) from predictive regressions of next-day returns on the selected proxy for investor sentiment. The regression specification is reported at the top of the table. Each column corresponds to a selected cryptocurrency, chosen to represent the full range of categories in the sample: Bitcoin (BTC), two high-cap altcoins, two mid-cap altcoins, two low-cap altcoins, one stablecoin, one gold-pegged token, and one meme coin. Each regression is estimated separately using the available time series data for the respective cryptocurrency and the sentiment proxy. The dependent variable is the next-day log return. Standard errors are computed using the Newey--West estimator with automatic lag selection. Statistical significance is denoted as follows: *$p<0.05$, **$p<0.01$.}\",\n",
    "        rf\"\\label{{tab:{proxy.lower()}_result_h1}}\",\n",
    "        r\"\\end{table}\"\n",
    "    ]\n",
    "    return proxy, lines\n",
    "\n",
    "# ===== Run over all CSVs =====\n",
    "for csv in sorted(Path(REGRESSIONS_DIR).glob(\"*.csv\")):\n",
    "    try:\n",
    "        proxy, lines = build_table_from_csv(csv)\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {csv.name}: {e}\")\n",
    "        continue\n",
    "    out = Path(OUT_DIR) / f\"rob_tab_{proxy}.tex\"\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    print(f\"✅ {csv.name} -> {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96829b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] wrote: Regressions/Tables/h1_level_vs_delta_summary.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:110: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:110: SyntaxWarning: invalid escape sequence '\\D'\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/323218520.py:110: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  patt = os.path.join(REG_DIR, f\"checkrobustness_$\\Delta$ {proxy_base}*.csv\")\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from statistics import NormalDist\n",
    "\n",
    "REG_DIR = \"Regressions\"\n",
    "TAB_DIR = Path(\"Regressions/Tables\")\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- sentiment proxies to include (adjust to your set) ---\n",
    "SENTIMENT_PROXIES = [\"ConSIX\",\"VIX\",\"TwitSIX\",\"InvSIX\",\"EPU_DUS\",\"fng_value\"]\n",
    "\n",
    "def format_proxy_label_level(p):   # Level row label\n",
    "    return p\n",
    "\n",
    "def format_proxy_label_delta(p):   # Δ only in math mode\n",
    "    return rf\"$\\Delta$ {p}\"\n",
    "\n",
    "def _find_cols(df: pd.DataFrame, proxy_base: str, scope: str):\n",
    "    \"\"\"\n",
    "    Return (coef_col, pval_col, se_col_or_None, t_col_or_None)\n",
    "    \"\"\"\n",
    "    if scope == \"Level\":\n",
    "        c = f\"coef_{proxy_base}\"\n",
    "        p = f\"pval_{proxy_base}\" if f\"pval_{proxy_base}\" in df.columns else f\"p_{proxy_base}\"\n",
    "        s = next((col for col in [f\"se_{proxy_base}\", f\"stderr_{proxy_base}\", f\"std_{proxy_base}\"] if col in df.columns), None)\n",
    "        t = next((col for col in [f\"t_{proxy_base}\", f\"tval_{proxy_base}\", f\"tstat_{proxy_base}\"] if col in df.columns), None)\n",
    "    else:\n",
    "        c = f\"coef_diff_{proxy_base}\"\n",
    "        p = f\"pval_diff_{proxy_base}\" if f\"pval_diff_{proxy_base}\" in df.columns else f\"p_diff_{proxy_base}\"\n",
    "        s = next((col for col in [f\"se_diff_{proxy_base}\", f\"stderr_diff_{proxy_base}\", f\"std_diff_{proxy_base}\"] if col in df.columns), None)\n",
    "        t = next((col for col in [f\"t_diff_{proxy_base}\", f\"tval_diff_{proxy_base}\", f\"tstat_diff_{proxy_base}\"] if col in df.columns), None)\n",
    "\n",
    "    if c not in df.columns or p not in df.columns:\n",
    "        raise KeyError(f\"Expected coef/pval for {scope} {proxy_base}: {c}, {p}\")\n",
    "    return c, p, s, t\n",
    "\n",
    "def _compute_se_series(beta: pd.Series,\n",
    "                       se: pd.Series | None,\n",
    "                       t: pd.Series | None,\n",
    "                       p: pd.Series | None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Fill SE using (in order of preference):\n",
    "      1) provided SE\n",
    "      2) SE = |beta| / |t|\n",
    "      3) SE = |beta| / z, where z = Phi^{-1}(1 - p/2)   (two-sided normal)\n",
    "    \"\"\"\n",
    "    out = pd.Series(np.nan, index=beta.index, dtype=\"float64\")\n",
    "\n",
    "    # 1) direct SE\n",
    "    if se is not None:\n",
    "        se = pd.to_numeric(se, errors=\"coerce\")\n",
    "        if se.notna().any():\n",
    "            return se\n",
    "\n",
    "    # 2) from t-stat\n",
    "    if t is not None:\n",
    "        tt = pd.to_numeric(t, errors=\"coerce\").abs().replace(0, np.nan)\n",
    "        cand = pd.to_numeric(beta, errors=\"coerce\").abs() / tt\n",
    "        if cand.notna().any():\n",
    "            out = cand\n",
    "\n",
    "    # 3) from two-sided p-value (normal approx)\n",
    "    if p is not None:\n",
    "        pp = pd.to_numeric(p, errors=\"coerce\")\n",
    "        q = (1 - pp/2.0).clip(lower=1e-16, upper=1-1e-16)\n",
    "        z = q.apply(NormalDist().inv_cdf).abs().replace(0, np.nan)\n",
    "        cand = pd.to_numeric(beta, errors=\"coerce\").abs() / z\n",
    "        out = out.where(out.notna(), cand)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _read_level_file(proxy_base: str) -> pd.DataFrame:\n",
    "    # crypto_regression_summary_<proxy>.csv  (allow minor name variations)\n",
    "    patt = os.path.join(REG_DIR, f\"crypto_regression_summary*{proxy_base}*.csv\")\n",
    "    files = glob.glob(patt)\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    # take the first best match\n",
    "    df = pd.read_csv(files[0])\n",
    "    if \"symbol\" not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        coef_col, pval_col, se_col, t_col = _find_cols(df, proxy_base, \"Level\")\n",
    "    except KeyError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    beta = df[coef_col]\n",
    "    pval = df[pval_col]\n",
    "    se_series = _compute_se_series(beta,\n",
    "                                df[se_col] if se_col else None,\n",
    "                                df[t_col] if t_col else None,\n",
    "                                pval)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"proxy_base\": proxy_base,\n",
    "        \"scope\": \"Level\",\n",
    "        \"symbol\": df[\"symbol\"],\n",
    "        \"beta\": beta,\n",
    "        \"pval\": pval,\n",
    "        \"se\": se_series,  # <-- now filled\n",
    "        \"r2\": df[\"r2\"] if \"r2\" in df.columns else np.nan,\n",
    "        \"adj_r2\": df[\"adj_r2\"] if \"adj_r2\" in df.columns else np.nan,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def _read_delta_file(proxy_base: str) -> pd.DataFrame:\n",
    "    # checkrobustness_*<proxy>*.csv (robust to \"Δ\", spaces, hyphens)\n",
    "    patt = os.path.join(REG_DIR, f\"checkrobustness_$\\Delta$ {proxy_base}*.csv\")\n",
    "    files = glob.glob(patt)\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(files[0])\n",
    "    if \"symbol\" not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        coef_col, pval_col, se_col, t_col = _find_cols(df, proxy_base, \"Delta\")\n",
    "    except KeyError:\n",
    "        # fallback auto-detect like you already had...\n",
    "        cand = [c for c in df.columns if c.startswith(\"coef_diff_\")]\n",
    "        if len(cand) == 1:\n",
    "            proxy_base_auto = cand[0].replace(\"coef_diff_\", \"\")\n",
    "            if proxy_base_auto != proxy_base:\n",
    "                proxy_base = proxy_base_auto\n",
    "            coef_col = cand[0]\n",
    "            pval_col = f\"pval_diff_{proxy_base}\" if f\"pval_diff_{proxy_base}\" in df.columns else (f\"p_diff_{proxy_base}\" if f\"p_diff_{proxy_base}\" in df.columns else None)\n",
    "            se_col   = f\"se_diff_{proxy_base}\" if f\"se_diff_{proxy_base}\" in df.columns else None\n",
    "            t_col    = f\"t_diff_{proxy_base}\" if f\"t_diff_{proxy_base}\" in df.columns else None\n",
    "            if pval_col is None:\n",
    "                return pd.DataFrame()\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    beta = df[coef_col]\n",
    "    pval = df[pval_col] if pval_col in df.columns else pd.Series(np.nan, index=df.index)\n",
    "    se_series = _compute_se_series(beta,\n",
    "                                df[se_col] if (se_col and se_col in df.columns) else None,\n",
    "                                df[t_col] if (t_col and t_col in df.columns) else None,\n",
    "                                pval)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"proxy_base\": proxy_base,\n",
    "        \"scope\": \"Delta\",\n",
    "        \"symbol\": df[\"symbol\"],\n",
    "        \"beta\": beta,\n",
    "        \"pval\": pval,\n",
    "        \"se\": se_series,  # <-- now filled\n",
    "        \"r2\": df[\"r2\"] if \"r2\" in df.columns else np.nan,\n",
    "        \"adj_r2\": df[\"adj_r2\"] if \"adj_r2\" in df.columns else np.nan,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def collect_long_from_separate_files():\n",
    "    rows = []\n",
    "    for prox in SENTIMENT_PROXIES:\n",
    "        lvl = _read_level_file(prox)\n",
    "        if not lvl.empty:\n",
    "            rows.append(lvl)\n",
    "        dlt = _read_delta_file(prox)\n",
    "        if not dlt.empty:\n",
    "            rows.append(dlt)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"proxy_base\",\"scope\",\"symbol\",\"beta\",\"pval\",\"se\",\"r2\",\"adj_r2\"])\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "def build_h1_summary(df_long: pd.DataFrame):\n",
    "    df = df_long.dropna(subset=[\"beta\"]).copy()\n",
    "\n",
    "    # aggregates across assets\n",
    "    agg = (df.groupby([\"proxy_base\",\"scope\"], as_index=False)\n",
    "             .agg(\n",
    "                 N_assets=(\"beta\",\"count\"),\n",
    "                 mean_beta=(\"beta\",\"mean\"),\n",
    "                 median_beta=(\"beta\",\"median\"),\n",
    "                 median_se=(\"se\", lambda s: np.nanmedian(s.values) if len(s) else np.nan),\n",
    "                 median_r2=(\"r2\", lambda s: np.nanmedian(s.values) if len(s) else np.nan),\n",
    "                 median_adj_r2=(\"adj_r2\", lambda s: np.nanmedian(s.values) if len(s) else np.nan),\n",
    "             ))\n",
    "\n",
    "    # pretty labels: Level vs Δ\n",
    "    agg[\"Proxy\"] = agg.apply(\n",
    "        lambda r: (format_proxy_label_level(r[\"proxy_base\"]) if r[\"scope\"]==\"Level\"\n",
    "                   else format_proxy_label_delta(r[\"proxy_base\"])),\n",
    "        axis=1\n",
    "    )\n",
    "    # order by proxy, Level first then Δ\n",
    "    agg[\"__ord__\"] = agg[\"proxy_base\"] + agg[\"scope\"].map({\"Level\":\"_0\",\"Delta\":\"_1\"})\n",
    "    agg = agg.sort_values(\"__ord__\").drop(columns=\"__ord__\")\n",
    "\n",
    "    # format numbers\n",
    "    def fmt(x): return \"\" if pd.isna(x) else f\"{x:.3f}\"\n",
    "    out = agg[[\"Proxy\",\"N_assets\",\"mean_beta\",\"median_beta\",\"median_se\",\"median_r2\",\"median_adj_r2\"]].copy()\n",
    "    for c in [\"mean_beta\",\"median_beta\",\"median_se\",\"median_r2\",\"median_adj_r2\"]:\n",
    "        out[c] = out[c].map(fmt)\n",
    "    return out\n",
    "\n",
    "# ---- run\n",
    "long_df = collect_long_from_separate_files()\n",
    "h1_summary = build_h1_summary(long_df)\n",
    "\n",
    "# rename columns to nice LaTeX labels\n",
    "h1_summary = h1_summary.rename(columns={\n",
    "    \"N_assets\": r\"\\# Coins\",\n",
    "    \"mean_beta\": r\"Mean $\\beta_{\\mathrm{sent}}$\",\n",
    "    \"median_beta\": r\"Median $\\beta_{\\mathrm{sent}}$\",\n",
    "    \"median_se\": r\"Median SE\",\n",
    "    \"median_r2\": r\"Median $R^2$\",\n",
    "    \"median_adj_r2\": r\"Median Adj.\\ $R^2$\"\n",
    "})\n",
    "\n",
    "latex = h1_summary.to_latex(\n",
    "    index=False, escape=False,\n",
    "    column_format=\"lcccc|cc\",\n",
    "    caption=(r\"\\textbf{H1 robustness summary: Level versus $\\Delta$ proxies.}\\\\\"\n",
    "             r\"For each sentiment proxy the table reports the number of coins used, \"\n",
    "             r\"the mean and median of $\\hat{\\beta}_{\\mathrm{sent}}$ across coins, \"\n",
    "             r\"the median standard error, and the median $R^2$ and adjusted $R^2$.\"),\n",
    "    label=\"tab:h1_level_vs_delta_summary\"\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"h1_level_vs_delta_summary.tex\").write_text(latex)\n",
    "print(\"[ok] wrote:\", TAB_DIR / \"h1_level_vs_delta_summary.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a668304",
   "metadata": {},
   "source": [
    "patt = os.path.join(REG_DIR, f\"checkrobustness_$\\Delta$ {proxy_base}*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ef08c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:105: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:127: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:105: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved robustness_violin_diff_ConSIX.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:127: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:105: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved robustness_violin_diff_EPU_DUS.png\n",
      "[ok] Saved robustness_violin_diff_InvSIX.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:127: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:105: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:127: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved robustness_violin_diff_TwitSIX.png\n",
      "[ok] Saved robustness_violin_diff_VIX.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:105: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:127: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:105: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.violinplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `scale` parameter is deprecated and will be removed in v0.15.0. You can now control the size of each plot element using matplotlib `Line2D` parameters (e.g., `linewidth`, `markersize`, etc.).\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:110: FutureWarning: \n",
      "\n",
      "The `errwidth` parameter is deprecated. And will be removed in v0.15.0. Pass `err_kws={'linewidth': 0}` instead.\n",
      "\n",
      "  sns.pointplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved robustness_violin_diff_fng_value.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_4187/2902801013.py:127: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ================== GLOBAL STYLE ==================\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "plt.rcParams.update({\n",
    "    \"font.family\":       \"serif\",\n",
    "    \"font.serif\":        [\"DejaVu Serif\"],\n",
    "    \"axes.titlesize\":    22,\n",
    "    \"axes.labelsize\":    20,\n",
    "    \"axes.titlecolor\":   (38/255, 38/255, 38/255),\n",
    "    \"xtick.labelsize\":   13,\n",
    "    \"ytick.labelsize\":   13,\n",
    "    \"legend.fontsize\":   10,\n",
    "    \"axes.titleweight\":  \"normal\",\n",
    "    \"axes.edgecolor\":    \"black\",\n",
    "    \"axes.linewidth\":    0.5,\n",
    "    \"grid.color\":        \"0.85\",\n",
    "    \"grid.linestyle\":    \"-\",\n",
    "    \"grid.linewidth\":    0.5,\n",
    "    \"figure.dpi\":        300,\n",
    "})\n",
    "\n",
    "# ====== Paths ======\n",
    "REG_DIR = \"Regressions\"\n",
    "OUT_DIR = \"Regressions/Figures\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====== Proxy label formatter ======\n",
    "# ====== Proxy label formatter ======\n",
    "def format_proxy_label(proxy: str) -> str:\n",
    "    if proxy.startswith(\"diff_\"):\n",
    "        # Δ inside math mode, rest as plain italic text\n",
    "        return rf\"$\\Delta$ {proxy.replace('diff_', '')}\"\n",
    "    return proxy\n",
    "\n",
    "# ====== Classification ======\n",
    "def classify_symbol(symbol: str) -> str:\n",
    "    if pd.isna(symbol):\n",
    "        return \"Unclassified\"\n",
    "    if symbol == \"MTL_METAL\":\n",
    "        symbol = \"MTL\"\n",
    "\n",
    "    BTC = {\"BTC\"}\n",
    "    ALT_HIGH = {\"AAVE\",\"ADA\",\"ALGO\",\"ATOM\",\"AVAX\",\"BCH\",\"BNB\",\"BSV\",\"CRO\",\"CRV\",\"DOT\",\"ENS\",\"EOS\",\"ETC\",\n",
    "                \"ETH\",\"FIL\",\"FLOW\",\"GALA\",\"GRT\",\"ICP\",\"LDO\",\"LINK\",\"LTC\",\"MANA\",\"MKR\",\"QNT\",\"SAND\",\"SOL\",\n",
    "                \"TRX\",\"UNI\",\"VET\",\"XLM\",\"XMR\",\"XRP\",\"XTZ\",\"ZEC\"}\n",
    "    ALT_MID  = {\"1INCH\",\"ANT\",\"AUDIO\",\"BAT\",\"COMP\",\"CVC\",\"CVX\",\"DASH\",\"DCR\",\"DGB\",\"ELF\",\"ENJ\",\"FTT\",\"FXS\",\n",
    "                \"GAS\",\"GLM\",\"GNO\",\"GNT\",\"ICX\",\"LPT\",\"LRC\",\"LUNA\",\"NEO\",\"QTUM\",\"RSR\",\"SKL\",\"SNT\",\"SNX\",\n",
    "                \"SUSHI\",\"UMA\",\"WAVES\",\"XVG\",\"YFI\",\"ZIL\",\"ZRX\"}\n",
    "    ALT_LOW  = {\"ALCX\",\"ALPHA\",\"API3\",\"BADGER\",\"BAL\",\"BIT\",\"BNT\",\"BTG\",\"CEL\",\"DRGN\",\"FUN\",\"GRIN\",\"HEDG\",\"HT\",\n",
    "                \"KNC\",\"LEND\",\"LOOM\",\"LSK\",\"MAID\",\"MTL\",\"NMR\",\"OGN\",\"OMG\",\"PAY\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\n",
    "                \"QASH\",\"REN\",\"REP\",\"ROOK\",\"SRM\",\"STORJ\",\"SWRV\",\"VTC\",\"WNXM\",\"WTC\",\"XEM\"}\n",
    "    STABLE   = {\"USDT\",\"USDC\",\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\"}\n",
    "    GOLDPEG  = {\"PAXG\",\"XAUT\"}\n",
    "    MEME     = {\"DOGE\",\"SHIB\",\"APE\"}\n",
    "\n",
    "    if symbol in BTC:     return \"Bitcoin\"\n",
    "    if symbol in ALT_HIGH:return \"Altcoins (High)\"\n",
    "    if symbol in ALT_MID: return \"Altcoins (Mid)\"\n",
    "    if symbol in ALT_LOW: return \"Altcoins (Low)\"\n",
    "    if symbol in STABLE:  return \"Stablecoins\"\n",
    "    if symbol in GOLDPEG: return \"Gold-Pegged\"\n",
    "    if symbol in MEME:    return \"Meme Coins\"\n",
    "    return \"Unclassified\"\n",
    "\n",
    "GROUP_ORDER = [\"Bitcoin\",\"Altcoins (High)\",\"Altcoins (Mid)\",\"Altcoins (Low)\",\n",
    "               \"Stablecoins\",\"Gold-Pegged\",\"Meme Coins\"]\n",
    "\n",
    "GROUP_PALETTE = {\n",
    "    \"Bitcoin\": \"#4C72B0\",\n",
    "    \"Altcoins (High)\": \"#599da2\",\n",
    "    \"Altcoins (Mid)\":  \"#83a075\",\n",
    "    \"Altcoins (Low)\":  \"#aca24a\",\n",
    "    \"Stablecoins\":     \"#eb9681\",\n",
    "    \"Gold-Pegged\":     \"#d2a022\",\n",
    "    \"Meme Coins\":      \"#8C8C8C\"\n",
    "}\n",
    "\n",
    "# ====== Proxy detection (only diff proxies) ======\n",
    "def detect_proxy(df: pd.DataFrame):\n",
    "    # only keep coefficients for diff_* proxies\n",
    "    cands = [c for c in df.columns if c.startswith(\"coef_diff_\")]\n",
    "    if not cands:\n",
    "        return None, None, None\n",
    "    proxy = cands[0].replace(\"coef_\", \"\")\n",
    "    coef_col = f\"coef_{proxy}\"\n",
    "    pval_col = f\"pval_{proxy}\" if f\"pval_{proxy}\" in df.columns else None\n",
    "    return proxy, coef_col, pval_col\n",
    "\n",
    "# ====== Violin plot generator ======\n",
    "def plot_violin(df, proxy, coef_col, pval_col):\n",
    "    # classify\n",
    "    df[\"Group\"] = df[\"symbol\"].map(classify_symbol)\n",
    "    df = df[df[\"Group\"].isin(GROUP_ORDER)]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.violinplot(\n",
    "        data=df, x=\"Group\", y=coef_col,\n",
    "        order=GROUP_ORDER, palette=GROUP_PALETTE,\n",
    "        inner=None, cut=2\n",
    "    )\n",
    "    sns.pointplot(\n",
    "        data=df, x=\"Group\", y=coef_col,\n",
    "        order=GROUP_ORDER, join=False, estimator=np.median,\n",
    "        color=\"black\", markers=\"_\", scale=1.3, errwidth=0\n",
    "    )\n",
    "\n",
    "    if pval_col and pval_col in df.columns:\n",
    "        sig = df[df[pval_col] < 0.05]\n",
    "        sns.stripplot(\n",
    "            data=sig, x=\"Group\", y=coef_col,\n",
    "            order=GROUP_ORDER, color=\"red\", size=4.5, jitter=True, alpha=0.85\n",
    "        )\n",
    "\n",
    "    ax.axhline(0, color=\"gray\", lw=1)\n",
    "    ax.set_title(rf\"Distribution of $\\beta_{{sent}}$ by Group — {format_proxy_label(proxy)}\")\n",
    "    ax.set_ylabel(r\"Estimated $\\beta_{\\mathrm{sent}}$\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xticklabels(GROUP_ORDER, rotation=15, ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(OUT_DIR) / f\"robustness_violin_{proxy}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[ok] Saved robustness_violin_{proxy}.png\")\n",
    "\n",
    "\n",
    "# ====== Run for all ======\n",
    "for csv in sorted(glob.glob(os.path.join(REG_DIR, \"*.csv\"))):\n",
    "    df = pd.read_csv(csv)\n",
    "    if \"symbol\" not in df.columns:\n",
    "        continue\n",
    "    proxy, coef_col, pval_col = detect_proxy(df)\n",
    "    if proxy:\n",
    "        plot_violin(df, proxy, coef_col, pval_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b46d027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Groups present: ['Utility', 'Asset', 'Payment', 'Hybrid (U-A)', 'Hybrid (U-P)', 'Hybrid (A-P)']\n",
      "[INFO] Wrote results to Regressions/rob_h2_functional_avg_then_transform_results.csv (rows=42)\n",
      "[INFO] Wrote functional membership table to Regressions/trash.tex\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "proxy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "nobs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_regs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "regs_used",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "opt_hac_lag",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "r2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "adj_r2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "aic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "log_likelihood",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fstat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f_pval",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "durbin_watson",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_R_g_t_lag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_diff_AdrActCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_diff_volume_trusted_spot_usd_1d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_diff_TxCnt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_UnemRt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_IndPro",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_CPIPrc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coef_log_TotRes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stderr_log_TotRes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tval_log_TotRes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pval_log_TotRes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "bb357c48-3866-4ade-9825-a19d34e33124",
       "rows": [
        [
         "0",
         "Asset",
         "ConSIX",
         "1922",
         "1922",
         "8",
         "R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.009821604081243773",
         "0.005680763951944279",
         "13803.617585576609",
         "13853.667679882381",
         "-6892.808792788304",
         "0.6309880353299868",
         "0.7523095271374289",
         "2.005733839671579",
         "0.19162866882646157",
         "0.1997110643774087",
         "0.9595295554798444",
         "0.33741329299585887",
         "-0.08640008379138216",
         "0.09292648427269269",
         "-0.9297681330312807",
         "0.3526084561476145",
         "0.603740776626774",
         "0.536383799655526",
         "1.1255760837939284",
         "0.26048618714390664",
         "0.14769565515124444",
         "0.08056434571410538",
         "1.8332632610877837",
         "0.06691873810789198",
         "-1.060196926416917",
         "0.9343588531945957",
         "-1.1346785261273846",
         "0.25665219268609857",
         "-1.4593345803978668",
         "1.6103969352562404",
         "-0.9061955772821083",
         "0.3649464657922501",
         "-1.0307817034460658",
         "1.063869723354057",
         "-0.968898428838002",
         "0.3327183487802253",
         "-0.7514391198722545",
         "0.8345188655071727",
         "-0.9004459346950449",
         "0.36799632360677514",
         "0.02780541948060683",
         "0.0823996113663899",
         "0.3374460027119548",
         "0.735817731774588"
        ],
        [
         "1",
         "Asset",
         "EPU_DUS",
         "1922",
         "1922",
         "8",
         "R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.009821604081243773",
         "0.005680763951944279",
         "13803.617585576609",
         "13853.667679882381",
         "-6892.808792788304",
         "0.6309880353299868",
         "0.7523095271374289",
         "2.005733839671579",
         "0.19162866882646157",
         "0.1997110643774087",
         "0.9595295554798444",
         "0.33741329299585887",
         "-0.08640008379138216",
         "0.09292648427269269",
         "-0.9297681330312807",
         "0.3526084561476145",
         "0.603740776626774",
         "0.536383799655526",
         "1.1255760837939284",
         "0.26048618714390664",
         "0.14769565515124444",
         "0.08056434571410538",
         "1.8332632610877837",
         "0.06691873810789198",
         "-1.060196926416917",
         "0.9343588531945957",
         "-1.1346785261273846",
         "0.25665219268609857",
         "-1.4593345803978668",
         "1.6103969352562404",
         "-0.9061955772821083",
         "0.3649464657922501",
         "-1.0307817034460658",
         "1.063869723354057",
         "-0.968898428838002",
         "0.3327183487802253",
         "-0.7514391198722545",
         "0.8345188655071727",
         "-0.9004459346950449",
         "0.36799632360677514",
         "0.02780541948060683",
         "0.0823996113663899",
         "0.3374460027119548",
         "0.735817731774588"
        ],
        [
         "2",
         "Asset",
         "InvSIX",
         "1922",
         "1922",
         "8",
         "R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.009821604081243773",
         "0.005680763951944279",
         "13803.617585576609",
         "13853.667679882381",
         "-6892.808792788304",
         "0.6309880353299868",
         "0.7523095271374289",
         "2.005733839671579",
         "0.19162866882646157",
         "0.1997110643774087",
         "0.9595295554798444",
         "0.33741329299585887",
         "-0.08640008379138216",
         "0.09292648427269269",
         "-0.9297681330312807",
         "0.3526084561476145",
         "0.603740776626774",
         "0.536383799655526",
         "1.1255760837939284",
         "0.26048618714390664",
         "0.14769565515124444",
         "0.08056434571410538",
         "1.8332632610877837",
         "0.06691873810789198",
         "-1.060196926416917",
         "0.9343588531945957",
         "-1.1346785261273846",
         "0.25665219268609857",
         "-1.4593345803978668",
         "1.6103969352562404",
         "-0.9061955772821083",
         "0.3649464657922501",
         "-1.0307817034460658",
         "1.063869723354057",
         "-0.968898428838002",
         "0.3327183487802253",
         "-0.7514391198722545",
         "0.8345188655071727",
         "-0.9004459346950449",
         "0.36799632360677514",
         "0.02780541948060683",
         "0.0823996113663899",
         "0.3374460027119548",
         "0.735817731774588"
        ],
        [
         "3",
         "Asset",
         "TwitSIX",
         "1922",
         "1922",
         "8",
         "R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.009821604081243773",
         "0.005680763951944279",
         "13803.617585576609",
         "13853.667679882381",
         "-6892.808792788304",
         "0.6309880353299868",
         "0.7523095271374289",
         "2.005733839671579",
         "0.19162866882646157",
         "0.1997110643774087",
         "0.9595295554798444",
         "0.33741329299585887",
         "-0.08640008379138216",
         "0.09292648427269269",
         "-0.9297681330312807",
         "0.3526084561476145",
         "0.603740776626774",
         "0.536383799655526",
         "1.1255760837939284",
         "0.26048618714390664",
         "0.14769565515124444",
         "0.08056434571410538",
         "1.8332632610877837",
         "0.06691873810789198",
         "-1.060196926416917",
         "0.9343588531945957",
         "-1.1346785261273846",
         "0.25665219268609857",
         "-1.4593345803978668",
         "1.6103969352562404",
         "-0.9061955772821083",
         "0.3649464657922501",
         "-1.0307817034460658",
         "1.063869723354057",
         "-0.968898428838002",
         "0.3327183487802253",
         "-0.7514391198722545",
         "0.8345188655071727",
         "-0.9004459346950449",
         "0.36799632360677514",
         "0.02780541948060683",
         "0.0823996113663899",
         "0.3374460027119548",
         "0.735817731774588"
        ],
        [
         "4",
         "Asset",
         "VIX",
         "1922",
         "1922",
         "8",
         "R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_trusted_spot_usd_1d,log_diff_TxCnt,log_UnemRt,log_IndPro,log_CPIPrc,log_TotRes",
         "0",
         "0.009821604081243773",
         "0.005680763951944279",
         "13803.617585576609",
         "13853.667679882381",
         "-6892.808792788304",
         "0.6309880353299868",
         "0.7523095271374289",
         "2.005733839671579",
         "0.19162866882646157",
         "0.1997110643774087",
         "0.9595295554798444",
         "0.33741329299585887",
         "-0.08640008379138216",
         "0.09292648427269269",
         "-0.9297681330312807",
         "0.3526084561476145",
         "0.603740776626774",
         "0.536383799655526",
         "1.1255760837939284",
         "0.26048618714390664",
         "0.14769565515124444",
         "0.08056434571410538",
         "1.8332632610877837",
         "0.06691873810789198",
         "-1.060196926416917",
         "0.9343588531945957",
         "-1.1346785261273846",
         "0.25665219268609857",
         "-1.4593345803978668",
         "1.6103969352562404",
         "-0.9061955772821083",
         "0.3649464657922501",
         "-1.0307817034460658",
         "1.063869723354057",
         "-0.968898428838002",
         "0.3327183487802253",
         "-0.7514391198722545",
         "0.8345188655071727",
         "-0.9004459346950449",
         "0.36799632360677514",
         "0.02780541948060683",
         "0.0823996113663899",
         "0.3374460027119548",
         "0.735817731774588"
        ]
       ],
       "shape": {
        "columns": 51,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>proxy</th>\n",
       "      <th>n</th>\n",
       "      <th>nobs</th>\n",
       "      <th>n_regs</th>\n",
       "      <th>regs_used</th>\n",
       "      <th>opt_hac_lag</th>\n",
       "      <th>r2</th>\n",
       "      <th>adj_r2</th>\n",
       "      <th>aic</th>\n",
       "      <th>...</th>\n",
       "      <th>tval_log_IndPro</th>\n",
       "      <th>pval_log_IndPro</th>\n",
       "      <th>coef_log_CPIPrc</th>\n",
       "      <th>stderr_log_CPIPrc</th>\n",
       "      <th>tval_log_CPIPrc</th>\n",
       "      <th>pval_log_CPIPrc</th>\n",
       "      <th>coef_log_TotRes</th>\n",
       "      <th>stderr_log_TotRes</th>\n",
       "      <th>tval_log_TotRes</th>\n",
       "      <th>pval_log_TotRes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asset</td>\n",
       "      <td>ConSIX</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>8</td>\n",
       "      <td>R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>13803.617586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968898</td>\n",
       "      <td>0.332718</td>\n",
       "      <td>-0.751439</td>\n",
       "      <td>0.834519</td>\n",
       "      <td>-0.900446</td>\n",
       "      <td>0.367996</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.337446</td>\n",
       "      <td>0.735818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asset</td>\n",
       "      <td>EPU_DUS</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>8</td>\n",
       "      <td>R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>13803.617586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968898</td>\n",
       "      <td>0.332718</td>\n",
       "      <td>-0.751439</td>\n",
       "      <td>0.834519</td>\n",
       "      <td>-0.900446</td>\n",
       "      <td>0.367996</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.337446</td>\n",
       "      <td>0.735818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asset</td>\n",
       "      <td>InvSIX</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>8</td>\n",
       "      <td>R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>13803.617586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968898</td>\n",
       "      <td>0.332718</td>\n",
       "      <td>-0.751439</td>\n",
       "      <td>0.834519</td>\n",
       "      <td>-0.900446</td>\n",
       "      <td>0.367996</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.337446</td>\n",
       "      <td>0.735818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asset</td>\n",
       "      <td>TwitSIX</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>8</td>\n",
       "      <td>R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>13803.617586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968898</td>\n",
       "      <td>0.332718</td>\n",
       "      <td>-0.751439</td>\n",
       "      <td>0.834519</td>\n",
       "      <td>-0.900446</td>\n",
       "      <td>0.367996</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.337446</td>\n",
       "      <td>0.735818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asset</td>\n",
       "      <td>VIX</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>8</td>\n",
       "      <td>R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>13803.617586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968898</td>\n",
       "      <td>0.332718</td>\n",
       "      <td>-0.751439</td>\n",
       "      <td>0.834519</td>\n",
       "      <td>-0.900446</td>\n",
       "      <td>0.367996</td>\n",
       "      <td>0.027805</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.337446</td>\n",
       "      <td>0.735818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   group    proxy     n  nobs  n_regs  \\\n",
       "0  Asset   ConSIX  1922  1922       8   \n",
       "1  Asset  EPU_DUS  1922  1922       8   \n",
       "2  Asset   InvSIX  1922  1922       8   \n",
       "3  Asset  TwitSIX  1922  1922       8   \n",
       "4  Asset      VIX  1922  1922       8   \n",
       "\n",
       "                                           regs_used  opt_hac_lag        r2  \\\n",
       "0  R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...            0  0.009822   \n",
       "1  R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...            0  0.009822   \n",
       "2  R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...            0  0.009822   \n",
       "3  R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...            0  0.009822   \n",
       "4  R_g_t_lag,log_diff_AdrActCnt,log_diff_volume_t...            0  0.009822   \n",
       "\n",
       "     adj_r2           aic  ...  tval_log_IndPro  pval_log_IndPro  \\\n",
       "0  0.005681  13803.617586  ...        -0.968898         0.332718   \n",
       "1  0.005681  13803.617586  ...        -0.968898         0.332718   \n",
       "2  0.005681  13803.617586  ...        -0.968898         0.332718   \n",
       "3  0.005681  13803.617586  ...        -0.968898         0.332718   \n",
       "4  0.005681  13803.617586  ...        -0.968898         0.332718   \n",
       "\n",
       "   coef_log_CPIPrc  stderr_log_CPIPrc  tval_log_CPIPrc  pval_log_CPIPrc  \\\n",
       "0        -0.751439           0.834519        -0.900446         0.367996   \n",
       "1        -0.751439           0.834519        -0.900446         0.367996   \n",
       "2        -0.751439           0.834519        -0.900446         0.367996   \n",
       "3        -0.751439           0.834519        -0.900446         0.367996   \n",
       "4        -0.751439           0.834519        -0.900446         0.367996   \n",
       "\n",
       "   coef_log_TotRes  stderr_log_TotRes  tval_log_TotRes  pval_log_TotRes  \n",
       "0         0.027805             0.0824         0.337446         0.735818  \n",
       "1         0.027805             0.0824         0.337446         0.735818  \n",
       "2         0.027805             0.0824         0.337446         0.735818  \n",
       "3         0.027805             0.0824         0.337446         0.735818  \n",
       "4         0.027805             0.0824         0.337446         0.735818  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# H2 — Functional groups (Average → then Transform)\n",
    "# Only two outputs:\n",
    "#   1) Reg/h2_functional_avg_then_transform_results.csv  (full results: coef/SE/t/p + diagnostics)\n",
    "#   2) Reg/h2_effective_membership_overall_table.tex     (functional groups table)\n",
    "# Mirrors H1 column naming: coef_*, stderr_*, tval_*, pval_*\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# -------------------- TOGGLES --------------------\n",
    "USE_JUMPS        = False  # include jump_* dummies as controls\n",
    "STANDARDIZE_CONT = True   # z-score continuous regressors only (not jumps)\n",
    "VERBOSE          = True\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATE_COL   = \"date\"\n",
    "SYMBOL_COL = \"symbol\"\n",
    "RET_COL    = \"log_daily_return\"   # already in df_crypto\n",
    "TARGET_COL = \"R_g_t_plus1\"        # target built below\n",
    "\n",
    "# Raw blockchain columns to average at group level\n",
    "BASE_BLOCKCHAIN = [\"TxCnt\", \"AdrActCnt\", \"volume_trusted_spot_usd_1d\"]\n",
    "\n",
    "# Macro (log levels in df) and sentiments\n",
    "MACRO_COLS     = [\"log_UnemRt\", \"log_IndPro\", \"log_CPIPrc\", \"log_TotRes\"]\n",
    "SENTIMENT_COLS = [\"diff_VIX\", \"diff_TwitSIX\", \"diff_EPU_DUS\", \"diff_InvSIX\", \"diff_fng_value\", \"diff_ConSIX\"]\n",
    "\n",
    "# Regression controls\n",
    "MIN_OBS_FLOOR = 365    # minimum sample length\n",
    "MARGIN_OBS    = 30     # buffer above number of parameters\n",
    "\n",
    "# Outputs (ONLY these two)\n",
    "EXPORT_DIR     = Path(\"Regressions\")\n",
    "RESULTS_CSV    = EXPORT_DIR / \"rob_h2_functional_avg_then_transform_results.csv\"\n",
    "FUNC_TABLE_TEX = EXPORT_DIR / \"trash.tex\"\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ---------- Functional groups (as per your taxonomy) ----------\n",
    "UTILITY = {\"1INCH\",\"AAVE\",\"ANT\",\"APE\",\"BADGER\",\"BAL\",\"BAT\",\"BIT\",\"CEL\",\"COMP\",\"CRV\",\"DCR\",\"ENS\",\"FXS\",\n",
    "           \"HEDG\",\"LDO\",\"MKR\",\"QNT\",\"ROOK\",\"SUSHI\",\"SWRV\",\"UNI\",\"YFI\"}\n",
    "ASSET   = {\"ALCX\",\"ALPHA\",\"CVX\",\"PAXG\",\"XAUT\"}\n",
    "PAYMENT = {\"BCH\",\"BNB\",\"BSV\",\"BTC\",\"BTG\",\"CRO\",\"DASH\",\"DGB\",\"DOGE\",\"ETC\",\"ETH\",\"FTT\",\"GRIN\",\"HT\",\"LTC\",\n",
    "           \"MTL_METAL\",\"PAY\",\"SHIB\",\"VTC\",\"XLM\",\"XMR\",\"XRP\",\"XVG\",\"ZEC\"}\n",
    "HYBRID_UA = {\"CVC\"}\n",
    "HYBRID_UP = {\"ADA\",\"ALGO\",\"API3\",\"ATOM\",\"AUDIO\",\"AVAX\",\"BNT\",\"DOT\",\"DRGN\",\"ELF\",\"ENJ\",\"EOS\",\"FIL\",\"FLOW\",\n",
    "             \"FUN\",\"GALA\",\"GAS\",\"GLM\",\"GNO\",\"GNT\",\"GRT\",\"ICP\",\"ICX\",\"KNC\",\"LEND\",\"LINK\",\"LOOM\",\"LPT\",\"LRC\",\n",
    "             \"LSK\",\"LUNA\",\"MAID\",\"MANA\",\"NEO\",\"NMR\",\"OGN\",\"OMG\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\"QASH\",\"QTUM\",\n",
    "             \"REN\",\"REP\",\"RSR\",\"SAND\",\"SKL\",\"SNT\",\"SNX\",\"SOL\",\"SRM\",\"STORJ\",\"TRX\",\"UMA\",\"VET\",\"WAVES\",\n",
    "             \"WNXM\",\"WTC\",\"XEM\",\"XTZ\",\"ZIL\",\"ZRX\"}\n",
    "HYBRID_AP = {\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\",\"USDC\",\"USDT\"}\n",
    "\n",
    "GROUP_ORDER = [\"Utility\", \"Asset\", \"Payment\", \"Hybrid (U-A)\", \"Hybrid (U-P)\", \"Hybrid (A-P)\"]\n",
    "\n",
    "def map_functional_group(sym: str) -> str:\n",
    "    s = str(sym).upper()\n",
    "    if s in UTILITY:   return \"Utility\"\n",
    "    if s in ASSET:     return \"Asset\"\n",
    "    if s in PAYMENT:   return \"Payment\"\n",
    "    if s in HYBRID_UA: return \"Hybrid (U-A)\"\n",
    "    if s in HYBRID_UP: return \"Hybrid (U-P)\"\n",
    "    if s in HYBRID_AP: return \"Hybrid (A-P)\"\n",
    "    return \"Unclassified\"\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def min_required_n(regressors: List[str], floor: int = MIN_OBS_FLOOR, margin: int = MARGIN_OBS) -> int:\n",
    "    p = 1 + len(regressors)  # + intercept\n",
    "    return max(floor, p + margin)\n",
    "\n",
    "def group_eligible(sub: pd.DataFrame, target: str, regressors: List[str]) -> Tuple[bool, str]:\n",
    "    if any(r not in sub.columns for r in regressors):\n",
    "        return False, \"missing_columns\"\n",
    "    cc = sub.dropna(subset=[target])\n",
    "    if len(cc) < min_required_n(regressors):\n",
    "        return False, f\"too_few_obs:{len(cc)}\"\n",
    "    # Require variation for continuous vars (allow jump_* to be constant)\n",
    "    for r in regressors:\n",
    "        if not r.startswith(\"jump_\") and r in cc.columns and cc[r].nunique() < 2:\n",
    "            return False, f\"no_variation:{r}\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "def select_hac_lag_via_resid_aic(resid: pd.Series, kmax: int) -> int:\n",
    "    n = resid.shape[0]\n",
    "    if n < 20 or kmax <= 0:\n",
    "        return 0\n",
    "    best_k, best_aic = 0, np.inf\n",
    "    for k in range(kmax + 1):\n",
    "        try:\n",
    "            if k == 0:\n",
    "                e = resid - resid.mean()\n",
    "                sigma2 = np.var(e, ddof=1)\n",
    "                aic = n * np.log(sigma2 + 1e-12) + 2\n",
    "            else:\n",
    "                ar = AutoReg(resid, lags=k, old_names=False, trend=\"c\").fit()\n",
    "                aic = ar.aic\n",
    "        except Exception:\n",
    "            aic = np.inf\n",
    "        if aic < best_aic:\n",
    "            best_aic, best_k = aic, k\n",
    "    return int(best_k)\n",
    "\n",
    "def _tex_escape_ticker(s: str) -> str:\n",
    "    return str(s).replace(\"_\", r\"\\_\")\n",
    "\n",
    "# ---------- Build group panel (avg raw → transform) + capture daily contributors ----------\n",
    "def make_group_panel_avg_then_transform_with_members(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      grp : group panel with R_g_t, R_g_t_lag, R_g_t_plus1 and group-level log_diffs/jumps\n",
    "      mem_day : long DataFrame with [date, h2_group, symbol] listing daily contributors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[DATE_COL]   = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "    df[SYMBOL_COL] = df[SYMBOL_COL].astype(str).str.upper()\n",
    "    df[\"h2_group\"] = df[SYMBOL_COL].map(map_functional_group)\n",
    "    df = df[df[\"h2_group\"].isin(GROUP_ORDER)].copy()\n",
    "\n",
    "    # Eligible rows for raw group means: need raw blockchain cols present that day\n",
    "    elig = df.dropna(subset=BASE_BLOCKCHAIN).copy()\n",
    "\n",
    "    # (A) daily contributor list per (date, group)\n",
    "    mem_day = (elig.loc[:, [DATE_COL, \"h2_group\", SYMBOL_COL]]\n",
    "                    .drop_duplicates()\n",
    "                    .rename(columns={SYMBOL_COL: \"symbol\"})\n",
    "                    .sort_values([\"h2_group\", DATE_COL, \"symbol\"])\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "    # (B) group means of RAW blockchain + returns\n",
    "    agg_cols = [RET_COL] + BASE_BLOCKCHAIN\n",
    "    grp_raw = (elig.groupby([DATE_COL, \"h2_group\"], dropna=False)[agg_cols]\n",
    "                    .mean(numeric_only=True)\n",
    "                    .reset_index()\n",
    "                    .sort_values([\"h2_group\", DATE_COL]))\n",
    "\n",
    "    # (C) zero-safe log-diffs (+ optional jump dummies) on group averages\n",
    "    for col in BASE_BLOCKCHAIN:\n",
    "        prev = grp_raw.groupby(\"h2_group\")[col].shift(1)\n",
    "        curr = grp_raw[col]\n",
    "        if USE_JUMPS:\n",
    "            grp_raw[f\"jump_{col}\"] = ((prev == 0) & (curr > 0)).astype(int)\n",
    "        grp_raw[f\"log_diff_{col}\"] = np.where((prev > 0) & (curr > 0), np.log(curr / prev), 0.0)\n",
    "\n",
    "    # (D) attach date-level macro + sentiment (unique per date)\n",
    "    date_cols = [DATE_COL] + [c for c in (MACRO_COLS + SENTIMENT_COLS) if c in df.columns]\n",
    "    date_level = df[date_cols].drop_duplicates(subset=[DATE_COL]).sort_values(DATE_COL)\n",
    "    grp = grp_raw.merge(date_level, on=DATE_COL, how=\"left\")\n",
    "\n",
    "    # (E) target and lags\n",
    "    grp[\"R_g_t\"]       = grp.groupby(\"h2_group\")[RET_COL].shift(0)\n",
    "    grp[\"R_g_t_lag\"]   = grp.groupby(\"h2_group\")[RET_COL].shift(1)\n",
    "    grp[\"R_g_t_plus1\"] = grp.groupby(\"h2_group\")[RET_COL].shift(-1)\n",
    "\n",
    "    # (F) drop missing edges\n",
    "    grp = grp.dropna(subset=[\"R_g_t_lag\", \"R_g_t_plus1\"]).reset_index(drop=True)\n",
    "    return grp, mem_day\n",
    "\n",
    "# ---------- Effective membership utilities ----------\n",
    "def build_effective_membership(mem_day: pd.DataFrame,\n",
    "                               used_dates: pd.Index,\n",
    "                               group_name: str) -> pd.DataFrame:\n",
    "    sub = mem_day[(mem_day[\"h2_group\"] == group_name) & (mem_day[DATE_COL].isin(used_dates))].copy()\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=[\"group\",\"symbol\",\"contrib_days\"])\n",
    "    out = (sub.groupby([\"h2_group\",\"symbol\"], as_index=False)\n",
    "               .agg(contrib_days=(\"symbol\",\"size\"))\n",
    "               .rename(columns={\"h2_group\":\"group\"}))\n",
    "    return out\n",
    "\n",
    "def summarize_membership(df_long: pd.DataFrame,\n",
    "                         group_order=GROUP_ORDER) -> pd.DataFrame:\n",
    "    if df_long.empty:\n",
    "        return pd.DataFrame(columns=[\"Token Type\",\"Ticker\",\"# Coins\"])\n",
    "    tmp = (df_long.groupby(\"group\", as_index=False)\n",
    "                 .agg(symbols=(\"symbol\", lambda s: sorted(set(s))),\n",
    "                      n_coins=(\"symbol\", \"nunique\")))\n",
    "    tmp[\"Ticker\"] = tmp[\"symbols\"].apply(lambda lst: \", \".join(_tex_escape_ticker(s) for s in lst))\n",
    "    tmp = tmp.drop(columns=[\"symbols\"]).rename(columns={\"group\":\"Token Type\", \"n_coins\":\"# Coins\"})\n",
    "    tmp[\"Token Type\"] = pd.Categorical(tmp[\"Token Type\"], categories=group_order, ordered=True)\n",
    "    return tmp.sort_values(\"Token Type\").reset_index(drop=True)\n",
    "\n",
    "def render_latex_functional_table(df_tbl: pd.DataFrame,\n",
    "                                  caption: str,\n",
    "                                  label: str,\n",
    "                                  colwidths=(\"3cm\",\"10cm\",\"1.2cm\")) -> str:\n",
    "    lines = []\n",
    "    lines += [r\"\\begin{table}[ht]\",\n",
    "              r\"\\centering\",\n",
    "              r\"\\scriptsize\",\n",
    "              r\"\\setlength{\\tabcolsep}{4pt}\",\n",
    "              rf\"\\begin{{tabular}}{{p{{{colwidths[0]}}}p{{{colwidths[1]}}}c}}\",\n",
    "              r\"\\toprule\",\n",
    "              r\"\\textbf{Token Type} & \\textbf{Ticker} & \\textbf{\\# Coins} \\\\\",\n",
    "              r\"\\midrule\"]\n",
    "    total = 0\n",
    "    for _, row in df_tbl.iterrows():\n",
    "        lines.append(f\"{row['Token Type']} & {row['Ticker']} & {int(row['# Coins'])} \\\\\\\\\")\n",
    "        lines.append(r\"\\addlinespace\")\n",
    "        total += int(row['# Coins'])\n",
    "    lines += [rf\"\\textbf{{Total:}} & & \\textbf{{{total}}} \\\\\",\n",
    "              r\"\\bottomrule\",\n",
    "              r\"\\end{tabular}\",\n",
    "              rf\"\\caption{{\\textbf{{{caption}}}}}\",\n",
    "              rf\"\\label{{{label}}}\",\n",
    "              r\"\\end{table}\"]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- One regression (group × proxy) that returns H1-style row ----------\n",
    "def run_group_proxy_regression(dfg: pd.DataFrame,\n",
    "                               group_name: str,\n",
    "                               proxy_label: Optional[str],\n",
    "                               regressors: List[str]) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    dfg must contain: TARGET_COL and regressors (already cleaned & standardized if needed)\n",
    "    Returns one row with H1-like columns: n, nobs, n_regs, regs_used, opt_hac_lag, r2, adj_r2, aic, bic,\n",
    "    log_likelihood, fstat, f_pval, durbin_watson, and per-parameter coef_*, stderr_*, tval_*, pval_*.\n",
    "    \"\"\"\n",
    "    rhs = \" + \".join(regressors)\n",
    "    fit = smf.ols(f\"{TARGET_COL} ~ {rhs}\", data=dfg).fit()\n",
    "    n = int(fit.nobs)\n",
    "    if n <= 8:\n",
    "        return None\n",
    "\n",
    "    # HAC lag selection via AR(k)-AIC\n",
    "    kmax = int(n ** 0.25)\n",
    "    best_aic, best_lag, best_res = np.inf, 0, None\n",
    "    for lag in range(kmax + 1):\n",
    "        r = fit.get_robustcov_results(cov_type=\"HAC\", maxlags=lag, use_correction=True)\n",
    "        if r.aic < best_aic:\n",
    "            best_aic, best_lag, best_res = r.aic, lag, r\n",
    "\n",
    "    # Build output row mirroring H1 style\n",
    "    row = {\n",
    "        \"group\": group_name,\n",
    "        \"proxy\": proxy_label if proxy_label is not None else \"no_sentiment\",\n",
    "        \"n\": n,                        # H1 included both n and nobs\n",
    "        \"nobs\": int(best_res.nobs),\n",
    "        \"n_regs\": len(regressors),\n",
    "        \"regs_used\": \",\".join(regressors),\n",
    "        \"opt_hac_lag\": int(best_lag),\n",
    "        \"r2\": float(best_res.rsquared),\n",
    "        \"adj_r2\": float(best_res.rsquared_adj),\n",
    "        \"aic\": float(best_res.aic),\n",
    "        \"bic\": float(best_res.bic),\n",
    "        \"log_likelihood\": float(best_res.llf),\n",
    "        \"fstat\": float(best_res.fvalue) if best_res.fvalue is not None else np.nan,\n",
    "        \"f_pval\": float(best_res.f_pvalue) if best_res.f_pvalue is not None else np.nan,\n",
    "        \"durbin_watson\": float(durbin_watson(fit.resid)),\n",
    "    }\n",
    "\n",
    "    # Add per-parameter columns, as in H1\n",
    "    for name, b, se, t, p in zip(best_res.model.exog_names,\n",
    "                                 best_res.params, best_res.bse, best_res.tvalues, best_res.pvalues):\n",
    "        row[f\"coef_{name}\"]   = float(b)\n",
    "        row[f\"stderr_{name}\"] = float(se)\n",
    "        row[f\"tval_{name}\"]   = float(t)\n",
    "        row[f\"pval_{name}\"]   = float(p)\n",
    "\n",
    "    return row\n",
    "\n",
    "# ---------- Driver: run all proxies, write one CSV + one LaTeX table ----------\n",
    "def run_h2_write_only_results_and_func_table(df_crypto: pd.DataFrame,\n",
    "                                             export_dir: Path = EXPORT_DIR,\n",
    "                                             verbose: bool = VERBOSE) -> pd.DataFrame:\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Build group panel and daily membership (to know who actually contributes)\n",
    "    grp, mem_day = make_group_panel_avg_then_transform_with_members(df_crypto)\n",
    "\n",
    "    # 2) Regressors lists\n",
    "    base_cont = [\"R_g_t_lag\",\n",
    "                 \"log_diff_AdrActCnt\", \"log_diff_volume_trusted_spot_usd_1d\", \"log_diff_TxCnt\"]\n",
    "    base_jumps = ([\"jump_AdrActCnt\", \"jump_volume_trusted_spot_usd_1d\", \"jump_TxCnt\"] if USE_JUMPS else [])\n",
    "    macro      = [c for c in MACRO_COLS if c in grp.columns]\n",
    "\n",
    "    proxy_dict = {\n",
    "        \"no_sentiment\": None,\n",
    "        \"EPU_DUS\":      \"EPU_DUS\",\n",
    "        \"VIX\":          \"VIX\",\n",
    "        \"InvSIX\":       \"InvSIX\",\n",
    "        \"TwitSIX\":      \"TwitSIX\",\n",
    "        \"ConSIX\":       \"ConSIX\",\n",
    "        \"fng_value\":    \"fng_value\"\n",
    "    }\n",
    "\n",
    "    results_rows = []\n",
    "    overall_members = []  # collect by (group, symbol) across all successful regressions\n",
    "\n",
    "    present_groups = [g for g in GROUP_ORDER if g in grp[\"h2_group\"].unique()]\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Groups present: {present_groups}\")\n",
    "\n",
    "    for label, proxy in proxy_dict.items():\n",
    "        for g in present_groups:\n",
    "            sub = grp[grp[\"h2_group\"] == g].copy()\n",
    "\n",
    "            # Assemble RHS\n",
    "            regressors = base_cont.copy()\n",
    "            if proxy is not None and proxy in sub.columns:\n",
    "                regressors.insert(1, proxy)  # sentiment after lagged return\n",
    "            regressors += base_jumps + macro\n",
    "\n",
    "            # Keep only needed cols + date for membership\n",
    "            cols_needed = [TARGET_COL] + regressors\n",
    "            dfg = sub[cols_needed + [DATE_COL]].copy()\n",
    "\n",
    "            # Coerce numeric & drop non-finite\n",
    "            for c in cols_needed:\n",
    "                dfg[c] = pd.to_numeric(dfg[c], errors=\"coerce\")\n",
    "            dfg = dfg.replace([np.inf, -np.inf], np.nan).dropna(subset=cols_needed)\n",
    "\n",
    "            # Eligibility\n",
    "            ok, reason = group_eligible(dfg, TARGET_COL, regressors)\n",
    "            if not ok:\n",
    "                if verbose: print(f\"[SKIP] {g} × {proxy or 'no_sentiment'} — {reason}\")\n",
    "                continue\n",
    "\n",
    "            # Standardize continuous (not jumps)\n",
    "            cont = [r for r in regressors if not r.startswith(\"jump_\")]\n",
    "            if STANDARDIZE_CONT and cont:\n",
    "                dfg[cont] = StandardScaler().fit_transform(dfg[cont])\n",
    "\n",
    "            # Run regression and append H1-style row\n",
    "            row = run_group_proxy_regression(dfg, g, proxy, regressors)\n",
    "            if row is None:\n",
    "                if verbose: print(f\"[SKIP] {g} × {proxy or 'no_sentiment'} — too few nobs after clean\")\n",
    "                continue\n",
    "            results_rows.append(row)\n",
    "\n",
    "            # Track effective membership for the dates actually used in this regression\n",
    "            used_dates = dfg[DATE_COL]\n",
    "            eff = build_effective_membership(mem_day, used_dates, g)\n",
    "            if not eff.empty:\n",
    "                overall_members.append(eff)\n",
    "\n",
    "    # 3) Write the single results CSV\n",
    "    results_df = pd.DataFrame(results_rows).sort_values([\"group\",\"proxy\"]).reset_index(drop=True)\n",
    "    results_df.to_csv(RESULTS_CSV, index=False)\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Wrote results to {RESULTS_CSV} (rows={len(results_df)})\")\n",
    "\n",
    "    # 4) Build and write ONLY the functional membership LaTeX table (overall union)\n",
    "    if overall_members:\n",
    "        overall_long = pd.concat(overall_members, ignore_index=True)\n",
    "        # Union across proxies by (group, symbol)\n",
    "        overall_union = overall_long.drop_duplicates(subset=[\"group\",\"symbol\"])\n",
    "        overall_wide  = summarize_membership(overall_union)\n",
    "        latex_code = render_latex_functional_table(\n",
    "            overall_wide,\n",
    "            caption=\"Functional Groups Used in the Final Regression Sample\",\n",
    "            label=\"tab:functional_groups_effective_sample\"\n",
    "        )\n",
    "        with open(FUNC_TABLE_TEX, \"w\") as f:\n",
    "            f.write(latex_code)\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Wrote functional membership table to {FUNC_TABLE_TEX}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"[WARN] No functional membership captured (no regressions passed eligibility).\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "# df_crypto must already be loaded & pre-engineered as in your earlier prep\n",
    "results_h2 = run_h2_write_only_results_and_func_table(df_crypto)\n",
    "results_h2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "907ea9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/andreazoccarato/.venvs/global313/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "proxy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "groups_tested",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nobs",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hac_maxlags",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wald_test",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "chi2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "df_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "p_value",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2bffb738-70ce-4102-83c1-3bd8ec7e22ca",
       "rows": [
        [
         "0",
         "diff_ConSIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18008",
         "0",
         "equal_betas_across_groups",
         "0.6199517890368689",
         "5",
         "0.6846104751038293"
        ],
        [
         "1",
         "diff_EPU_DUS",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18008",
         "0",
         "equal_betas_across_groups",
         "2.108094362897244",
         "5",
         "0.06134363999508933"
        ],
        [
         "2",
         "diff_InvSIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18008",
         "0",
         "equal_betas_across_groups",
         "0.23022886153880603",
         "5",
         "0.9494721866438236"
        ],
        [
         "3",
         "diff_TwitSIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "13888",
         "0",
         "equal_betas_across_groups",
         "1.1138755759941716",
         "5",
         "0.35045397516202137"
        ],
        [
         "4",
         "diff_VIX",
         "Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P),Hybrid (A-P)",
         "18008",
         "0",
         "equal_betas_across_groups",
         "0.47322948214438953",
         "5",
         "0.7964983566850671"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proxy</th>\n",
       "      <th>groups_tested</th>\n",
       "      <th>nobs</th>\n",
       "      <th>hac_maxlags</th>\n",
       "      <th>wald_test</th>\n",
       "      <th>chi2</th>\n",
       "      <th>df_num</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diff_ConSIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18008</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>0.619952</td>\n",
       "      <td>5</td>\n",
       "      <td>0.684610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diff_EPU_DUS</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18008</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>2.108094</td>\n",
       "      <td>5</td>\n",
       "      <td>0.061344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diff_InvSIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18008</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>0.230229</td>\n",
       "      <td>5</td>\n",
       "      <td>0.949472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diff_TwitSIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>13888</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>1.113876</td>\n",
       "      <td>5</td>\n",
       "      <td>0.350454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diff_VIX</td>\n",
       "      <td>Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...</td>\n",
       "      <td>18008</td>\n",
       "      <td>0</td>\n",
       "      <td>equal_betas_across_groups</td>\n",
       "      <td>0.473229</td>\n",
       "      <td>5</td>\n",
       "      <td>0.796498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          proxy                                      groups_tested   nobs  \\\n",
       "0   diff_ConSIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18008   \n",
       "1  diff_EPU_DUS  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18008   \n",
       "2   diff_InvSIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18008   \n",
       "3  diff_TwitSIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  13888   \n",
       "4      diff_VIX  Utility,Asset,Payment,Hybrid (U-A),Hybrid (U-P...  18008   \n",
       "\n",
       "   hac_maxlags                  wald_test      chi2  df_num   p_value  \n",
       "0            0  equal_betas_across_groups  0.619952       5  0.684610  \n",
       "1            0  equal_betas_across_groups  2.108094       5  0.061344  \n",
       "2            0  equal_betas_across_groups  0.230229       5  0.949472  \n",
       "3            0  equal_betas_across_groups  1.113876       5  0.350454  \n",
       "4            0  equal_betas_across_groups  0.473229       5  0.796498  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# H2 Wald test: equality of sentiment slopes across groups\n",
    "# - Pooled OLS with group FE and group-specific sentiment slopes\n",
    "# - Newey–West HAC (AIC-selected lag)\n",
    "# - One χ² Wald test per sentiment proxy\n",
    "# - Returns DataFrame; optionally writes Reg/h2_wald_tests.csv\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "DATE_COL   = \"date\"\n",
    "SYMBOL_COL = \"symbol\"\n",
    "RET_COL    = \"log_daily_return\"\n",
    "TARGET_COL = \"R_g_t_plus1\"  # constructed below\n",
    "\n",
    "# Raw blockchain columns to be averaged (RAW → then transform)\n",
    "BASE_BLOCKCHAIN = [\"TxCnt\", \"AdrActCnt\", \"volume_trusted_spot_usd_1d\"]\n",
    "\n",
    "# Macros (log levels already in df_crypto) and Sentiments\n",
    "MACRO_COLS     = [\"log_UnemRt\", \"log_IndPro\", \"log_CPIPrc\", \"log_TotRes\"]\n",
    "SENTIMENT_COLS = [\"diff_VIX\", \"diff_TwitSIX\", \"diff_EPU_DUS\", \"diff_InvSIX\", \"diff_fng_value\", \"diff_ConSIX\"]\n",
    "\n",
    "# Functional groups (as in your taxonomy)\n",
    "UTILITY = {\"1INCH\",\"AAVE\",\"ANT\",\"APE\",\"BADGER\",\"BAL\",\"BAT\",\"BIT\",\"CEL\",\"COMP\",\"CRV\",\"DCR\",\"ENS\",\"FXS\",\n",
    "           \"HEDG\",\"LDO\",\"MKR\",\"QNT\",\"ROOK\",\"SUSHI\",\"SWRV\",\"UNI\",\"YFI\"}\n",
    "ASSET   = {\"ALCX\",\"ALPHA\",\"CVX\",\"PAXG\",\"XAUT\"}\n",
    "PAYMENT = {\"BCH\",\"BNB\",\"BSV\",\"BTC\",\"BTG\",\"CRO\",\"DASH\",\"DGB\",\"DOGE\",\"ETC\",\"ETH\",\"FTT\",\"GRIN\",\"HT\",\"LTC\",\n",
    "           \"MTL_METAL\",\"PAY\",\"SHIB\",\"VTC\",\"XLM\",\"XMR\",\"XRP\",\"XVG\",\"ZEC\"}\n",
    "HYBRID_UA = {\"CVC\"}\n",
    "HYBRID_UP = {\"ADA\",\"ALGO\",\"API3\",\"ATOM\",\"AUDIO\",\"AVAX\",\"BNT\",\"DOT\",\"DRGN\",\"ELF\",\"ENJ\",\"EOS\",\"FIL\",\"FLOW\",\n",
    "             \"FUN\",\"GALA\",\"GAS\",\"GLM\",\"GNO\",\"GNT\",\"GRT\",\"ICP\",\"ICX\",\"KNC\",\"LEND\",\"LINK\",\"LOOM\",\"LPT\",\"LRC\",\n",
    "             \"LSK\",\"LUNA\",\"MAID\",\"MANA\",\"NEO\",\"NMR\",\"OGN\",\"OMG\",\"PERP\",\"POLY\",\"POWR\",\"PPT\",\"QASH\",\"QTUM\",\n",
    "             \"REN\",\"REP\",\"RSR\",\"SAND\",\"SKL\",\"SNT\",\"SNX\",\"SOL\",\"SRM\",\"STORJ\",\"TRX\",\"UMA\",\"VET\",\"WAVES\",\n",
    "             \"WNXM\",\"WTC\",\"XEM\",\"XTZ\",\"ZIL\",\"ZRX\"}\n",
    "HYBRID_AP = {\"BUSD\",\"DAI\",\"GUSD\",\"HUSD\",\"PAX\",\"TUSD\",\"USDC\",\"USDT\"}\n",
    "\n",
    "GROUP_ORDER = [\"Utility\", \"Asset\", \"Payment\", \"Hybrid (U-A)\", \"Hybrid (U-P)\", \"Hybrid (A-P)\"]\n",
    "\n",
    "# Output\n",
    "EXPORT_CSV       = Path(\"Regressions/rob_h2_wald_tests.csv\")\n",
    "WRITE_WALD_CSV   = True  # set False if you don't want a CSV\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def map_functional_group(sym: str) -> str:\n",
    "    s = str(sym).upper()\n",
    "    if s in UTILITY:   return \"Utility\"\n",
    "    if s in ASSET:     return \"Asset\"\n",
    "    if s in PAYMENT:   return \"Payment\"\n",
    "    if s in HYBRID_UA: return \"Hybrid (U-A)\"\n",
    "    if s in HYBRID_UP: return \"Hybrid (U-P)\"\n",
    "    if s in HYBRID_AP: return \"Hybrid (A-P)\"\n",
    "    return \"Unclassified\"\n",
    "\n",
    "def select_hac_lag_via_resid_aic(resid: pd.Series, kmax: int) -> int:\n",
    "    \"\"\"\n",
    "    Choose HAC lag via AIC on AR(k) residual model (k=0..kmax).\n",
    "    For k=0, use log-variance proxy objective.\n",
    "    \"\"\"\n",
    "    n = resid.shape[0]\n",
    "    if n < 20 or kmax <= 0:\n",
    "        return 0\n",
    "    best_k, best_aic = 0, np.inf\n",
    "    for k in range(kmax + 1):\n",
    "        try:\n",
    "            if k == 0:\n",
    "                e = resid - resid.mean()\n",
    "                sigma2 = np.var(e, ddof=1)\n",
    "                aic = n * np.log(sigma2 + 1e-12) + 2\n",
    "            else:\n",
    "                ar = AutoReg(resid, lags=k, old_names=False, trend=\"c\").fit()\n",
    "                aic = ar.aic\n",
    "        except Exception:\n",
    "            aic = np.inf\n",
    "        if aic < best_aic:\n",
    "            best_aic, best_k = aic, k\n",
    "    return int(best_k)\n",
    "\n",
    "def make_group_panel_avg_then_transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    H2 panel: average RAW → zero-safe log-diffs → build lags.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    d[DATE_COL]   = pd.to_datetime(d[DATE_COL], errors=\"coerce\")\n",
    "    d[SYMBOL_COL] = d[SYMBOL_COL].astype(str).str.upper()\n",
    "    d[\"h2_group\"] = d[SYMBOL_COL].map(map_functional_group)\n",
    "    d = d[d[\"h2_group\"].isin(GROUP_ORDER)].copy()\n",
    "\n",
    "    # days where all raw blockchain inputs are present\n",
    "    elig = d.dropna(subset=BASE_BLOCKCHAIN).copy()\n",
    "\n",
    "    # equal-weight means (RAW)\n",
    "    agg_cols = [RET_COL] + BASE_BLOCKCHAIN\n",
    "    grp_raw = (elig.groupby([DATE_COL, \"h2_group\"], dropna=False)[agg_cols]\n",
    "                   .mean(numeric_only=True)\n",
    "                   .reset_index()\n",
    "                   .sort_values([\"h2_group\", DATE_COL]))\n",
    "\n",
    "    # zero-safe log-diffs on group averages\n",
    "    for col in BASE_BLOCKCHAIN:\n",
    "        prev = grp_raw.groupby(\"h2_group\")[col].shift(1)\n",
    "        curr = grp_raw[col]\n",
    "        grp_raw[f\"log_diff_{col}\"] = np.where((prev > 0) & (curr > 0), np.log(curr/prev), 0.0)\n",
    "\n",
    "    # attach date-level macros + sentiments (unique per date)\n",
    "    date_cols = [DATE_COL] + [c for c in (MACRO_COLS + SENTIMENT_COLS) if c in d.columns]\n",
    "    date_level = d[date_cols].drop_duplicates(subset=[DATE_COL]).sort_values(DATE_COL)\n",
    "    grp = grp_raw.merge(date_level, on=DATE_COL, how=\"left\")\n",
    "\n",
    "    # target and lags\n",
    "    grp[\"R_g_t\"]       = grp.groupby(\"h2_group\")[RET_COL].shift(0)\n",
    "    grp[\"R_g_t_lag\"]   = grp.groupby(\"h2_group\")[RET_COL].shift(1)\n",
    "    grp[\"R_g_t_plus1\"] = grp.groupby(\"h2_group\")[RET_COL].shift(-1)\n",
    "\n",
    "    grp = grp.dropna(subset=[\"R_g_t_lag\", \"R_g_t_plus1\"]).reset_index(drop=True)\n",
    "    return grp\n",
    "\n",
    "def find_interaction_param_names(param_names, group_list, proxy):\n",
    "    \"\"\"\n",
    "    Map each group to the parameter name of its interaction slope with the proxy.\n",
    "    Handles both 'C(h2_group)[T.Group]:Proxy' and 'Proxy:C(h2_group)[T.Group]'.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for g in group_list:\n",
    "        pat1 = re.compile(rf\"C\\(h2_group\\)\\[(?:T\\.)?{re.escape(g)}\\]\\:{re.escape(proxy)}$\")\n",
    "        pat2 = re.compile(rf\"{re.escape(proxy)}\\:C\\(h2_group\\)\\[(?:T\\.)?{re.escape(g)}\\]$\")\n",
    "        hit = next((n for n in param_names if pat1.search(n) or pat2.search(n)), None)\n",
    "        mapping[g] = hit\n",
    "    return mapping\n",
    "\n",
    "def build_R_equal_betas(param_names, beta_names):\n",
    "    \"\"\"\n",
    "    Build R for H0: beta_g1 = beta_g2 = ... = beta_gk, implemented as\n",
    "    (beta_gi - beta_g1) = 0 for i=2..k.\n",
    "    \"\"\"\n",
    "    valid = [b for b in beta_names if b is not None]\n",
    "    if len(valid) < 2:\n",
    "        return None, None\n",
    "    p = len(param_names)\n",
    "    base = valid[0]\n",
    "    rows = []\n",
    "    for b in valid[1:]:\n",
    "        r = np.zeros(p)\n",
    "        r[param_names.index(b)] = 1.0\n",
    "        r[param_names.index(base)] = -1.0\n",
    "        rows.append(r)\n",
    "    R = np.vstack(rows)\n",
    "    r = np.zeros(R.shape[0])\n",
    "    return R, r\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "def run_h2_wald_tests(df_crypto: pd.DataFrame,\n",
    "                      write_csv: bool = WRITE_WALD_CSV,\n",
    "                      export_csv: Path = EXPORT_CSV) -> pd.DataFrame:\n",
    "    grp = make_group_panel_avg_then_transform(df_crypto)\n",
    "\n",
    "    present_groups = [g for g in GROUP_ORDER if g in grp[\"h2_group\"].unique()]\n",
    "    if len(present_groups) < 2:\n",
    "        raise ValueError(\"Not enough groups present for a Wald equality test.\")\n",
    "\n",
    "    # common controls (same slope across groups)\n",
    "    controls = [\"R_g_t_lag\", \"log_diff_AdrActCnt\",\n",
    "                \"log_diff_volume_trusted_spot_usd_1d\", \"log_diff_TxCnt\"] \\\n",
    "               + [c for c in MACRO_COLS if c in grp.columns]\n",
    "\n",
    "    out_rows = []\n",
    "    for proxy in [c for c in SENTIMENT_COLS if c in grp.columns]:\n",
    "        # keep rows with all needed vars\n",
    "        sub = grp.dropna(subset=[TARGET_COL, proxy] + controls).copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        # standardize continuous controls + proxy (not FE)\n",
    "        z_cols = controls + [proxy]\n",
    "        for c in z_cols:\n",
    "            m, s = sub[c].mean(), sub[c].std(ddof=0)\n",
    "            if s and np.isfinite(s) and s > 0:\n",
    "                sub[c] = (sub[c] - m) / s\n",
    "\n",
    "        # pooled OLS: group FE + group-specific sentiment slopes + common controls\n",
    "        # FE: 0 + C(h2_group)\n",
    "        # Group-specific proxy slopes: 0 + C(h2_group):proxy\n",
    "        rhs = \" + \".join([\"0 + C(h2_group)\", f\"0 + C(h2_group):{proxy}\"] + controls)\n",
    "        formula = f\"{TARGET_COL} ~ {rhs}\"\n",
    "\n",
    "        fit = smf.ols(formula, data=sub).fit()\n",
    "        n = int(fit.nobs)\n",
    "        if n <= 8:\n",
    "            continue\n",
    "\n",
    "        # HAC lag selection by AIC\n",
    "        kmax = int(n ** 0.25)\n",
    "        best_aic, best_lag, best_res = np.inf, 0, None\n",
    "        for lag in range(kmax + 1):\n",
    "            r = fit.get_robustcov_results(cov_type=\"HAC\", maxlags=lag, use_correction=True)\n",
    "            if r.aic < best_aic:\n",
    "                best_aic, best_lag, best_res = r.aic, lag, r\n",
    "\n",
    "        # parameter names (order) from the robust model's design\n",
    "        param_names = list(best_res.model.exog_names)\n",
    "\n",
    "        # pull the names of the per-group proxy slopes\n",
    "        beta_map = find_interaction_param_names(param_names, present_groups, proxy)\n",
    "        beta_names = [beta_map[g] for g in present_groups]\n",
    "\n",
    "        # build R for equality of those slopes\n",
    "        R, rvec = build_R_equal_betas(param_names, beta_names)\n",
    "        if R is None:\n",
    "            # likely some groups dropped in design (no variation)\n",
    "            continue\n",
    "\n",
    "        wt = best_res.wald_test((R, rvec), scalar=False)  # χ²\n",
    "        chi2 = float(np.asarray(wt.statistic).ravel()[0])\n",
    "        pval = float(np.asarray(wt.pvalue).ravel()[0])\n",
    "        df_num = int(R.shape[0])\n",
    "\n",
    "        out_rows.append({\n",
    "            \"proxy\": proxy,\n",
    "            \"groups_tested\": \",\".join(present_groups),\n",
    "            \"nobs\": n,\n",
    "            \"hac_maxlags\": best_lag,\n",
    "            \"wald_test\": \"equal_betas_across_groups\",\n",
    "            \"chi2\": chi2,\n",
    "            \"df_num\": df_num,\n",
    "            \"p_value\": pval\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(out_rows).sort_values([\"proxy\"]).reset_index(drop=True)\n",
    "    if write_csv:\n",
    "        export_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "        out.to_csv(export_csv, index=False)\n",
    "    return out\n",
    "\n",
    "# -------------------- RUN (example) --------------------\n",
    "wald_results = run_h2_wald_tests(df_crypto)\n",
    "wald_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba99bce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] wrote: Regressions/Tables/h2_level_vs_delta_summary.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\D'\n",
      "/var/folders/40/rktqqvhn67376psyzng8whjc0000gn/T/ipykernel_6469/3470567343.py:13: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  DELTA_PATTERN = \"checkrobustness_$\\Delta$ {proxy}*.csv\"\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "REG_DIR = \"Regressions\"\n",
    "TAB_DIR = Path(\"Regressions/Tables\")\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Per-asset files\n",
    "LEVEL_PATTERN = \"crypto_regression_summary*{proxy}*.csv\"\n",
    "DELTA_PATTERN = \"checkrobustness_$\\Delta$ {proxy}*.csv\"\n",
    "\n",
    "# Wald files\n",
    "LEVEL_WALD = os.path.join(REG_DIR, \"h2_wald_tests.csv\")\n",
    "DELTA_WALD = os.path.join(REG_DIR, \"rob_h2_wald_tests.csv\")  # proxies named diff_<proxy>\n",
    "\n",
    "# Mapping file (same folder as notebook)\n",
    "CRYPTO_MAP = \"cryptomap.csv\"   # needs columns: symbol, Final_Category\n",
    "\n",
    "# Proxies to include (adjust if needed)\n",
    "SENTIMENT_PROXIES = [\"ConSIX\",\"VIX\",\"TwitSIX\",\"InvSIX\",\"EPU_DUS\",\"fng_value\"]\n",
    "\n",
    "# ---------------- Load category map ----------------\n",
    "def load_category_map(path=CRYPTO_MAP):\n",
    "    m = pd.read_csv(path, dtype=str)\n",
    "    # normalize\n",
    "    cols = {c.lower(): c for c in m.columns}\n",
    "    sym_col = cols.get(\"symbol\") or cols.get(\"ticker\") or \"symbol\"\n",
    "    cat_col = cols.get(\"final_category\") or \"Final_Category\"\n",
    "    m[sym_col] = m[sym_col].str.upper().str.strip()\n",
    "    m[cat_col] = m[cat_col].str.strip()\n",
    "    return dict(zip(m[sym_col], m[cat_col]))\n",
    "\n",
    "SYM2CAT = load_category_map()\n",
    "\n",
    "# Determine group (category) order from mapping; prefer a nice order if present\n",
    "preferred = [\"Utility Tokens\", \"Asset Tokens\", \"Payment Tokens\",  \"Hybrid_UA\", \"Hybrid_UP\", \"Hybrid_AP\"]\n",
    "all_groups = pd.Series(list(SYM2CAT.values())).dropna().unique().tolist()\n",
    "GROUP_ORDER = [g for g in preferred if g in all_groups] + [g for g in all_groups if g not in preferred]\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _attach_category(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper()\n",
    "    df[\"Category\"] = df[\"symbol\"].map(SYM2CAT).fillna(\"Other\")\n",
    "    return df\n",
    "\n",
    "def _read_level_proxy(proxy: str) -> pd.DataFrame:\n",
    "    patt = os.path.join(REG_DIR, LEVEL_PATTERN.format(proxy=proxy))\n",
    "    files = glob.glob(patt)\n",
    "    if not files: return pd.DataFrame(columns=[\"symbol\",\"Category\",\"beta\"])\n",
    "    df = pd.read_csv(files[0])\n",
    "    if \"symbol\" not in df.columns: return pd.DataFrame(columns=[\"symbol\",\"Category\",\"beta\"])\n",
    "    coef_col = f\"coef_{proxy}\"\n",
    "    if coef_col not in df.columns:\n",
    "        # Fallback: if exactly one non-control coef exists, use it\n",
    "        cand = [c for c in df.columns if c.startswith(\"coef_\") and not c.startswith((\"coef_log_\",\"coef_R_\",\"coef_diff_\"))]\n",
    "        if len(cand) == 1:\n",
    "            coef_col = cand[0]\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"symbol\",\"Category\",\"beta\"])\n",
    "    out = df[[\"symbol\", coef_col]].rename(columns={coef_col: \"beta\"})\n",
    "    out = _attach_category(out)\n",
    "    return out\n",
    "\n",
    "def _read_delta_proxy(proxy: str) -> pd.DataFrame:\n",
    "    patt = os.path.join(REG_DIR, DELTA_PATTERN.format(proxy=proxy))\n",
    "    files = glob.glob(patt)\n",
    "    if not files: return pd.DataFrame(columns=[\"symbol\",\"Category\",\"beta\"])\n",
    "    df = pd.read_csv(files[0])\n",
    "    if \"symbol\" not in df.columns: return pd.DataFrame(columns=[\"symbol\",\"Category\",\"beta\"])\n",
    "    coef_col = f\"coef_diff_{proxy}\"\n",
    "    if coef_col not in df.columns:\n",
    "        cand = [c for c in df.columns if c.startswith(\"coef_diff_\")]\n",
    "        if len(cand) == 1:\n",
    "            coef_col = cand[0]\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"symbol\",\"Category\",\"beta\"])\n",
    "    out = df[[\"symbol\", coef_col]].rename(columns={coef_col: \"beta\"})\n",
    "    out = _attach_category(out)\n",
    "    return out\n",
    "\n",
    "def _load_wald(path: str, proxies: list[str], is_delta: bool) -> pd.Series:\n",
    "    \"\"\"Return Series index=proxy_base -> p_value; Δ file uses 'diff_<proxy>' rows.\"\"\"\n",
    "    if not os.path.exists(path): return pd.Series(dtype=float)\n",
    "    df = pd.read_csv(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    proxy_col = cols.get(\"proxy\") or list(df.columns)[0]\n",
    "    p_col = cols.get(\"p_value\") or cols.get(\"wald_p\") or cols.get(\"p\") or list(df.columns)[-1]\n",
    "    ser = {}\n",
    "    for p in proxies:\n",
    "        key = f\"diff_{p}\" if is_delta else p\n",
    "        r = df[df[proxy_col] == key]\n",
    "        if not r.empty:\n",
    "            try:\n",
    "                ser[p] = float(r.iloc[0][p_col])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return pd.Series(ser)\n",
    "\n",
    "# ---------------- Build table ----------------\n",
    "def build_h2_table() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    wald_L = _load_wald(LEVEL_WALD, SENTIMENT_PROXIES, is_delta=False)\n",
    "    wald_D = _load_wald(DELTA_WALD, SENTIMENT_PROXIES, is_delta=True)\n",
    "\n",
    "    for prox in SENTIMENT_PROXIES:\n",
    "        # Level row\n",
    "        lvl = _read_level_proxy(prox)\n",
    "        if not lvl.empty:\n",
    "            med = lvl.groupby(\"Category\")[\"beta\"].median()\n",
    "            rowL = {\"Proxy\": prox}\n",
    "            for g in GROUP_ORDER:\n",
    "                v = med.get(g, np.nan)\n",
    "                rowL[g] = \"\" if pd.isna(v) else f\"{v:.3f}\"\n",
    "            rowL[\"Wald $p$-value\"] = \"\" if prox not in wald_L.index else f\"{wald_L.loc[prox]:.3f}\"\n",
    "            rows.append(rowL)\n",
    "\n",
    "        # Δ row\n",
    "        dlt = _read_delta_proxy(prox)\n",
    "        if not dlt.empty:\n",
    "            med = dlt.groupby(\"Category\")[\"beta\"].median()\n",
    "            rowD = {\"Proxy\": rf\"$\\Delta$ {prox}\"}\n",
    "            for g in GROUP_ORDER:\n",
    "                v = med.get(g, np.nan)\n",
    "                rowD[g] = \"\" if pd.isna(v) else f\"{v:.3f}\"\n",
    "            rowD[\"Wald $p$-value\"] = \"\" if prox not in wald_D.index else f\"{wald_D.loc[prox]:.3f}\"\n",
    "            rows.append(rowD)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    cols = [\"Proxy\"] + GROUP_ORDER + [\"Wald $p$-value\"]\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            out[c] = \"\"\n",
    "    return out[cols]\n",
    "\n",
    "# ---- Run & export\n",
    "h2 = build_h2_table()\n",
    "\n",
    "# After you run h2 = build_h2_table()\n",
    "\n",
    "RENAME_GROUPS = {\n",
    "    \"Utility Tokens\": \"Utility\",\n",
    "    \"Asset Tokens\": \"Asset\",\n",
    "    \"Payment Tokens\": \"Payment\",\n",
    "    \"Hybrid_UA\": \"Hybrid (U-A)\",\n",
    "    \"Hybrid_UP\": \"Hybrid (U-P)\",\n",
    "    \"Hybrid_AP\": \"Hybrid (A-P)\",\n",
    "}\n",
    "\n",
    "h2 = h2.rename(columns=RENAME_GROUPS)\n",
    "\n",
    "\n",
    "# Now enforce the correct order\n",
    "GROUP_ORDER = [\"Utility\", \"Asset\", \"Payment\", \"Hybrid (U-A)\", \"Hybrid (U-P)\", \"Hybrid (A-P)\"]\n",
    "cols = [\"Proxy\"] + GROUP_ORDER + [\"Wald $p$-value\"]\n",
    "h2 = h2[cols]\n",
    "\n",
    "latex = h2.to_latex(\n",
    "    index=False, escape=False,\n",
    "    column_format=\"l\" + \"c\"*len(GROUP_ORDER) + \"c\",\n",
    "    caption=(r\"\\textbf{H2 robustness summary (functional groups): Level vs.\\ $\\Delta$ proxies.} \"\n",
    "             r\"Cells report the median $\\hat{\\beta}_{\\mathrm{sent}}$ within each functional group; \"\n",
    "             r\"Wald $p$ refers to the cross-group equality test for the corresponding specification.\"),\n",
    "    label=\"tab:h2_level_vs_delta_summary\"\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"h2_level_vs_delta_summary.tex\").write_text(latex)\n",
    "print(\"[ok] wrote:\", TAB_DIR / \"h2_level_vs_delta_summary.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
